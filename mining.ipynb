{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-14T12:41:37.823932Z",
     "start_time": "2023-12-14T12:41:35.694383Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from itertools import combinations\n",
    "import shutup\n",
    "\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "     day  feature1  feature2  feature3  index_return\n0      1         9        59        72        0.0010\n1      2         5        17        64        0.0002\n2      3        20        38        69       -0.0029\n3      4        14        15        66       -0.0115\n4      5        14        18        61        0.0036\n..   ...       ...       ...       ...           ...\n700  701         3        14        20        0.0092\n701  702        18        21        46       -0.0010\n702  703         2         2        42       -0.0049\n703  704         2         5        41        0.0052\n704  705         3         5        55           NaN\n\n[705 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day</th>\n      <th>feature1</th>\n      <th>feature2</th>\n      <th>feature3</th>\n      <th>index_return</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>9</td>\n      <td>59</td>\n      <td>72</td>\n      <td>0.0010</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>5</td>\n      <td>17</td>\n      <td>64</td>\n      <td>0.0002</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>20</td>\n      <td>38</td>\n      <td>69</td>\n      <td>-0.0029</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>14</td>\n      <td>15</td>\n      <td>66</td>\n      <td>-0.0115</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>14</td>\n      <td>18</td>\n      <td>61</td>\n      <td>0.0036</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>700</th>\n      <td>701</td>\n      <td>3</td>\n      <td>14</td>\n      <td>20</td>\n      <td>0.0092</td>\n    </tr>\n    <tr>\n      <th>701</th>\n      <td>702</td>\n      <td>18</td>\n      <td>21</td>\n      <td>46</td>\n      <td>-0.0010</td>\n    </tr>\n    <tr>\n      <th>702</th>\n      <td>703</td>\n      <td>2</td>\n      <td>2</td>\n      <td>42</td>\n      <td>-0.0049</td>\n    </tr>\n    <tr>\n      <th>703</th>\n      <td>704</td>\n      <td>2</td>\n      <td>5</td>\n      <td>41</td>\n      <td>0.0052</td>\n    </tr>\n    <tr>\n      <th>704</th>\n      <td>705</td>\n      <td>3</td>\n      <td>5</td>\n      <td>55</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>705 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Dataset.csv', dtype={'feature1': np.int8, 'feature2': np.int8, 'feature3': np.int8, 'index_return': str})\n",
    "data['index_return'] = data['index_return'].str.rstrip('%').astype('float') / 100.0\n",
    "data['index_return'] = data['index_return'].shift(-1)  # 很重要，当天预测的是第二天的收益率\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T12:41:37.833528Z",
     "start_time": "2023-12-14T12:41:37.824754Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "        print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        print(f\"Decreased by {decrease:.2f}%\")\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T12:41:37.840457Z",
     "start_time": "2023-12-14T12:41:37.836241Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 根据已有特征构建出新的特征\n",
    "\n",
    "由于根据现有feature无法得知具体是什么feature，因此尽可能多地根据给到地3个feature构建新的feature，并用机器学习模型预测"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "     day  feature1  feature2  feature3  index_return  feature1_feature2_sum  \\\n0      1         9        59        72        0.0010                     68   \n1      2         5        17        64        0.0002                     22   \n2      3        20        38        69       -0.0029                     58   \n3      4        14        15        66       -0.0115                     29   \n4      5        14        18        61        0.0036                     32   \n..   ...       ...       ...       ...           ...                    ...   \n700  701         3        14        20        0.0092                     17   \n701  702        18        21        46       -0.0010                     39   \n702  703         2         2        42       -0.0049                      4   \n703  704         2         5        41        0.0052                      7   \n704  705         3         5        55           NaN                      8   \n\n     feature1_feature2_difference  feature1_feature2_product  \\\n0                             -50                         19   \n1                             -12                         85   \n2                             -18                         -8   \n3                              -1                        -46   \n4                              -4                         -4   \n..                            ...                        ...   \n700                           -11                         42   \n701                            -3                        122   \n702                             0                          4   \n703                            -3                         10   \n704                            -2                         15   \n\n     feature1_feature2_quotient  feature1_feature3_sum  \\\n0                      0.152542                     81   \n1                      0.294118                     69   \n2                      0.526316                     89   \n3                      0.933333                     80   \n4                      0.777778                     75   \n..                          ...                    ...   \n700                    0.214286                     23   \n701                    0.857143                     64   \n702                    1.000000                     44   \n703                    0.400000                     43   \n704                    0.600000                     58   \n\n     feature1_feature3_difference  feature1_feature3_product  \\\n0                             -63                       -120   \n1                             -59                         64   \n2                             -49                        100   \n3                             -52                       -100   \n4                             -47                         86   \n..                            ...                        ...   \n700                           -17                         60   \n701                           -28                         60   \n702                           -40                         84   \n703                           -39                         82   \n704                           -52                        -91   \n\n     feature1_feature3_quotient  feature2_feature3_sum  \\\n0                      0.125000                   -125   \n1                      0.078125                     81   \n2                      0.289855                    107   \n3                      0.212121                     81   \n4                      0.229508                     79   \n..                          ...                    ...   \n700                    0.150000                     34   \n701                    0.391304                     67   \n702                    0.047619                     44   \n703                    0.048780                     46   \n704                    0.054545                     60   \n\n     feature2_feature3_difference  feature2_feature3_product  \\\n0                             -13                       -104   \n1                             -47                         64   \n2                             -31                         62   \n3                             -51                        -34   \n4                             -43                         74   \n..                            ...                        ...   \n700                            -6                         24   \n701                           -25                        -58   \n702                           -40                         84   \n703                           -36                        -51   \n704                           -50                         19   \n\n     feature2_feature3_quotient  \n0                      0.819444  \n1                      0.265625  \n2                      0.550725  \n3                      0.227273  \n4                      0.295082  \n..                          ...  \n700                    0.700000  \n701                    0.456522  \n702                    0.047619  \n703                    0.121951  \n704                    0.090909  \n\n[705 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day</th>\n      <th>feature1</th>\n      <th>feature2</th>\n      <th>feature3</th>\n      <th>index_return</th>\n      <th>feature1_feature2_sum</th>\n      <th>feature1_feature2_difference</th>\n      <th>feature1_feature2_product</th>\n      <th>feature1_feature2_quotient</th>\n      <th>feature1_feature3_sum</th>\n      <th>feature1_feature3_difference</th>\n      <th>feature1_feature3_product</th>\n      <th>feature1_feature3_quotient</th>\n      <th>feature2_feature3_sum</th>\n      <th>feature2_feature3_difference</th>\n      <th>feature2_feature3_product</th>\n      <th>feature2_feature3_quotient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>9</td>\n      <td>59</td>\n      <td>72</td>\n      <td>0.0010</td>\n      <td>68</td>\n      <td>-50</td>\n      <td>19</td>\n      <td>0.152542</td>\n      <td>81</td>\n      <td>-63</td>\n      <td>-120</td>\n      <td>0.125000</td>\n      <td>-125</td>\n      <td>-13</td>\n      <td>-104</td>\n      <td>0.819444</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>5</td>\n      <td>17</td>\n      <td>64</td>\n      <td>0.0002</td>\n      <td>22</td>\n      <td>-12</td>\n      <td>85</td>\n      <td>0.294118</td>\n      <td>69</td>\n      <td>-59</td>\n      <td>64</td>\n      <td>0.078125</td>\n      <td>81</td>\n      <td>-47</td>\n      <td>64</td>\n      <td>0.265625</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>20</td>\n      <td>38</td>\n      <td>69</td>\n      <td>-0.0029</td>\n      <td>58</td>\n      <td>-18</td>\n      <td>-8</td>\n      <td>0.526316</td>\n      <td>89</td>\n      <td>-49</td>\n      <td>100</td>\n      <td>0.289855</td>\n      <td>107</td>\n      <td>-31</td>\n      <td>62</td>\n      <td>0.550725</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>14</td>\n      <td>15</td>\n      <td>66</td>\n      <td>-0.0115</td>\n      <td>29</td>\n      <td>-1</td>\n      <td>-46</td>\n      <td>0.933333</td>\n      <td>80</td>\n      <td>-52</td>\n      <td>-100</td>\n      <td>0.212121</td>\n      <td>81</td>\n      <td>-51</td>\n      <td>-34</td>\n      <td>0.227273</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>14</td>\n      <td>18</td>\n      <td>61</td>\n      <td>0.0036</td>\n      <td>32</td>\n      <td>-4</td>\n      <td>-4</td>\n      <td>0.777778</td>\n      <td>75</td>\n      <td>-47</td>\n      <td>86</td>\n      <td>0.229508</td>\n      <td>79</td>\n      <td>-43</td>\n      <td>74</td>\n      <td>0.295082</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>700</th>\n      <td>701</td>\n      <td>3</td>\n      <td>14</td>\n      <td>20</td>\n      <td>0.0092</td>\n      <td>17</td>\n      <td>-11</td>\n      <td>42</td>\n      <td>0.214286</td>\n      <td>23</td>\n      <td>-17</td>\n      <td>60</td>\n      <td>0.150000</td>\n      <td>34</td>\n      <td>-6</td>\n      <td>24</td>\n      <td>0.700000</td>\n    </tr>\n    <tr>\n      <th>701</th>\n      <td>702</td>\n      <td>18</td>\n      <td>21</td>\n      <td>46</td>\n      <td>-0.0010</td>\n      <td>39</td>\n      <td>-3</td>\n      <td>122</td>\n      <td>0.857143</td>\n      <td>64</td>\n      <td>-28</td>\n      <td>60</td>\n      <td>0.391304</td>\n      <td>67</td>\n      <td>-25</td>\n      <td>-58</td>\n      <td>0.456522</td>\n    </tr>\n    <tr>\n      <th>702</th>\n      <td>703</td>\n      <td>2</td>\n      <td>2</td>\n      <td>42</td>\n      <td>-0.0049</td>\n      <td>4</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1.000000</td>\n      <td>44</td>\n      <td>-40</td>\n      <td>84</td>\n      <td>0.047619</td>\n      <td>44</td>\n      <td>-40</td>\n      <td>84</td>\n      <td>0.047619</td>\n    </tr>\n    <tr>\n      <th>703</th>\n      <td>704</td>\n      <td>2</td>\n      <td>5</td>\n      <td>41</td>\n      <td>0.0052</td>\n      <td>7</td>\n      <td>-3</td>\n      <td>10</td>\n      <td>0.400000</td>\n      <td>43</td>\n      <td>-39</td>\n      <td>82</td>\n      <td>0.048780</td>\n      <td>46</td>\n      <td>-36</td>\n      <td>-51</td>\n      <td>0.121951</td>\n    </tr>\n    <tr>\n      <th>704</th>\n      <td>705</td>\n      <td>3</td>\n      <td>5</td>\n      <td>55</td>\n      <td>NaN</td>\n      <td>8</td>\n      <td>-2</td>\n      <td>15</td>\n      <td>0.600000</td>\n      <td>58</td>\n      <td>-52</td>\n      <td>-91</td>\n      <td>0.054545</td>\n      <td>60</td>\n      <td>-50</td>\n      <td>19</td>\n      <td>0.090909</td>\n    </tr>\n  </tbody>\n</table>\n<p>705 rows × 17 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先构建各个feature的和与差，积与熵\n",
    "ori_features = ['feature1', 'feature2', 'feature3']\n",
    "for c in combinations(ori_features, 2):\n",
    "    data[f\"{c[0]}_{c[1]}_sum\"] = data[c[0]] + data[c[1]]  # 和\n",
    "    data[f\"{c[0]}_{c[1]}_difference\"] = data[c[0]] - data[c[1]]  # 差\n",
    "    data[f\"{c[0]}_{c[1]}_product\"] = data[c[0]] * data[c[1]]  # 积\n",
    "    data[f\"{c[0]}_{c[1]}_quotient\"] = data[c[0]] / data[c[1]]  # 商\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T12:41:37.856561Z",
     "start_time": "2023-12-14T12:41:37.838753Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "     day  feature1  feature2  feature3  index_return  feature1_feature2_sum  \\\n0      1         9        59        72        0.0010                     68   \n1      2         5        17        64        0.0002                     22   \n2      3        20        38        69       -0.0029                     58   \n3      4        14        15        66       -0.0115                     29   \n4      5        14        18        61        0.0036                     32   \n..   ...       ...       ...       ...           ...                    ...   \n700  701         3        14        20        0.0092                     17   \n701  702        18        21        46       -0.0010                     39   \n702  703         2         2        42       -0.0049                      4   \n703  704         2         5        41        0.0052                      7   \n704  705         3         5        55           NaN                      8   \n\n     feature1_feature2_difference  feature1_feature2_product  \\\n0                             -50                         19   \n1                             -12                         85   \n2                             -18                         -8   \n3                              -1                        -46   \n4                              -4                         -4   \n..                            ...                        ...   \n700                           -11                         42   \n701                            -3                        122   \n702                             0                          4   \n703                            -3                         10   \n704                            -2                         15   \n\n     feature1_feature2_quotient  feature1_feature3_sum  ...  \\\n0                      0.152542                     81  ...   \n1                      0.294118                     69  ...   \n2                      0.526316                     89  ...   \n3                      0.933333                     80  ...   \n4                      0.777778                     75  ...   \n..                          ...                    ...  ...   \n700                    0.214286                     23  ...   \n701                    0.857143                     64  ...   \n702                    1.000000                     44  ...   \n703                    0.400000                     43  ...   \n704                    0.600000                     58  ...   \n\n     feature2_feature3_quotient_5_vol  feature2_feature3_quotient_10_mean  \\\n0                                 NaN                                 NaN   \n1                                 NaN                                 NaN   \n2                                 NaN                                 NaN   \n3                                 NaN                                 NaN   \n4                            0.251271                                 NaN   \n..                                ...                                 ...   \n700                          0.144241                            0.516044   \n701                          0.147655                            0.510716   \n702                          0.287898                            0.473478   \n703                          0.333390                            0.449958   \n704                          0.283830                            0.410070   \n\n     feature2_feature3_quotient_10_bias  feature2_feature3_quotient_10_vol  \\\n0                                   NaN                                NaN   \n1                                   NaN                                NaN   \n2                                   NaN                                NaN   \n3                                   NaN                                NaN   \n4                                   NaN                                NaN   \n..                                  ...                                ...   \n700                            0.356474                           0.132196   \n701                           -0.106114                           0.133543   \n702                           -0.899427                           0.198008   \n703                           -0.728972                           0.225431   \n704                           -0.778308                           0.251394   \n\n     feature2_feature3_quotient_20_mean  feature2_feature3_quotient_20_bias  \\\n0                                   NaN                                 NaN   \n1                                   NaN                                 NaN   \n2                                   NaN                                 NaN   \n3                                   NaN                                 NaN   \n4                                   NaN                                 NaN   \n..                                  ...                                 ...   \n700                            0.525114                            0.333044   \n701                            0.535120                           -0.146879   \n702                            0.513890                           -0.907336   \n703                            0.503320                           -0.757707   \n704                            0.465678                           -0.804781   \n\n     feature2_feature3_quotient_20_vol  feature2_feature3_quotient_5_10  \\\n0                                  NaN                              NaN   \n1                                  NaN                              NaN   \n2                                  NaN                              NaN   \n3                                  NaN                              NaN   \n4                                  NaN                              NaN   \n..                                 ...                              ...   \n700                           0.165271                         1.148872   \n701                           0.153807                         1.154709   \n702                           0.188367                         1.063893   \n703                           0.204288                         0.940882   \n704                           0.207592                         0.691103   \n\n     feature2_feature3_quotient_5_20  feature2_feature3_quotient_10_20  \n0                                NaN                               NaN  \n1                                NaN                               NaN  \n2                                NaN                               NaN  \n3                                NaN                               NaN  \n4                                NaN                               NaN  \n..                               ...                               ...  \n700                         1.129028                          0.982727  \n701                         1.102049                          0.954395  \n702                         0.980229                          0.921361  \n703                         0.841130                          0.893980  \n704                         0.608575                          0.880586  \n\n[705 rows x 197 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day</th>\n      <th>feature1</th>\n      <th>feature2</th>\n      <th>feature3</th>\n      <th>index_return</th>\n      <th>feature1_feature2_sum</th>\n      <th>feature1_feature2_difference</th>\n      <th>feature1_feature2_product</th>\n      <th>feature1_feature2_quotient</th>\n      <th>feature1_feature3_sum</th>\n      <th>...</th>\n      <th>feature2_feature3_quotient_5_vol</th>\n      <th>feature2_feature3_quotient_10_mean</th>\n      <th>feature2_feature3_quotient_10_bias</th>\n      <th>feature2_feature3_quotient_10_vol</th>\n      <th>feature2_feature3_quotient_20_mean</th>\n      <th>feature2_feature3_quotient_20_bias</th>\n      <th>feature2_feature3_quotient_20_vol</th>\n      <th>feature2_feature3_quotient_5_10</th>\n      <th>feature2_feature3_quotient_5_20</th>\n      <th>feature2_feature3_quotient_10_20</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>9</td>\n      <td>59</td>\n      <td>72</td>\n      <td>0.0010</td>\n      <td>68</td>\n      <td>-50</td>\n      <td>19</td>\n      <td>0.152542</td>\n      <td>81</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>5</td>\n      <td>17</td>\n      <td>64</td>\n      <td>0.0002</td>\n      <td>22</td>\n      <td>-12</td>\n      <td>85</td>\n      <td>0.294118</td>\n      <td>69</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>20</td>\n      <td>38</td>\n      <td>69</td>\n      <td>-0.0029</td>\n      <td>58</td>\n      <td>-18</td>\n      <td>-8</td>\n      <td>0.526316</td>\n      <td>89</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>14</td>\n      <td>15</td>\n      <td>66</td>\n      <td>-0.0115</td>\n      <td>29</td>\n      <td>-1</td>\n      <td>-46</td>\n      <td>0.933333</td>\n      <td>80</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>14</td>\n      <td>18</td>\n      <td>61</td>\n      <td>0.0036</td>\n      <td>32</td>\n      <td>-4</td>\n      <td>-4</td>\n      <td>0.777778</td>\n      <td>75</td>\n      <td>...</td>\n      <td>0.251271</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>700</th>\n      <td>701</td>\n      <td>3</td>\n      <td>14</td>\n      <td>20</td>\n      <td>0.0092</td>\n      <td>17</td>\n      <td>-11</td>\n      <td>42</td>\n      <td>0.214286</td>\n      <td>23</td>\n      <td>...</td>\n      <td>0.144241</td>\n      <td>0.516044</td>\n      <td>0.356474</td>\n      <td>0.132196</td>\n      <td>0.525114</td>\n      <td>0.333044</td>\n      <td>0.165271</td>\n      <td>1.148872</td>\n      <td>1.129028</td>\n      <td>0.982727</td>\n    </tr>\n    <tr>\n      <th>701</th>\n      <td>702</td>\n      <td>18</td>\n      <td>21</td>\n      <td>46</td>\n      <td>-0.0010</td>\n      <td>39</td>\n      <td>-3</td>\n      <td>122</td>\n      <td>0.857143</td>\n      <td>64</td>\n      <td>...</td>\n      <td>0.147655</td>\n      <td>0.510716</td>\n      <td>-0.106114</td>\n      <td>0.133543</td>\n      <td>0.535120</td>\n      <td>-0.146879</td>\n      <td>0.153807</td>\n      <td>1.154709</td>\n      <td>1.102049</td>\n      <td>0.954395</td>\n    </tr>\n    <tr>\n      <th>702</th>\n      <td>703</td>\n      <td>2</td>\n      <td>2</td>\n      <td>42</td>\n      <td>-0.0049</td>\n      <td>4</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1.000000</td>\n      <td>44</td>\n      <td>...</td>\n      <td>0.287898</td>\n      <td>0.473478</td>\n      <td>-0.899427</td>\n      <td>0.198008</td>\n      <td>0.513890</td>\n      <td>-0.907336</td>\n      <td>0.188367</td>\n      <td>1.063893</td>\n      <td>0.980229</td>\n      <td>0.921361</td>\n    </tr>\n    <tr>\n      <th>703</th>\n      <td>704</td>\n      <td>2</td>\n      <td>5</td>\n      <td>41</td>\n      <td>0.0052</td>\n      <td>7</td>\n      <td>-3</td>\n      <td>10</td>\n      <td>0.400000</td>\n      <td>43</td>\n      <td>...</td>\n      <td>0.333390</td>\n      <td>0.449958</td>\n      <td>-0.728972</td>\n      <td>0.225431</td>\n      <td>0.503320</td>\n      <td>-0.757707</td>\n      <td>0.204288</td>\n      <td>0.940882</td>\n      <td>0.841130</td>\n      <td>0.893980</td>\n    </tr>\n    <tr>\n      <th>704</th>\n      <td>705</td>\n      <td>3</td>\n      <td>5</td>\n      <td>55</td>\n      <td>NaN</td>\n      <td>8</td>\n      <td>-2</td>\n      <td>15</td>\n      <td>0.600000</td>\n      <td>58</td>\n      <td>...</td>\n      <td>0.283830</td>\n      <td>0.410070</td>\n      <td>-0.778308</td>\n      <td>0.251394</td>\n      <td>0.465678</td>\n      <td>-0.804781</td>\n      <td>0.207592</td>\n      <td>0.691103</td>\n      <td>0.608575</td>\n      <td>0.880586</td>\n    </tr>\n  </tbody>\n</table>\n<p>705 rows × 197 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下面计算移动平均指标\n",
    "feature_cols = list(data.columns)\n",
    "feature_cols.remove('day')\n",
    "feature_cols.remove('index_return')\n",
    "\n",
    "periods = [5, 10, 20]  # 短线操作中常用的均线参数\n",
    "for ft in feature_cols:\n",
    "    for period in periods:\n",
    "        data[f'{ft}_{period}_mean'] = data[ft].rolling(period).mean()  # 每个feature的移动平均\n",
    "        data[f'{ft}_{period}_bias'] = data[ft] / data[f'{ft}_{period}_mean'] - 1  # 每个feature的乖离度\n",
    "        try:\n",
    "            data[f'{ft}_{period}_vol'] = np.sqrt(data[ft].rolling(period).var())  # 每个feature的波动率\n",
    "        except TypeError:\n",
    "            print(data[ft].rolling(period))\n",
    "            break\n",
    "    for c in combinations(periods, 2):\n",
    "        data[f'{ft}_{c[0]}_{c[1]}'] = data[ft].rolling(c[0]).mean() / data[ft].rolling(c[1]).mean()  # 每个feature的移动平均之比\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T12:41:37.954022Z",
     "start_time": "2023-12-14T12:41:37.849526Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1.00 MB\n",
      "Memory usage after optimization is: 0.51 MB\n",
      "Decreased by 49.39%\n"
     ]
    },
    {
     "data": {
      "text/plain": "     day  feature1  feature2  feature3  index_return  feature1_feature2_sum  \\\n0      1         9        59        72        0.0010                     68   \n1      2         5        17        64        0.0002                     22   \n2      3        20        38        69       -0.0029                     58   \n3      4        14        15        66       -0.0115                     29   \n4      5        14        18        61        0.0036                     32   \n..   ...       ...       ...       ...           ...                    ...   \n700  701         3        14        20        0.0092                     17   \n701  702        18        21        46       -0.0010                     39   \n702  703         2         2        42       -0.0049                      4   \n703  704         2         5        41        0.0052                      7   \n704  705         3         5        55           NaN                      8   \n\n     feature1_feature2_difference  feature1_feature2_product  \\\n0                             -50                         19   \n1                             -12                         85   \n2                             -18                         -8   \n3                              -1                        -46   \n4                              -4                         -4   \n..                            ...                        ...   \n700                           -11                         42   \n701                            -3                        122   \n702                             0                          4   \n703                            -3                         10   \n704                            -2                         15   \n\n     feature1_feature2_quotient  feature1_feature3_sum  ...  \\\n0                      0.152542                     81  ...   \n1                      0.294118                     69  ...   \n2                      0.526316                     89  ...   \n3                      0.933333                     80  ...   \n4                      0.777778                     75  ...   \n..                          ...                    ...  ...   \n700                    0.214286                     23  ...   \n701                    0.857143                     64  ...   \n702                    1.000000                     44  ...   \n703                    0.400000                     43  ...   \n704                    0.600000                     58  ...   \n\n     feature2_feature3_quotient_5_vol  feature2_feature3_quotient_10_mean  \\\n0                                 NaN                                 NaN   \n1                                 NaN                                 NaN   \n2                                 NaN                                 NaN   \n3                                 NaN                                 NaN   \n4                            0.251271                                 NaN   \n..                                ...                                 ...   \n700                          0.144241                            0.516044   \n701                          0.147655                            0.510716   \n702                          0.287898                            0.473478   \n703                          0.333390                            0.449958   \n704                          0.283830                            0.410070   \n\n     feature2_feature3_quotient_10_bias  feature2_feature3_quotient_10_vol  \\\n0                                   NaN                                NaN   \n1                                   NaN                                NaN   \n2                                   NaN                                NaN   \n3                                   NaN                                NaN   \n4                                   NaN                                NaN   \n..                                  ...                                ...   \n700                            0.356474                           0.132196   \n701                           -0.106114                           0.133543   \n702                           -0.899427                           0.198008   \n703                           -0.728972                           0.225431   \n704                           -0.778308                           0.251394   \n\n     feature2_feature3_quotient_20_mean  feature2_feature3_quotient_20_bias  \\\n0                                   NaN                                 NaN   \n1                                   NaN                                 NaN   \n2                                   NaN                                 NaN   \n3                                   NaN                                 NaN   \n4                                   NaN                                 NaN   \n..                                  ...                                 ...   \n700                            0.525114                            0.333044   \n701                            0.535120                           -0.146879   \n702                            0.513889                           -0.907336   \n703                            0.503320                           -0.757707   \n704                            0.465678                           -0.804781   \n\n     feature2_feature3_quotient_20_vol  feature2_feature3_quotient_5_10  \\\n0                                  NaN                              NaN   \n1                                  NaN                              NaN   \n2                                  NaN                              NaN   \n3                                  NaN                              NaN   \n4                                  NaN                              NaN   \n..                                 ...                              ...   \n700                           0.165271                         1.148872   \n701                           0.153807                         1.154709   \n702                           0.188367                         1.063893   \n703                           0.204288                         0.940882   \n704                           0.207592                         0.691103   \n\n     feature2_feature3_quotient_5_20  feature2_feature3_quotient_10_20  \n0                                NaN                               NaN  \n1                                NaN                               NaN  \n2                                NaN                               NaN  \n3                                NaN                               NaN  \n4                                NaN                               NaN  \n..                               ...                               ...  \n700                         1.129027                          0.982727  \n701                         1.102049                          0.954395  \n702                         0.980229                          0.921361  \n703                         0.841130                          0.893980  \n704                         0.608575                          0.880586  \n\n[705 rows x 197 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day</th>\n      <th>feature1</th>\n      <th>feature2</th>\n      <th>feature3</th>\n      <th>index_return</th>\n      <th>feature1_feature2_sum</th>\n      <th>feature1_feature2_difference</th>\n      <th>feature1_feature2_product</th>\n      <th>feature1_feature2_quotient</th>\n      <th>feature1_feature3_sum</th>\n      <th>...</th>\n      <th>feature2_feature3_quotient_5_vol</th>\n      <th>feature2_feature3_quotient_10_mean</th>\n      <th>feature2_feature3_quotient_10_bias</th>\n      <th>feature2_feature3_quotient_10_vol</th>\n      <th>feature2_feature3_quotient_20_mean</th>\n      <th>feature2_feature3_quotient_20_bias</th>\n      <th>feature2_feature3_quotient_20_vol</th>\n      <th>feature2_feature3_quotient_5_10</th>\n      <th>feature2_feature3_quotient_5_20</th>\n      <th>feature2_feature3_quotient_10_20</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>9</td>\n      <td>59</td>\n      <td>72</td>\n      <td>0.0010</td>\n      <td>68</td>\n      <td>-50</td>\n      <td>19</td>\n      <td>0.152542</td>\n      <td>81</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>5</td>\n      <td>17</td>\n      <td>64</td>\n      <td>0.0002</td>\n      <td>22</td>\n      <td>-12</td>\n      <td>85</td>\n      <td>0.294118</td>\n      <td>69</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>20</td>\n      <td>38</td>\n      <td>69</td>\n      <td>-0.0029</td>\n      <td>58</td>\n      <td>-18</td>\n      <td>-8</td>\n      <td>0.526316</td>\n      <td>89</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>14</td>\n      <td>15</td>\n      <td>66</td>\n      <td>-0.0115</td>\n      <td>29</td>\n      <td>-1</td>\n      <td>-46</td>\n      <td>0.933333</td>\n      <td>80</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>14</td>\n      <td>18</td>\n      <td>61</td>\n      <td>0.0036</td>\n      <td>32</td>\n      <td>-4</td>\n      <td>-4</td>\n      <td>0.777778</td>\n      <td>75</td>\n      <td>...</td>\n      <td>0.251271</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>700</th>\n      <td>701</td>\n      <td>3</td>\n      <td>14</td>\n      <td>20</td>\n      <td>0.0092</td>\n      <td>17</td>\n      <td>-11</td>\n      <td>42</td>\n      <td>0.214286</td>\n      <td>23</td>\n      <td>...</td>\n      <td>0.144241</td>\n      <td>0.516044</td>\n      <td>0.356474</td>\n      <td>0.132196</td>\n      <td>0.525114</td>\n      <td>0.333044</td>\n      <td>0.165271</td>\n      <td>1.148872</td>\n      <td>1.129027</td>\n      <td>0.982727</td>\n    </tr>\n    <tr>\n      <th>701</th>\n      <td>702</td>\n      <td>18</td>\n      <td>21</td>\n      <td>46</td>\n      <td>-0.0010</td>\n      <td>39</td>\n      <td>-3</td>\n      <td>122</td>\n      <td>0.857143</td>\n      <td>64</td>\n      <td>...</td>\n      <td>0.147655</td>\n      <td>0.510716</td>\n      <td>-0.106114</td>\n      <td>0.133543</td>\n      <td>0.535120</td>\n      <td>-0.146879</td>\n      <td>0.153807</td>\n      <td>1.154709</td>\n      <td>1.102049</td>\n      <td>0.954395</td>\n    </tr>\n    <tr>\n      <th>702</th>\n      <td>703</td>\n      <td>2</td>\n      <td>2</td>\n      <td>42</td>\n      <td>-0.0049</td>\n      <td>4</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1.000000</td>\n      <td>44</td>\n      <td>...</td>\n      <td>0.287898</td>\n      <td>0.473478</td>\n      <td>-0.899427</td>\n      <td>0.198008</td>\n      <td>0.513889</td>\n      <td>-0.907336</td>\n      <td>0.188367</td>\n      <td>1.063893</td>\n      <td>0.980229</td>\n      <td>0.921361</td>\n    </tr>\n    <tr>\n      <th>703</th>\n      <td>704</td>\n      <td>2</td>\n      <td>5</td>\n      <td>41</td>\n      <td>0.0052</td>\n      <td>7</td>\n      <td>-3</td>\n      <td>10</td>\n      <td>0.400000</td>\n      <td>43</td>\n      <td>...</td>\n      <td>0.333390</td>\n      <td>0.449958</td>\n      <td>-0.728972</td>\n      <td>0.225431</td>\n      <td>0.503320</td>\n      <td>-0.757707</td>\n      <td>0.204288</td>\n      <td>0.940882</td>\n      <td>0.841130</td>\n      <td>0.893980</td>\n    </tr>\n    <tr>\n      <th>704</th>\n      <td>705</td>\n      <td>3</td>\n      <td>5</td>\n      <td>55</td>\n      <td>NaN</td>\n      <td>8</td>\n      <td>-2</td>\n      <td>15</td>\n      <td>0.600000</td>\n      <td>58</td>\n      <td>...</td>\n      <td>0.283830</td>\n      <td>0.410070</td>\n      <td>-0.778308</td>\n      <td>0.251394</td>\n      <td>0.465678</td>\n      <td>-0.804781</td>\n      <td>0.207592</td>\n      <td>0.691103</td>\n      <td>0.608575</td>\n      <td>0.880586</td>\n    </tr>\n  </tbody>\n</table>\n<p>705 rows × 197 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = reduce_mem_usage(data, verbose=1)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T12:41:38.031109Z",
     "start_time": "2023-12-14T12:41:37.893791Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 缩尾（4%）并标准化"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "     day  feature1  feature2  feature3  index_return  feature1_feature2_sum  \\\n0      1 -0.466642  1.317203  1.357251      0.117669               0.942208   \n1      2 -0.615909 -0.452524  0.865544      0.038288               0.008084   \n2      3 -0.056160  0.432339  1.172861     -0.269313               0.739137   \n3      4 -0.280059 -0.536797  0.988470     -1.122658               0.150233   \n4      5 -0.280059 -0.410388  0.681154      0.375657               0.211154   \n..   ...       ...       ...       ...           ...                    ...   \n700  701 -0.690542 -0.578933 -1.838845      0.931324              -0.093452   \n701  702 -0.130793 -0.283979 -0.240797     -0.080783               0.353303   \n702  703 -0.727858 -1.084569 -0.486650     -0.467765              -0.357443   \n703  704 -0.727858 -0.958160 -0.548114      0.534419              -0.296522   \n704  705 -0.690542 -0.958160  0.312373           NaN              -0.276215   \n\n     feature1_feature2_difference  feature1_feature2_product  \\\n0                       -1.920112                   0.181151   \n1                       -0.358175                   1.200372   \n2                       -0.748659                  -0.235803   \n3                        0.357713                  -0.822627   \n4                        0.162471                  -0.174032   \n..                            ...                        ...   \n700                     -0.293094                   0.536334   \n701                      0.227552                   1.617326   \n702                      0.422794                  -0.050490   \n703                      0.227552                   0.042166   \n704                      0.292632                   0.119380   \n\n     feature1_feature2_quotient  feature1_feature3_sum  ...  \\\n0                     -0.650135               0.527213  ...   \n1                     -0.525788               0.040429  ...   \n2                     -0.321847               0.851735  ...   \n3                      0.035639               0.486647  ...   \n4                     -0.100986               0.283821  ...   \n..                          ...                    ...  ...   \n700                   -0.595905              -1.622747  ...   \n701                   -0.031279              -0.162397  ...   \n702                    0.094193              -0.973702  ...   \n703                   -0.432791              -1.014268  ...   \n704                   -0.257130              -0.405789  ...   \n\n     feature2_feature3_quotient_5_vol  feature2_feature3_quotient_10_mean  \\\n0                                 NaN                                 NaN   \n1                                 NaN                                 NaN   \n2                                 NaN                                 NaN   \n3                                 NaN                                 NaN   \n4                           -0.053922                                 NaN   \n..                                ...                                 ...   \n700                         -0.339185                           -0.239254   \n701                         -0.330087                           -0.245885   \n702                          0.043699                           -0.292229   \n703                          0.164947                           -0.321500   \n704                          0.032856                           -0.371143   \n\n     feature2_feature3_quotient_10_bias  feature2_feature3_quotient_10_vol  \\\n0                                   NaN                                NaN   \n1                                   NaN                                NaN   \n2                                   NaN                                NaN   \n3                                   NaN                                NaN   \n4                                   NaN                                NaN   \n..                                  ...                                ...   \n700                            0.494242                          -0.476520   \n701                           -0.236152                          -0.473627   \n702                           -1.455313                          -0.335132   \n703                           -1.219603                          -0.276219   \n704                           -1.297501                          -0.220441   \n\n     feature2_feature3_quotient_20_mean  feature2_feature3_quotient_20_bias  \\\n0                                   NaN                                 NaN   \n1                                   NaN                                 NaN   \n2                                   NaN                                 NaN   \n3                                   NaN                                 NaN   \n4                                   NaN                                 NaN   \n..                                  ...                                 ...   \n700                           -0.237888                            0.289437   \n701                           -0.225353                           -0.264600   \n702                           -0.251949                           -1.142495   \n703                           -0.265190                           -0.969758   \n704                           -0.312347                           -1.024103   \n\n     feature2_feature3_quotient_20_vol  feature2_feature3_quotient_5_10  \\\n0                                  NaN                              NaN   \n1                                  NaN                              NaN   \n2                                  NaN                              NaN   \n3                                  NaN                              NaN   \n4                                  NaN                              NaN   \n..                                 ...                              ...   \n700                          -0.397931                         0.432678   \n701                          -0.409356                         0.450037   \n702                          -0.374912                         0.179975   \n703                          -0.359044                        -0.185826   \n704                          -0.355752                        -0.928604   \n\n     feature2_feature3_quotient_5_20  feature2_feature3_quotient_10_20  \n0                                NaN                               NaN  \n1                                NaN                               NaN  \n2                                NaN                               NaN  \n3                                NaN                               NaN  \n4                                NaN                               NaN  \n..                               ...                               ...  \n700                         0.141660                         -0.059595  \n701                         0.097192                         -0.132994  \n702                        -0.103598                         -0.218575  \n703                        -0.332869                         -0.289509  \n704                        -0.716179                         -0.324210  \n\n[705 rows x 197 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day</th>\n      <th>feature1</th>\n      <th>feature2</th>\n      <th>feature3</th>\n      <th>index_return</th>\n      <th>feature1_feature2_sum</th>\n      <th>feature1_feature2_difference</th>\n      <th>feature1_feature2_product</th>\n      <th>feature1_feature2_quotient</th>\n      <th>feature1_feature3_sum</th>\n      <th>...</th>\n      <th>feature2_feature3_quotient_5_vol</th>\n      <th>feature2_feature3_quotient_10_mean</th>\n      <th>feature2_feature3_quotient_10_bias</th>\n      <th>feature2_feature3_quotient_10_vol</th>\n      <th>feature2_feature3_quotient_20_mean</th>\n      <th>feature2_feature3_quotient_20_bias</th>\n      <th>feature2_feature3_quotient_20_vol</th>\n      <th>feature2_feature3_quotient_5_10</th>\n      <th>feature2_feature3_quotient_5_20</th>\n      <th>feature2_feature3_quotient_10_20</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>-0.466642</td>\n      <td>1.317203</td>\n      <td>1.357251</td>\n      <td>0.117669</td>\n      <td>0.942208</td>\n      <td>-1.920112</td>\n      <td>0.181151</td>\n      <td>-0.650135</td>\n      <td>0.527213</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>-0.615909</td>\n      <td>-0.452524</td>\n      <td>0.865544</td>\n      <td>0.038288</td>\n      <td>0.008084</td>\n      <td>-0.358175</td>\n      <td>1.200372</td>\n      <td>-0.525788</td>\n      <td>0.040429</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>-0.056160</td>\n      <td>0.432339</td>\n      <td>1.172861</td>\n      <td>-0.269313</td>\n      <td>0.739137</td>\n      <td>-0.748659</td>\n      <td>-0.235803</td>\n      <td>-0.321847</td>\n      <td>0.851735</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>-0.280059</td>\n      <td>-0.536797</td>\n      <td>0.988470</td>\n      <td>-1.122658</td>\n      <td>0.150233</td>\n      <td>0.357713</td>\n      <td>-0.822627</td>\n      <td>0.035639</td>\n      <td>0.486647</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>-0.280059</td>\n      <td>-0.410388</td>\n      <td>0.681154</td>\n      <td>0.375657</td>\n      <td>0.211154</td>\n      <td>0.162471</td>\n      <td>-0.174032</td>\n      <td>-0.100986</td>\n      <td>0.283821</td>\n      <td>...</td>\n      <td>-0.053922</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>700</th>\n      <td>701</td>\n      <td>-0.690542</td>\n      <td>-0.578933</td>\n      <td>-1.838845</td>\n      <td>0.931324</td>\n      <td>-0.093452</td>\n      <td>-0.293094</td>\n      <td>0.536334</td>\n      <td>-0.595905</td>\n      <td>-1.622747</td>\n      <td>...</td>\n      <td>-0.339185</td>\n      <td>-0.239254</td>\n      <td>0.494242</td>\n      <td>-0.476520</td>\n      <td>-0.237888</td>\n      <td>0.289437</td>\n      <td>-0.397931</td>\n      <td>0.432678</td>\n      <td>0.141660</td>\n      <td>-0.059595</td>\n    </tr>\n    <tr>\n      <th>701</th>\n      <td>702</td>\n      <td>-0.130793</td>\n      <td>-0.283979</td>\n      <td>-0.240797</td>\n      <td>-0.080783</td>\n      <td>0.353303</td>\n      <td>0.227552</td>\n      <td>1.617326</td>\n      <td>-0.031279</td>\n      <td>-0.162397</td>\n      <td>...</td>\n      <td>-0.330087</td>\n      <td>-0.245885</td>\n      <td>-0.236152</td>\n      <td>-0.473627</td>\n      <td>-0.225353</td>\n      <td>-0.264600</td>\n      <td>-0.409356</td>\n      <td>0.450037</td>\n      <td>0.097192</td>\n      <td>-0.132994</td>\n    </tr>\n    <tr>\n      <th>702</th>\n      <td>703</td>\n      <td>-0.727858</td>\n      <td>-1.084569</td>\n      <td>-0.486650</td>\n      <td>-0.467765</td>\n      <td>-0.357443</td>\n      <td>0.422794</td>\n      <td>-0.050490</td>\n      <td>0.094193</td>\n      <td>-0.973702</td>\n      <td>...</td>\n      <td>0.043699</td>\n      <td>-0.292229</td>\n      <td>-1.455313</td>\n      <td>-0.335132</td>\n      <td>-0.251949</td>\n      <td>-1.142495</td>\n      <td>-0.374912</td>\n      <td>0.179975</td>\n      <td>-0.103598</td>\n      <td>-0.218575</td>\n    </tr>\n    <tr>\n      <th>703</th>\n      <td>704</td>\n      <td>-0.727858</td>\n      <td>-0.958160</td>\n      <td>-0.548114</td>\n      <td>0.534419</td>\n      <td>-0.296522</td>\n      <td>0.227552</td>\n      <td>0.042166</td>\n      <td>-0.432791</td>\n      <td>-1.014268</td>\n      <td>...</td>\n      <td>0.164947</td>\n      <td>-0.321500</td>\n      <td>-1.219603</td>\n      <td>-0.276219</td>\n      <td>-0.265190</td>\n      <td>-0.969758</td>\n      <td>-0.359044</td>\n      <td>-0.185826</td>\n      <td>-0.332869</td>\n      <td>-0.289509</td>\n    </tr>\n    <tr>\n      <th>704</th>\n      <td>705</td>\n      <td>-0.690542</td>\n      <td>-0.958160</td>\n      <td>0.312373</td>\n      <td>NaN</td>\n      <td>-0.276215</td>\n      <td>0.292632</td>\n      <td>0.119380</td>\n      <td>-0.257130</td>\n      <td>-0.405789</td>\n      <td>...</td>\n      <td>0.032856</td>\n      <td>-0.371143</td>\n      <td>-1.297501</td>\n      <td>-0.220441</td>\n      <td>-0.312347</td>\n      <td>-1.024103</td>\n      <td>-0.355752</td>\n      <td>-0.928604</td>\n      <td>-0.716179</td>\n      <td>-0.324210</td>\n    </tr>\n  </tbody>\n</table>\n<p>705 rows × 197 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "feature_cols = list(data.columns)\n",
    "feature_cols.remove('day')\n",
    "feature_cols.remove('index_return')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data[feature_cols + ['index_return']] = data[feature_cols + ['index_return']].clip(lower=data[feature_cols + ['index_return']].quantile(0.04), upper=data[feature_cols + ['index_return']].quantile(0.96), axis=1)\n",
    "data[feature_cols + ['index_return']] = pd.DataFrame(scaler.fit_transform(data[feature_cols + ['index_return']]), columns=[feature_cols + ['index_return']])\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T12:41:38.057003Z",
     "start_time": "2023-12-14T12:41:37.919157Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 使用LightGBM进行表格数据预测并解释特征重要性"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-14 20:41:37,990] A new study created in memory with name: no-name-3248e342-f401-4c2a-9157-05ef1f3c4ba6\n",
      "[I 2023-12-14 20:41:38,096] Trial 0 finished with value: 0.902107184956533 and parameters: {'lambda_l1': 3.3847725629400265, 'lambda_l2': 3.580268884191221e-06, 'num_leaves': 121, 'feature_fraction': 0.7405563668038072, 'bagging_fraction': 0.6852214142685529, 'bagging_freq': 1, 'min_child_samples': 78}. Best is trial 0 with value: 0.902107184956533.\n",
      "[I 2023-12-14 20:41:38,142] Trial 1 finished with value: 0.908941373372138 and parameters: {'lambda_l1': 3.124310101425691e-08, 'lambda_l2': 3.4457920921459404e-05, 'num_leaves': 28, 'feature_fraction': 0.9238810427278918, 'bagging_fraction': 0.5584518539380834, 'bagging_freq': 6, 'min_child_samples': 74}. Best is trial 0 with value: 0.902107184956533.\n",
      "[I 2023-12-14 20:41:38,187] Trial 2 finished with value: 0.9016977254964055 and parameters: {'lambda_l1': 7.785541430295969e-08, 'lambda_l2': 5.7822325262669735e-08, 'num_leaves': 10, 'feature_fraction': 0.7910893109064675, 'bagging_fraction': 0.6368710451692484, 'bagging_freq': 1, 'min_child_samples': 91}. Best is trial 2 with value: 0.9016977254964055.\n",
      "[I 2023-12-14 20:41:38,526] Trial 3 finished with value: 0.9793382987728797 and parameters: {'lambda_l1': 0.10017529426059647, 'lambda_l2': 2.4306410912775303e-05, 'num_leaves': 235, 'feature_fraction': 0.7022726347193201, 'bagging_fraction': 0.6982246169961471, 'bagging_freq': 2, 'min_child_samples': 12}. Best is trial 2 with value: 0.9016977254964055.\n",
      "[I 2023-12-14 20:41:38,629] Trial 4 finished with value: 1.0265271929528788 and parameters: {'lambda_l1': 1.9532827081944744, 'lambda_l2': 0.00863496422914562, 'num_leaves': 132, 'feature_fraction': 0.9201139405537093, 'bagging_fraction': 0.41332442593343444, 'bagging_freq': 5, 'min_child_samples': 29}. Best is trial 2 with value: 0.9016977254964055.\n",
      "[I 2023-12-14 20:41:38,662] Trial 5 finished with value: 0.87451748023708 and parameters: {'lambda_l1': 0.015078598051795527, 'lambda_l2': 0.003481945385184526, 'num_leaves': 201, 'feature_fraction': 0.6864995853781716, 'bagging_fraction': 0.48470072881961607, 'bagging_freq': 6, 'min_child_samples': 90}. Best is trial 5 with value: 0.87451748023708.\n",
      "[I 2023-12-14 20:41:38,735] Trial 6 finished with value: 1.0146402274098658 and parameters: {'lambda_l1': 1.9887675298367555, 'lambda_l2': 0.0011966511873972234, 'num_leaves': 143, 'feature_fraction': 0.8551066673014263, 'bagging_fraction': 0.5333391198675476, 'bagging_freq': 2, 'min_child_samples': 49}. Best is trial 5 with value: 0.87451748023708.\n",
      "[I 2023-12-14 20:41:38,918] Trial 7 finished with value: 1.0444058188811502 and parameters: {'lambda_l1': 0.00013050475607800352, 'lambda_l2': 6.0585695567150186e-05, 'num_leaves': 231, 'feature_fraction': 0.533262069385646, 'bagging_fraction': 0.4522019907090232, 'bagging_freq': 1, 'min_child_samples': 13}. Best is trial 5 with value: 0.87451748023708.\n",
      "[I 2023-12-14 20:41:39,009] Trial 8 finished with value: 1.0027639112961635 and parameters: {'lambda_l1': 6.018143979480086e-08, 'lambda_l2': 3.842115663181621e-05, 'num_leaves': 150, 'feature_fraction': 0.6437779715467171, 'bagging_fraction': 0.7440161832518457, 'bagging_freq': 3, 'min_child_samples': 49}. Best is trial 5 with value: 0.87451748023708.\n",
      "[I 2023-12-14 20:41:39,062] Trial 9 finished with value: 0.9761016513156426 and parameters: {'lambda_l1': 2.5882860174415815e-07, 'lambda_l2': 2.2923625233693168e-05, 'num_leaves': 197, 'feature_fraction': 0.6704598805187088, 'bagging_fraction': 0.41196414409208354, 'bagging_freq': 4, 'min_child_samples': 40}. Best is trial 5 with value: 0.87451748023708.\n",
      "[I 2023-12-14 20:41:39,119] Trial 10 finished with value: 0.9460356449419086 and parameters: {'lambda_l1': 0.0011733044121055663, 'lambda_l2': 6.252815981923532, 'num_leaves': 74, 'feature_fraction': 0.4604212604351021, 'bagging_fraction': 0.9224845812689171, 'bagging_freq': 7, 'min_child_samples': 99}. Best is trial 5 with value: 0.87451748023708.\n",
      "[I 2023-12-14 20:41:39,169] Trial 11 finished with value: 0.9113237788834037 and parameters: {'lambda_l1': 7.939896100408862e-06, 'lambda_l2': 1.3910416816695336e-08, 'num_leaves': 26, 'feature_fraction': 0.8055614259901573, 'bagging_fraction': 0.6103460118420152, 'bagging_freq': 5, 'min_child_samples': 99}. Best is trial 5 with value: 0.87451748023708.\n",
      "[I 2023-12-14 20:41:39,242] Trial 12 finished with value: 0.9876789815250665 and parameters: {'lambda_l1': 0.006683995448887612, 'lambda_l2': 0.06075523102465638, 'num_leaves': 185, 'feature_fraction': 0.5619941350115927, 'bagging_fraction': 0.8173318510419021, 'bagging_freq': 7, 'min_child_samples': 75}. Best is trial 5 with value: 0.87451748023708.\n",
      "[I 2023-12-14 20:41:39,291] Trial 13 finished with value: 0.8948274985571711 and parameters: {'lambda_l1': 4.062584288591778e-06, 'lambda_l2': 1.3581735588276082e-08, 'num_leaves': 68, 'feature_fraction': 0.8061413161229598, 'bagging_fraction': 0.5065052373388316, 'bagging_freq': 4, 'min_child_samples': 83}. Best is trial 5 with value: 0.87451748023708.\n",
      "[I 2023-12-14 20:41:39,355] Trial 14 finished with value: 0.8841642307208171 and parameters: {'lambda_l1': 7.954815304767265e-06, 'lambda_l2': 4.845254736494493e-07, 'num_leaves': 82, 'feature_fraction': 0.8452305105424525, 'bagging_fraction': 0.5100852950955833, 'bagging_freq': 4, 'min_child_samples': 63}. Best is trial 5 with value: 0.87451748023708.\n",
      "[I 2023-12-14 20:41:39,413] Trial 15 finished with value: 0.9411107991670524 and parameters: {'lambda_l1': 3.9066508132219246e-05, 'lambda_l2': 5.809353106393782e-07, 'num_leaves': 89, 'feature_fraction': 0.9841763228447932, 'bagging_fraction': 0.4972507829906045, 'bagging_freq': 5, 'min_child_samples': 62}. Best is trial 5 with value: 0.87451748023708.\n",
      "[I 2023-12-14 20:41:39,480] Trial 16 finished with value: 0.9735584574589288 and parameters: {'lambda_l1': 0.01377679042447928, 'lambda_l2': 0.41595448322449524, 'num_leaves': 182, 'feature_fraction': 0.598248006457575, 'bagging_fraction': 0.6084390010318627, 'bagging_freq': 6, 'min_child_samples': 65}. Best is trial 5 with value: 0.87451748023708.\n",
      "[I 2023-12-14 20:41:39,572] Trial 17 finished with value: 1.0196859396617564 and parameters: {'lambda_l1': 2.11023610858558e-06, 'lambda_l2': 0.0010717499391061799, 'num_leaves': 100, 'feature_fraction': 0.4393829508948906, 'bagging_fraction': 0.9824482425283834, 'bagging_freq': 3, 'min_child_samples': 66}. Best is trial 5 with value: 0.87451748023708.\n",
      "[I 2023-12-14 20:41:39,649] Trial 18 finished with value: 0.9666073284332747 and parameters: {'lambda_l1': 0.0018064858417514028, 'lambda_l2': 5.269995128622781e-07, 'num_leaves': 52, 'feature_fraction': 0.8824112490514222, 'bagging_fraction': 0.7970588720857522, 'bagging_freq': 6, 'min_child_samples': 87}. Best is trial 5 with value: 0.87451748023708.\n",
      "[I 2023-12-14 20:41:39,777] Trial 19 finished with value: 0.9732213193838564 and parameters: {'lambda_l1': 0.055815657071605296, 'lambda_l2': 0.02224833340816613, 'num_leaves': 164, 'feature_fraction': 0.7631831888802253, 'bagging_fraction': 0.5659650528168719, 'bagging_freq': 3, 'min_child_samples': 31}. Best is trial 5 with value: 0.87451748023708.\n",
      "[I 2023-12-14 20:41:39,883] Trial 20 finished with value: 0.9551089919130658 and parameters: {'lambda_l1': 0.00011912466114552795, 'lambda_l2': 0.00036334539955721213, 'num_leaves': 208, 'feature_fraction': 0.9810295468806097, 'bagging_fraction': 0.46478397044725683, 'bagging_freq': 4, 'min_child_samples': 57}. Best is trial 5 with value: 0.87451748023708.\n",
      "[I 2023-12-14 20:41:39,928] Trial 21 finished with value: 0.8871347951099079 and parameters: {'lambda_l1': 2.1030037193555265e-06, 'lambda_l2': 6.559056776563083e-08, 'num_leaves': 65, 'feature_fraction': 0.8233790855280676, 'bagging_fraction': 0.5044483414024339, 'bagging_freq': 4, 'min_child_samples': 85}. Best is trial 5 with value: 0.87451748023708.\n",
      "[I 2023-12-14 20:41:39,966] Trial 22 finished with value: 0.8710976824370373 and parameters: {'lambda_l1': 1.3526377191375002e-05, 'lambda_l2': 2.3571721867443586e-07, 'num_leaves': 109, 'feature_fraction': 0.8550850944739606, 'bagging_fraction': 0.47361137043219115, 'bagging_freq': 4, 'min_child_samples': 89}. Best is trial 22 with value: 0.8710976824370373.\n",
      "[I 2023-12-14 20:41:40,002] Trial 23 finished with value: 0.8777034501995076 and parameters: {'lambda_l1': 2.4889263647462584e-05, 'lambda_l2': 1.9051272340300145e-06, 'num_leaves': 109, 'feature_fraction': 0.7284871248607035, 'bagging_fraction': 0.45330650910536385, 'bagging_freq': 5, 'min_child_samples': 94}. Best is trial 22 with value: 0.8710976824370373.\n",
      "[I 2023-12-14 20:41:40,040] Trial 24 finished with value: 0.8816609435231632 and parameters: {'lambda_l1': 0.0005938466606499357, 'lambda_l2': 3.3020739756194316e-06, 'num_leaves': 111, 'feature_fraction': 0.7225144120697491, 'bagging_fraction': 0.4539228015706994, 'bagging_freq': 5, 'min_child_samples': 95}. Best is trial 22 with value: 0.8710976824370373.\n",
      "[I 2023-12-14 20:41:40,080] Trial 25 finished with value: 0.905318555143819 and parameters: {'lambda_l1': 3.3364124898549125e-05, 'lambda_l2': 3.151746140186996e-06, 'num_leaves': 170, 'feature_fraction': 0.6493423894687385, 'bagging_fraction': 0.4049365552953099, 'bagging_freq': 6, 'min_child_samples': 91}. Best is trial 22 with value: 0.8710976824370373.\n",
      "[I 2023-12-14 20:41:40,146] Trial 26 finished with value: 0.8778766411356093 and parameters: {'lambda_l1': 0.17023957309891352, 'lambda_l2': 0.003693076654494262, 'num_leaves': 214, 'feature_fraction': 0.595708320001276, 'bagging_fraction': 0.5596370008222212, 'bagging_freq': 7, 'min_child_samples': 80}. Best is trial 22 with value: 0.8710976824370373.\n",
      "[I 2023-12-14 20:41:40,225] Trial 27 finished with value: 0.9722661146496626 and parameters: {'lambda_l1': 5.533918301509026e-07, 'lambda_l2': 0.18364298775288432, 'num_leaves': 122, 'feature_fraction': 0.7581630202383672, 'bagging_fraction': 0.6472430805818928, 'bagging_freq': 5, 'min_child_samples': 74}. Best is trial 22 with value: 0.8710976824370373.\n",
      "[I 2023-12-14 20:41:40,271] Trial 28 finished with value: 0.8829312239531599 and parameters: {'lambda_l1': 0.010532833885543736, 'lambda_l2': 0.00018057325641311509, 'num_leaves': 104, 'feature_fraction': 0.6743168975554988, 'bagging_fraction': 0.45604378501413656, 'bagging_freq': 6, 'min_child_samples': 90}. Best is trial 22 with value: 0.8710976824370373.\n",
      "[I 2023-12-14 20:41:40,338] Trial 29 finished with value: 0.8871852524987948 and parameters: {'lambda_l1': 3.362660173100015e-05, 'lambda_l2': 5.949893287092835e-06, 'num_leaves': 134, 'feature_fraction': 0.7194207879090437, 'bagging_fraction': 0.599923688446091, 'bagging_freq': 5, 'min_child_samples': 79}. Best is trial 22 with value: 0.8710976824370373.\n",
      "[I 2023-12-14 20:41:40,383] Trial 30 finished with value: 0.8519177202952362 and parameters: {'lambda_l1': 0.00011588994440319826, 'lambda_l2': 1.1504474904544954e-07, 'num_leaves': 159, 'feature_fraction': 0.5128676616127086, 'bagging_fraction': 0.475459671457479, 'bagging_freq': 3, 'min_child_samples': 99}. Best is trial 30 with value: 0.8519177202952362.\n",
      "[I 2023-12-14 20:41:40,424] Trial 31 finished with value: 0.8585743504458792 and parameters: {'lambda_l1': 0.00016367448328188287, 'lambda_l2': 1.1439657447104137e-07, 'num_leaves': 157, 'feature_fraction': 0.5031393754958776, 'bagging_fraction': 0.4697458959504328, 'bagging_freq': 3, 'min_child_samples': 99}. Best is trial 30 with value: 0.8519177202952362.\n",
      "[I 2023-12-14 20:41:40,477] Trial 32 finished with value: 0.8880913136770597 and parameters: {'lambda_l1': 0.0003177072456467984, 'lambda_l2': 8.573946918620782e-08, 'num_leaves': 256, 'feature_fraction': 0.4933626231832298, 'bagging_fraction': 0.5473638883574202, 'bagging_freq': 2, 'min_child_samples': 99}. Best is trial 30 with value: 0.8519177202952362.\n",
      "[I 2023-12-14 20:41:40,526] Trial 33 finished with value: 0.8794654217351864 and parameters: {'lambda_l1': 0.0027384931879988208, 'lambda_l2': 2.0610237765700555e-07, 'num_leaves': 158, 'feature_fraction': 0.490192773399816, 'bagging_fraction': 0.48526472233000645, 'bagging_freq': 3, 'min_child_samples': 86}. Best is trial 30 with value: 0.8519177202952362.\n",
      "[I 2023-12-14 20:41:40,581] Trial 34 finished with value: 0.8858890957912847 and parameters: {'lambda_l1': 0.00013096763942244957, 'lambda_l2': 5.332198659342652e-08, 'num_leaves': 178, 'feature_fraction': 0.5311440295423018, 'bagging_fraction': 0.6599791865716257, 'bagging_freq': 3, 'min_child_samples': 94}. Best is trial 30 with value: 0.8519177202952362.\n",
      "[I 2023-12-14 20:41:40,641] Trial 35 finished with value: 0.9520296134663362 and parameters: {'lambda_l1': 0.3400122106307351, 'lambda_l2': 1.1929140347706703e-08, 'num_leaves': 146, 'feature_fraction': 0.5858002941345728, 'bagging_fraction': 0.5817357173456511, 'bagging_freq': 2, 'min_child_samples': 77}. Best is trial 30 with value: 0.8519177202952362.\n",
      "[I 2023-12-14 20:41:40,689] Trial 36 finished with value: 0.8654122749707636 and parameters: {'lambda_l1': 0.023996923122823095, 'lambda_l2': 1.9992752699063567e-07, 'num_leaves': 195, 'feature_fraction': 0.43482908262420544, 'bagging_fraction': 0.5372044257219382, 'bagging_freq': 3, 'min_child_samples': 100}. Best is trial 30 with value: 0.8519177202952362.\n",
      "[I 2023-12-14 20:41:40,737] Trial 37 finished with value: 0.8296021212596943 and parameters: {'lambda_l1': 8.863586865837645, 'lambda_l2': 1.56295231926178e-07, 'num_leaves': 221, 'feature_fraction': 0.4043413904538567, 'bagging_fraction': 0.5373830946165632, 'bagging_freq': 3, 'min_child_samples': 99}. Best is trial 37 with value: 0.8296021212596943.\n",
      "[I 2023-12-14 20:41:40,866] Trial 38 finished with value: 0.8665241200149969 and parameters: {'lambda_l1': 6.3231477108304475, 'lambda_l2': 1.015188944057494e-06, 'num_leaves': 225, 'feature_fraction': 0.40231474585630095, 'bagging_fraction': 0.5343870660047102, 'bagging_freq': 1, 'min_child_samples': 71}. Best is trial 37 with value: 0.8296021212596943.\n",
      "[I 2023-12-14 20:41:40,931] Trial 39 finished with value: 0.8924973011455419 and parameters: {'lambda_l1': 0.48161033672325343, 'lambda_l2': 9.61776534093396e-06, 'num_leaves': 249, 'feature_fraction': 0.4208282270266594, 'bagging_fraction': 0.6841356803363166, 'bagging_freq': 2, 'min_child_samples': 98}. Best is trial 37 with value: 0.8296021212596943.\n",
      "[I 2023-12-14 20:41:41,055] Trial 40 finished with value: 1.0068708938403303 and parameters: {'lambda_l1': 0.05447770619326391, 'lambda_l2': 3.4108843874472504e-08, 'num_leaves': 193, 'feature_fraction': 0.4879022547138599, 'bagging_fraction': 0.4349946003170283, 'bagging_freq': 3, 'min_child_samples': 17}. Best is trial 37 with value: 0.8296021212596943.\n",
      "[I 2023-12-14 20:41:41,107] Trial 41 finished with value: 0.8680635531378906 and parameters: {'lambda_l1': 6.996935275749082, 'lambda_l2': 1.6389669186676063e-06, 'num_leaves': 219, 'feature_fraction': 0.41525193788864845, 'bagging_fraction': 0.5319996559105449, 'bagging_freq': 1, 'min_child_samples': 70}. Best is trial 37 with value: 0.8296021212596943.\n",
      "[I 2023-12-14 20:41:41,148] Trial 42 finished with value: 0.8617261163736626 and parameters: {'lambda_l1': 1.9963524977731055, 'lambda_l2': 1.561648980794011e-07, 'num_leaves': 229, 'feature_fraction': 0.4020845344897972, 'bagging_fraction': 0.5297893379274642, 'bagging_freq': 1, 'min_child_samples': 95}. Best is trial 37 with value: 0.8296021212596943.\n",
      "[I 2023-12-14 20:41:41,211] Trial 43 finished with value: 0.9293106299845607 and parameters: {'lambda_l1': 0.7591592862475781, 'lambda_l2': 1.427274826654191e-07, 'num_leaves': 207, 'feature_fraction': 0.4543680750181247, 'bagging_fraction': 0.7279771884611197, 'bagging_freq': 2, 'min_child_samples': 100}. Best is trial 37 with value: 0.8296021212596943.\n",
      "[I 2023-12-14 20:41:41,262] Trial 44 finished with value: 0.8363099608060942 and parameters: {'lambda_l1': 1.6135055778116683, 'lambda_l2': 2.7536363305096912e-08, 'num_leaves': 235, 'feature_fraction': 0.5232099033024489, 'bagging_fraction': 0.43428826858828656, 'bagging_freq': 3, 'min_child_samples': 94}. Best is trial 37 with value: 0.8296021212596943.\n",
      "[I 2023-12-14 20:41:41,311] Trial 45 finished with value: 0.8667496225878819 and parameters: {'lambda_l1': 1.8691323376533042, 'lambda_l2': 4.048362759546771e-08, 'num_leaves': 238, 'feature_fraction': 0.5383709057793886, 'bagging_fraction': 0.42277059489989194, 'bagging_freq': 1, 'min_child_samples': 94}. Best is trial 37 with value: 0.8296021212596943.\n",
      "[I 2023-12-14 20:41:41,363] Trial 46 finished with value: 0.8435591304609075 and parameters: {'lambda_l1': 2.986701534507448, 'lambda_l2': 2.912186365179856e-08, 'num_leaves': 239, 'feature_fraction': 0.5095664252564339, 'bagging_fraction': 0.40088507935419676, 'bagging_freq': 2, 'min_child_samples': 82}. Best is trial 37 with value: 0.8296021212596943.\n",
      "[I 2023-12-14 20:41:41,423] Trial 47 finished with value: 0.8390376038697199 and parameters: {'lambda_l1': 7.85435539482945, 'lambda_l2': 3.741999894815859e-08, 'num_leaves': 248, 'feature_fraction': 0.5053145842082821, 'bagging_fraction': 0.43807083427332955, 'bagging_freq': 2, 'min_child_samples': 83}. Best is trial 37 with value: 0.8296021212596943.\n",
      "[I 2023-12-14 20:41:41,478] Trial 48 finished with value: 0.8670041256599696 and parameters: {'lambda_l1': 0.9893886747365401, 'lambda_l2': 2.769619390681324e-08, 'num_leaves': 242, 'feature_fraction': 0.467173788494084, 'bagging_fraction': 0.4332757507333236, 'bagging_freq': 2, 'min_child_samples': 82}. Best is trial 37 with value: 0.8296021212596943.\n",
      "[I 2023-12-14 20:41:41,538] Trial 49 finished with value: 0.8331632546745067 and parameters: {'lambda_l1': 9.203045078004946, 'lambda_l2': 1.990163327827198e-08, 'num_leaves': 256, 'feature_fraction': 0.6186223684392852, 'bagging_fraction': 0.4175537959658816, 'bagging_freq': 2, 'min_child_samples': 83}. Best is trial 37 with value: 0.8296021212596943.\n",
      "[I 2023-12-14 20:41:41,607] Trial 50 finished with value: 0.8716261134317209 and parameters: {'lambda_l1': 9.2587545873757, 'lambda_l2': 1.0509838927797313e-08, 'num_leaves': 256, 'feature_fraction': 0.6200338646546941, 'bagging_fraction': 0.4006321839459542, 'bagging_freq': 2, 'min_child_samples': 69}. Best is trial 37 with value: 0.8296021212596943.\n",
      "[I 2023-12-14 20:41:41,665] Trial 51 finished with value: 0.8560446717395898 and parameters: {'lambda_l1': 2.573016653663992, 'lambda_l2': 3.082778013048544e-08, 'num_leaves': 242, 'feature_fraction': 0.5636093854058163, 'bagging_fraction': 0.430566329701118, 'bagging_freq': 2, 'min_child_samples': 85}. Best is trial 37 with value: 0.8296021212596943.\n",
      "[I 2023-12-14 20:41:41,723] Trial 52 finished with value: 0.8263280440724177 and parameters: {'lambda_l1': 5.291808297475316, 'lambda_l2': 1.9909760291057786e-08, 'num_leaves': 225, 'feature_fraction': 0.5255725797612429, 'bagging_fraction': 0.42261485638359153, 'bagging_freq': 3, 'min_child_samples': 89}. Best is trial 52 with value: 0.8263280440724177.\n",
      "[I 2023-12-14 20:41:41,775] Trial 53 finished with value: 0.8579382495529059 and parameters: {'lambda_l1': 4.171143487219811, 'lambda_l2': 2.3724315259766058e-08, 'num_leaves': 231, 'feature_fraction': 0.5571110539436224, 'bagging_fraction': 0.43238363440876093, 'bagging_freq': 2, 'min_child_samples': 81}. Best is trial 52 with value: 0.8263280440724177.\n",
      "[I 2023-12-14 20:41:41,823] Trial 54 finished with value: 0.8568058505217345 and parameters: {'lambda_l1': 0.2278174779821346, 'lambda_l2': 3.4968569991780823e-07, 'num_leaves': 222, 'feature_fraction': 0.4671379529967853, 'bagging_fraction': 0.40894585721138055, 'bagging_freq': 3, 'min_child_samples': 90}. Best is trial 52 with value: 0.8263280440724177.\n",
      "[I 2023-12-14 20:41:41,891] Trial 55 finished with value: 0.9048569147628979 and parameters: {'lambda_l1': 1.0393456702890036, 'lambda_l2': 1.904903298493841e-08, 'num_leaves': 247, 'feature_fraction': 0.5256493409529157, 'bagging_fraction': 0.5032380552384438, 'bagging_freq': 2, 'min_child_samples': 77}. Best is trial 52 with value: 0.8263280440724177.\n",
      "[I 2023-12-14 20:41:41,997] Trial 56 finished with value: 0.8036301480973435 and parameters: {'lambda_l1': 9.607176564991734, 'lambda_l2': 6.372203465703871e-07, 'num_leaves': 234, 'feature_fraction': 0.6203682987639026, 'bagging_fraction': 0.4011412552337647, 'bagging_freq': 3, 'min_child_samples': 43}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:42,092] Trial 57 finished with value: 0.9242287573287102 and parameters: {'lambda_l1': 3.7295918828143773, 'lambda_l2': 6.234052653254134e-07, 'num_leaves': 214, 'feature_fraction': 0.6307559126554478, 'bagging_fraction': 0.44439407418500343, 'bagging_freq': 3, 'min_child_samples': 40}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:42,237] Trial 58 finished with value: 0.8965655182258574 and parameters: {'lambda_l1': 8.39151256831063, 'lambda_l2': 7.753226885412549e-08, 'num_leaves': 206, 'feature_fraction': 0.5789407299290278, 'bagging_fraction': 0.4874425135969787, 'bagging_freq': 4, 'min_child_samples': 40}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:42,324] Trial 59 finished with value: 0.9897030286613901 and parameters: {'lambda_l1': 1.3675503832618472, 'lambda_l2': 9.864816181157888e-07, 'num_leaves': 249, 'feature_fraction': 0.6147559579149832, 'bagging_fraction': 0.5121561554856189, 'bagging_freq': 3, 'min_child_samples': 49}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:42,383] Trial 60 finished with value: 0.8870452608561644 and parameters: {'lambda_l1': 0.5442898429519285, 'lambda_l2': 1.0661366493688562e-08, 'num_leaves': 233, 'feature_fraction': 0.6554875527728875, 'bagging_fraction': 0.42266190911109924, 'bagging_freq': 4, 'min_child_samples': 56}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:42,444] Trial 61 finished with value: 0.9070186207702641 and parameters: {'lambda_l1': 3.9152434296835494, 'lambda_l2': 5.504316910626911e-08, 'num_leaves': 238, 'feature_fraction': 0.5488968168830349, 'bagging_fraction': 0.400435951201754, 'bagging_freq': 2, 'min_child_samples': 44}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:42,529] Trial 62 finished with value: 0.9762355724236037 and parameters: {'lambda_l1': 3.667399102218708, 'lambda_l2': 2.1715420032481746e-08, 'num_leaves': 219, 'feature_fraction': 0.688903919768693, 'bagging_fraction': 0.44801132528856336, 'bagging_freq': 3, 'min_child_samples': 28}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:42,599] Trial 63 finished with value: 0.9738110145337379 and parameters: {'lambda_l1': 1.7567470588097405, 'lambda_l2': 3.968132387097581e-07, 'num_leaves': 256, 'feature_fraction': 0.5782036821562365, 'bagging_fraction': 0.8747176463904431, 'bagging_freq': 1, 'min_child_samples': 88}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:42,643] Trial 64 finished with value: 0.8606553810212595 and parameters: {'lambda_l1': 0.12731862590913734, 'lambda_l2': 4.153103824444881e-08, 'num_leaves': 231, 'feature_fraction': 0.5126677992949218, 'bagging_fraction': 0.4168937566634102, 'bagging_freq': 2, 'min_child_samples': 84}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:42,693] Trial 65 finished with value: 0.8328226031452384 and parameters: {'lambda_l1': 5.062317052413248, 'lambda_l2': 7.270701606657119e-08, 'num_leaves': 248, 'feature_fraction': 0.5979903634906504, 'bagging_fraction': 0.4611831850372079, 'bagging_freq': 3, 'min_child_samples': 91}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:42,733] Trial 66 finished with value: 0.8330967500212836 and parameters: {'lambda_l1': 5.701245417181075, 'lambda_l2': 7.64657090135866e-08, 'num_leaves': 7, 'feature_fraction': 0.5972301436653317, 'bagging_fraction': 0.4608093938184937, 'bagging_freq': 4, 'min_child_samples': 94}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:42,787] Trial 67 finished with value: 0.8607925853413737 and parameters: {'lambda_l1': 1.3472201081074582e-08, 'lambda_l2': 2.82428216778735e-07, 'num_leaves': 33, 'feature_fraction': 0.6148630919458372, 'bagging_fraction': 0.47428455208841347, 'bagging_freq': 4, 'min_child_samples': 92}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:42,825] Trial 68 finished with value: 0.8326747652541886 and parameters: {'lambda_l1': 9.597878490705906, 'lambda_l2': 1.27112134175909e-05, 'num_leaves': 49, 'feature_fraction': 0.6641666742771424, 'bagging_fraction': 0.46270368656144967, 'bagging_freq': 4, 'min_child_samples': 92}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:42,873] Trial 69 finished with value: 0.8475798156332673 and parameters: {'lambda_l1': 0.3411031361551731, 'lambda_l2': 3.1111800870377124e-05, 'num_leaves': 26, 'feature_fraction': 0.6607816424854217, 'bagging_fraction': 0.4663320988130254, 'bagging_freq': 4, 'min_child_samples': 88}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:42,910] Trial 70 finished with value: 0.8331187195358589 and parameters: {'lambda_l1': 9.803687178949096, 'lambda_l2': 8.576425743286046e-05, 'num_leaves': 15, 'feature_fraction': 0.6356498827546053, 'bagging_fraction': 0.512766908388133, 'bagging_freq': 4, 'min_child_samples': 97}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:42,942] Trial 71 finished with value: 0.8229139654689228 and parameters: {'lambda_l1': 9.636372266966063, 'lambda_l2': 5.557120314230647e-05, 'num_leaves': 2, 'feature_fraction': 0.6374613395083204, 'bagging_fraction': 0.513657473486542, 'bagging_freq': 4, 'min_child_samples': 97}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:42,993] Trial 72 finished with value: 0.8657393661194182 and parameters: {'lambda_l1': 4.727564485047609, 'lambda_l2': 0.00011521666541068936, 'num_leaves': 7, 'feature_fraction': 0.6962922438625321, 'bagging_fraction': 0.5190746708400694, 'bagging_freq': 4, 'min_child_samples': 96}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:43,047] Trial 73 finished with value: 0.9134930659863476 and parameters: {'lambda_l1': 0.6757461869716787, 'lambda_l2': 0.0005001277264280152, 'num_leaves': 17, 'feature_fraction': 0.6738408928943543, 'bagging_fraction': 0.5797272682547788, 'bagging_freq': 4, 'min_child_samples': 92}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:43,124] Trial 74 finished with value: 0.9375325961674221 and parameters: {'lambda_l1': 5.025145572882207, 'lambda_l2': 1.4746156368723668e-05, 'num_leaves': 36, 'feature_fraction': 0.6334897003123621, 'bagging_fraction': 0.4880464250015483, 'bagging_freq': 4, 'min_child_samples': 33}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:43,164] Trial 75 finished with value: 0.8475232587043642 and parameters: {'lambda_l1': 1.1582402657168573, 'lambda_l2': 7.076350775357048e-05, 'num_leaves': 2, 'feature_fraction': 0.6071515083586586, 'bagging_fraction': 0.5563477483682532, 'bagging_freq': 5, 'min_child_samples': 97}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:43,203] Trial 76 finished with value: 0.8196096677147224 and parameters: {'lambda_l1': 9.640129158146, 'lambda_l2': 5.925105623979066e-06, 'num_leaves': 47, 'feature_fraction': 0.6399107633521196, 'bagging_fraction': 0.46055751582501847, 'bagging_freq': 4, 'min_child_samples': 92}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:43,254] Trial 77 finished with value: 0.8309650278804169 and parameters: {'lambda_l1': 2.816323116924425, 'lambda_l2': 6.262727775342802e-06, 'num_leaves': 50, 'feature_fraction': 0.5922320005115689, 'bagging_fraction': 0.49306645041097397, 'bagging_freq': 5, 'min_child_samples': 92}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:43,299] Trial 78 finished with value: 0.8487281138122085 and parameters: {'lambda_l1': 2.6777797028437975, 'lambda_l2': 6.653830150084523e-06, 'num_leaves': 47, 'feature_fraction': 0.6643770367212855, 'bagging_fraction': 0.49601769973222376, 'bagging_freq': 5, 'min_child_samples': 92}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:43,359] Trial 79 finished with value: 0.9217320793973678 and parameters: {'lambda_l1': 0.07750234754465432, 'lambda_l2': 2.1935922892427304e-05, 'num_leaves': 60, 'feature_fraction': 0.7042671822709345, 'bagging_fraction': 0.626602732252896, 'bagging_freq': 3, 'min_child_samples': 86}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:43,402] Trial 80 finished with value: 0.8552809226410543 and parameters: {'lambda_l1': 0.3500221401115231, 'lambda_l2': 1.8664590042451506e-06, 'num_leaves': 90, 'feature_fraction': 0.6408102004754908, 'bagging_fraction': 0.4762772055096825, 'bagging_freq': 3, 'min_child_samples': 89}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:43,441] Trial 81 finished with value: 0.8448725594464295 and parameters: {'lambda_l1': 5.173135939767667, 'lambda_l2': 4.373270652456315e-06, 'num_leaves': 48, 'feature_fraction': 0.5938227885286477, 'bagging_fraction': 0.4571552773629748, 'bagging_freq': 4, 'min_child_samples': 92}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:43,475] Trial 82 finished with value: 0.8391881357011002 and parameters: {'lambda_l1': 2.4861211438452058, 'lambda_l2': 1.0030942028427622e-05, 'num_leaves': 42, 'feature_fraction': 0.5740939052386035, 'bagging_fraction': 0.461547538547093, 'bagging_freq': 4, 'min_child_samples': 96}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:43,527] Trial 83 finished with value: 0.8242505302797679 and parameters: {'lambda_l1': 9.909424264831994, 'lambda_l2': 4.6199589596567006e-05, 'num_leaves': 73, 'feature_fraction': 0.6054166746180583, 'bagging_fraction': 0.4938371243587264, 'bagging_freq': 5, 'min_child_samples': 88}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:43,579] Trial 84 finished with value: 0.8375246331665732 and parameters: {'lambda_l1': 9.62110132713167, 'lambda_l2': 4.1550888311273145e-05, 'num_leaves': 73, 'feature_fraction': 0.754671835726888, 'bagging_fraction': 0.5235199868900374, 'bagging_freq': 5, 'min_child_samples': 87}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:43,626] Trial 85 finished with value: 0.862823220751144 and parameters: {'lambda_l1': 1.2180034931577148, 'lambda_l2': 0.00028884763530786524, 'num_leaves': 57, 'feature_fraction': 0.6805845572555365, 'bagging_fraction': 0.49956074125967315, 'bagging_freq': 5, 'min_child_samples': 90}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:43,672] Trial 86 finished with value: 0.8551213861165404 and parameters: {'lambda_l1': 2.8706879711153515, 'lambda_l2': 1.869754277642983e-05, 'num_leaves': 90, 'feature_fraction': 0.6521365943880253, 'bagging_fraction': 0.5448718447155981, 'bagging_freq': 6, 'min_child_samples': 100}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:43,742] Trial 87 finished with value: 0.8990527194943178 and parameters: {'lambda_l1': 4.84247594274453, 'lambda_l2': 4.471458778536735e-05, 'num_leaves': 82, 'feature_fraction': 0.7126611514020276, 'bagging_fraction': 0.7856829583669012, 'bagging_freq': 5, 'min_child_samples': 92}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:43,800] Trial 88 finished with value: 0.8699140389396601 and parameters: {'lambda_l1': 0.7581467302418136, 'lambda_l2': 2.53917389358148e-06, 'num_leaves': 64, 'feature_fraction': 0.564628182262859, 'bagging_fraction': 0.5703760655468915, 'bagging_freq': 3, 'min_child_samples': 79}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:43,876] Trial 89 finished with value: 0.9307415790642418 and parameters: {'lambda_l1': 2.040621431027169, 'lambda_l2': 7.932866243009634e-07, 'num_leaves': 20, 'feature_fraction': 0.629380687750336, 'bagging_fraction': 0.48470761454027217, 'bagging_freq': 3, 'min_child_samples': 44}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:43,924] Trial 90 finished with value: 0.878291148876057 and parameters: {'lambda_l1': 5.623153410177606, 'lambda_l2': 0.00014648684972031056, 'num_leaves': 32, 'feature_fraction': 0.6057931651612468, 'bagging_fraction': 0.45192987216664277, 'bagging_freq': 5, 'min_child_samples': 75}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:43,965] Trial 91 finished with value: 0.8269039610131138 and parameters: {'lambda_l1': 6.5938278366848575, 'lambda_l2': 1.072503485170581e-05, 'num_leaves': 25, 'feature_fraction': 0.5961673781562288, 'bagging_fraction': 0.4651654443797336, 'bagging_freq': 4, 'min_child_samples': 94}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:44,006] Trial 92 finished with value: 0.8249605615185642 and parameters: {'lambda_l1': 9.959852863151024, 'lambda_l2': 1.276321710857925e-05, 'num_leaves': 41, 'feature_fraction': 0.5401419451878402, 'bagging_fraction': 0.44121955074315955, 'bagging_freq': 4, 'min_child_samples': 97}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:44,137] Trial 93 finished with value: 0.9752199319096976 and parameters: {'lambda_l1': 8.855033012248139, 'lambda_l2': 1.2297715928478279e-05, 'num_leaves': 42, 'feature_fraction': 0.6446401317529483, 'bagging_fraction': 0.4405052255476266, 'bagging_freq': 4, 'min_child_samples': 7}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:44,173] Trial 94 finished with value: 0.828861914230438 and parameters: {'lambda_l1': 2.9677460274383685, 'lambda_l2': 6.195109092675382e-06, 'num_leaves': 25, 'feature_fraction': 0.5500977878471864, 'bagging_fraction': 0.49413464503881765, 'bagging_freq': 4, 'min_child_samples': 97}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:44,239] Trial 95 finished with value: 0.949828477822457 and parameters: {'lambda_l1': 3.1153966028576403, 'lambda_l2': 6.191925284568592e-06, 'num_leaves': 26, 'feature_fraction': 0.5454265395670886, 'bagging_fraction': 0.9966723251279394, 'bagging_freq': 5, 'min_child_samples': 98}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:44,278] Trial 96 finished with value: 0.8215700163263083 and parameters: {'lambda_l1': 1.4559557249434507, 'lambda_l2': 4.362758891454765e-06, 'num_leaves': 21, 'feature_fraction': 0.5852795917671204, 'bagging_fraction': 0.4950438456405969, 'bagging_freq': 4, 'min_child_samples': 95}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:44,327] Trial 97 finished with value: 0.8900935782120979 and parameters: {'lambda_l1': 1.410946517021021, 'lambda_l2': 1.4528836958551674e-06, 'num_leaves': 21, 'feature_fraction': 0.5671052550790323, 'bagging_fraction': 0.5958505636009899, 'bagging_freq': 4, 'min_child_samples': 96}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:44,361] Trial 98 finished with value: 0.8443880199065117 and parameters: {'lambda_l1': 0.0041717556712930854, 'lambda_l2': 2.0809256481087903, 'num_leaves': 14, 'feature_fraction': 0.5743904057454351, 'bagging_fraction': 0.4767648617037851, 'bagging_freq': 4, 'min_child_samples': 100}. Best is trial 56 with value: 0.8036301480973435.\n",
      "[I 2023-12-14 20:41:44,406] Trial 99 finished with value: 0.825940260305457 and parameters: {'lambda_l1': 0.8858439334841316, 'lambda_l2': 3.9935644969449865e-06, 'num_leaves': 37, 'feature_fraction': 0.4819997627287333, 'bagging_fraction': 0.4176879660959712, 'bagging_freq': 4, 'min_child_samples': 95}. Best is trial 56 with value: 0.8036301480973435.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 100\n",
      "Best trial:\n",
      "  Value: 0.8036301480973435\n",
      "  Params: \n",
      "    lambda_l1: 9.607176564991734\n",
      "    lambda_l2: 6.372203465703871e-07\n",
      "    num_leaves: 234\n",
      "    feature_fraction: 0.6203682987639026\n",
      "    bagging_fraction: 0.4011412552337647\n",
      "    bagging_freq: 3\n",
      "    min_child_samples: 43\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = data[feature_cols]\n",
    "y = data['index_return']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "# 使用LightGBM，用optuna调参\n",
    "def objective(trial):\n",
    "    ...\n",
    "\n",
    "    # 2. Suggest values of the hyperparameters using a trial object.\n",
    "    param = {\n",
    "        'objective': 'regression',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'feature_pre_filter': False\n",
    "    }\n",
    "\n",
    "    gbm = lgb.train(param, train_data)\n",
    "    preds = gbm.predict(X_test)\n",
    "    try:\n",
    "        loss = mean_squared_error(y_test, preds)\n",
    "    except ValueError:\n",
    "        print(y_test)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T12:41:44.419080Z",
     "start_time": "2023-12-14T12:41:37.984773Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00015462870391635963\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "bst_params = {\n",
    "        'objective': 'regression',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'feature_pre_filter': False\n",
    "    }\n",
    "bst_params.update(trial.params)\n",
    "gbm = lgb.train(bst_params, train_data)\n",
    "y_pred = gbm.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(r2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T12:41:44.478864Z",
     "start_time": "2023-12-14T12:41:44.417529Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "                                        0\nfeature2_feature3_quotient_5_bias     9.0\nfeature2_feature3_product_20_bias     9.0\nfeature2_feature3_product_5_bias      8.0\nfeature2_feature3_product_10_vol      8.0\nfeature1_feature2_difference_20_bias  7.0\n...                                   ...\nfeature1_feature3_quotient_10_vol     1.0\nfeature1_feature2_sum_10_bias         1.0\nfeature1_feature3_quotient_5_20       1.0\nfeature1_feature3_quotient_10_20      1.0\nfeature1_feature3_quotient_5_vol      1.0\n\n[128 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>feature2_feature3_quotient_5_bias</th>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>feature2_feature3_product_20_bias</th>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>feature2_feature3_product_5_bias</th>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>feature2_feature3_product_10_vol</th>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>feature1_feature2_difference_20_bias</th>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>feature1_feature3_quotient_10_vol</th>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>feature1_feature2_sum_10_bias</th>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>feature1_feature3_quotient_5_20</th>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>feature1_feature3_quotient_10_20</th>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>feature1_feature3_quotient_5_vol</th>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>128 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = gbm.feature_importance()\n",
    "importance = pd.DataFrame(importance).T\n",
    "importance.columns = X.columns\n",
    "importance = importance.T\n",
    "importance = importance[importance != 0]\n",
    "importance = importance.T.dropna(axis=1)\n",
    "importance = importance.T\n",
    "importance.sort_values(by=[0], ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T12:41:44.492016Z",
     "start_time": "2023-12-14T12:41:44.484360Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 再使用shapley来解释特征重要性\n",
    "\n",
    "好处是可以看出相关性是正向还是负向，这样一来可以方便我们后续计算量化指标"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "gbm_explainer = shap.TreeExplainer(gbm)\n",
    "gbm_shap = gbm_explainer.shap_values(X)\n",
    "gbm_shap = pd.DataFrame(gbm_shap)\n",
    "gbm_shap.columns = X.columns\n",
    "shap_importance = gbm_shap.mean(axis=0)\n",
    "shap_importance = shap_importance[shap_importance != 0].sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T12:41:46.228574Z",
     "start_time": "2023-12-14T12:41:44.488032Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "feature1_feature2_sum_20_mean           0.004471\nfeature1_feature2_difference_20_bias    0.003792\nfeature2_feature3_product_10_vol        0.002943\nfeature2_feature3_difference_5_mean     0.002656\nfeature1_feature3_difference_5_mean     0.002641\n                                          ...   \nfeature1_feature3_product              -0.002408\nfeature2_feature3_product_5_vol        -0.002878\nfeature1_feature3_difference_20_mean   -0.003263\nfeature2_feature3_sum_5_20             -0.003284\nfeature3_5_20                          -0.003750\nLength: 128, dtype: float64"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap_importance"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T12:41:46.235509Z",
     "start_time": "2023-12-14T12:41:46.228336Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 计算量化指标\n",
    "\n",
    "现在我们已经得到了我们构造最重要的一些特征，接下来对这些特征分别计算量化指标，我们有一些量化指标：\n",
    "\n",
    "- Normal-IC：由于我们没有一个股票池，所以这里的Normal-IC改为计算因子值时间序列-下一期index_return时间序列的相关性（由于我们没有股票池，因此IR无法计算）\n",
    "- Sharpe：对每一个因子单独提出一个交易策略，根据因子与收益率相关性和因子每一时期的正负号选择做多或者做空，进行交易，然后报告夏普（无风险利率算2%）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "{'feature1_feature2_sum_20_mean': -0.025818011230521153,\n 'feature1_feature2_difference_20_bias': -0.04183846521935986,\n 'feature2_feature3_product_10_vol': -0.010301736649899433,\n 'feature2_feature3_difference_5_mean': -0.04133967457518,\n 'feature1_feature3_difference_5_mean': 0.0003444343735868096,\n 'feature2_20_bias': -0.02202286366361527,\n 'feature3_10_vol': 0.02409930523008868,\n 'feature1_20_bias': 0.00501235399754592,\n 'feature2_feature3_sum_10_bias': -5.720735129093518e-05,\n 'feature1_feature2_quotient_10_bias': 0.02945392175848562,\n 'feature1_feature2_difference_20_mean': 0.06272079713053656,\n 'feature1_feature2_product': 0.03713235264228935,\n 'feature1_feature2_difference_20_vol': -0.017459249747035572,\n 'feature1_feature3_product_10_vol': 0.04819695557905687,\n 'feature1_feature3_difference_5_20': 0.019473481723268203,\n 'feature1_feature2_difference_10_bias': -0.00040618137683608445,\n 'feature1_feature3_difference_20_vol': -0.0005407511719362567,\n 'feature2_feature3_product_20_bias': -0.10640163620334894,\n 'feature2_feature3_product_5_bias': 0.04388804262804185,\n 'feature1_feature3_quotient_5_vol': 0.00452532605341494,\n 'feature2_feature3_sum_10_mean': -0.08451946182665508,\n 'feature1_feature3_difference_5_bias': 0.013625421202868275,\n 'feature2_feature3_quotient_5_20': -0.062160097815370796,\n 'feature1_5_10': -0.022826512988150985,\n 'feature1_feature2_quotient': 0.037454786931396984,\n 'feature1_feature2_quotient_20_bias': 0.05435929468350581,\n 'feature1_feature3_product_20_bias': 0.04074838653355425,\n 'feature2_feature3_quotient_5_vol': -0.033418465537251495,\n 'feature1_feature2_product_20_bias': 0.0977818236773981,\n 'feature1_feature3_product_10_bias': 0.06716009933355524,\n 'feature2_feature3_sum_5_bias': -0.035404678836160455,\n 'feature1_feature3_sum_5_vol': -0.01590628340201564,\n 'feature1_feature3_product_10_20': 0.06406812568488184,\n 'feature1_feature3_sum_20_bias': 0.005361407027350875,\n 'feature1_feature3_sum_10_vol': 0.004886012808925509,\n 'feature1_feature3_quotient_10_20': -0.015873764101957285,\n 'feature2_20_vol': -0.009824058292039336,\n 'feature1_feature2_product_5_bias': 0.0960228389086944,\n 'feature1_feature3_sum_5_mean': -0.05588566543269233,\n 'feature1_feature3_quotient_10_vol': 0.008583954839243105,\n 'feature1_feature2_quotient_5_bias': 0.05165204639145103,\n 'feature1_feature3_quotient_5_20': -0.014299355209594541,\n 'feature1_feature3_difference_10_bias': -0.009636347832465246,\n 'feature1_feature2_quotient_5_20': 0.030848559572125674,\n 'feature1_feature2_sum_10_bias': 0.02756200673548596,\n 'feature1_feature3_product_5_10': 0.061577176520202136,\n 'feature1_feature3_sum_20_vol': 0.004832248280955547,\n 'feature1_feature2_quotient_10_20': 0.05150565941065449,\n 'feature1_feature3_product_20_vol': 0.029214845550786222,\n 'feature1_20_vol': 0.01537561443090406,\n 'feature2_feature3_product_20_mean': 0.046089488693424115,\n 'feature1_feature3_sum_20_mean': -0.04455990936560991,\n 'feature1_feature3_sum_5_10': -0.058919930559003145,\n 'feature1_feature3_product_5_bias': 0.03262123673276824,\n 'feature2_feature3_product_5_20': -0.014461276398874135,\n 'feature2_feature3_product_5_mean': 0.06203804594198149,\n 'feature1_feature2_product_10_bias': 0.01246325912382474,\n 'feature1_feature2_quotient_5_vol': 0.05074767537082414,\n 'feature1_feature2_product_5_20': 0.08052141813215632,\n 'feature2_5_vol': -0.003131445614313094,\n 'feature2_feature3_product_10_20': -0.003183359071783354,\n 'feature1_feature2_difference_5_20': -0.0809503129354586,\n 'feature2_10_vol': 0.0014210381231470831,\n 'feature2_feature3_sum_20_vol': -0.022693201354443936,\n 'feature3_10_mean': -0.027679350022063932,\n 'feature1_feature3_product_5_20': 0.03902590138820535,\n 'feature2_feature3_sum': -0.0567926590478615,\n 'feature2_feature3_product_10_bias': 0.030850642757099644,\n 'feature1_feature3_quotient_20_bias': 0.013374177152553152,\n 'feature1_feature2_difference_5_mean': 0.09138152409730183,\n 'feature3_10_bias': -0.06278924575378958,\n 'feature2_5_20': -0.07647396455951558,\n 'feature2_5_10': -0.027836955821802834,\n 'feature3_10_20': -0.02450097744383351,\n 'feature2_feature3_difference': -0.013117741029656961,\n 'feature1_feature3_product_10_mean': -0.04136348576589749,\n 'feature2_feature3_quotient_5_10': -0.023704688034143266,\n 'feature1_feature2_quotient_5_10': -0.011696217146126286,\n 'feature2_feature3_product': 0.08069419079269644,\n 'feature1_feature2_product_5_mean': 0.0687738903534162,\n 'feature1_feature3_difference_20_bias': 0.021001031559109068,\n 'feature1_feature2_difference_10_vol': -0.009515787048337946,\n 'feature1_feature2_quotient_5_mean': 0.058227937880326895,\n 'feature1_feature2_difference_5_vol': -0.004881806193078852,\n 'feature1_feature2_difference_5_bias': 0.06652479180584353,\n 'feature1_feature2_sum_5_10': -0.026643941317261007,\n 'feature3_5_bias': -0.04360786993870382,\n 'feature1_feature2_difference_5_10': -0.021476613145204566,\n 'feature1_feature2_difference_10_20': -0.05615686653584774,\n 'feature1_feature3_product_5_mean': -0.01420585185804104,\n 'feature1_feature2_product_10_mean': 0.03806285512002409,\n 'feature1_feature3_difference_10_mean': -0.0020448109027542552,\n 'feature1_feature2_product_10_20': 0.02925725204325918,\n 'feature3_20_vol': 0.016322209513205642,\n 'feature2_feature3_difference_10_20': 0.01989560447562502,\n 'feature1_feature2_quotient_20_mean': 0.04390482879405962,\n 'feature2_feature3_sum_10_20': -0.07851452597084933,\n 'feature3_5_vol': -0.018543195962148495,\n 'feature2_feature3_product_5_10': 0.013506101838415453,\n 'feature1_feature2_difference_10_mean': 0.08261633640572778,\n 'feature2_feature3_difference_5_20': 0.02312376485658882,\n 'feature2_feature3_sum_5_10': -0.00693326632126988,\n 'feature1_feature2_sum_10_20': -0.035827726442522384,\n 'feature3': -0.06337589109260659,\n 'feature2_feature3_product_10_mean': 0.04395719591858533,\n 'feature1_feature2_quotient_20_vol': 0.043600668065171724,\n 'feature2_feature3_difference_20_bias': 0.04204545284781328,\n 'feature1_feature3_product_5_vol': 0.03056761896211973,\n 'feature2_5_bias': 0.07100216867524103,\n 'feature1_5_bias': 0.05037114667647803,\n 'feature1_10_20': -0.014727855751955418,\n 'feature1_feature2_sum_5_20': -0.0280486311491543,\n 'feature1_feature3_quotient_5_bias': 0.07108944612265503,\n 'feature2_feature3_quotient_10_20': -0.05621670531679952,\n 'feature2_feature3_quotient_5_bias': 0.09892477422329414,\n 'feature1_feature2_product_5_10': -0.043899556399616436,\n 'feature2_feature3_sum_5_vol': -0.022248934387244797,\n 'feature1_feature2_sum_10_mean': -0.020178433638609477,\n 'feature1_feature3_difference_5_vol': 0.00024821800296034337,\n 'feature3_20_mean': -0.023365186870790835,\n 'feature3_5_10': -0.07944141042268447,\n 'feature1_feature2_product_20_mean': 0.04767510064569164,\n 'feature1_feature2_quotient_10_vol': 0.03801497644937447,\n 'feature1_feature3_product': -0.023645667161346574,\n 'feature2_feature3_product_5_vol': 0.03574518205115602,\n 'feature1_feature3_difference_20_mean': 0.008749963547299452,\n 'feature2_feature3_sum_5_20': -0.0642799875493936,\n 'feature3_5_20': -0.05483216646565557}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IC_dict = {}\n",
    "\n",
    "for feature in shap_importance.index:\n",
    "    # 准备每个特征对应的数据（注意两列数据应该相差一期））\n",
    "    backtest_iter = data[[feature, 'index_return']]\n",
    "    backtest_iter.dropna(inplace=True, how='any')\n",
    "    # 计算相关系数\n",
    "    IC_dict[feature] = backtest_iter.corr().iloc[1, 0]\n",
    "\n",
    "IC_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T12:41:46.298431Z",
     "start_time": "2023-12-14T12:41:46.232090Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5779346439570948 5.662866963933878e-64\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "y_all_pred = gbm.predict(X)\n",
    "r, p = stats.spearmanr(y_all_pred[:-1], y[:-1])\n",
    "print(r, p)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T12:41:46.304891Z",
     "start_time": "2023-12-14T12:41:46.297566Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "{'feature1_feature2_sum_20_mean': (0.35855837735861673,\n  0.11932775877591872,\n  0.1819199751934733,\n  0.5459969894470516),\n 'feature1_feature2_difference_20_bias': (0.33217955158642876,\n  0.11128268944727249,\n  0.1819397456392459,\n  0.5017193418983327),\n 'feature2_feature3_difference_5_mean': (0.5014032562820014,\n  0.15754810208831427,\n  0.1820321498684073,\n  0.7556253232615725),\n 'feature2_20_bias': (0.20133944047479568,\n  0.0698125457679355,\n  0.1820252480673681,\n  0.2736573431258268),\n 'feature3_10_vol': (-0.23256681036161853,\n  -0.09151686136705384,\n  0.18253279942650552,\n  -0.6109414949939157),\n 'feature1_feature2_quotient_10_bias': (0.06124585503375801,\n  0.023277189323524405,\n  0.1788636304379522,\n  0.01832227890879841),\n 'feature1_feature2_difference_20_mean': (0.38879651699831586,\n  0.1284292972090153,\n  0.1818964302416283,\n  0.5961045913049506),\n 'feature1_feature2_product': (0.291405439423013,\n  0.09586073560387143,\n  0.18181451231937157,\n  0.4172424667103363),\n 'feature1_feature3_product_10_vol': (0.3342619428465705,\n  0.11022540513659207,\n  0.18244126117283466,\n  0.49454495412152166),\n 'feature2_feature3_product_20_bias': (0.9891934433635601,\n  0.2878884874844345,\n  0.18130889059494515,\n  1.4775253800593449),\n 'feature2_feature3_product_5_bias': (0.013779383051443217,\n  0.005032560243077189,\n  0.18112317696970232,\n  -0.08263680003485427),\n 'feature2_feature3_sum_10_mean': (1.1419877234289326,\n  0.31810772116244435,\n  0.18166695755324597,\n  1.6409573054861668),\n 'feature2_feature3_quotient_5_20': (0.7165611826744984,\n  0.22722309446636357,\n  0.17846097321715626,\n  1.1611675691928944),\n 'feature1_5_10': (0.39306047241306397,\n  0.12830982631665977,\n  0.18253012260027196,\n  0.5933805597328755),\n 'feature1_feature2_quotient': (0.3555153351104432,\n  0.11660543558271552,\n  0.18164059493778223,\n  0.5318493677903114),\n 'feature1_feature2_quotient_20_bias': (0.506629654028347,\n  0.17846259984495583,\n  0.1762906260230967,\n  0.8988713887951982),\n 'feature1_feature3_product_20_bias': (0.5297766315076866,\n  0.1692881539483555,\n  0.181776086748599,\n  0.821274990669289),\n 'feature2_feature3_quotient_5_vol': (0.2628112234870097,\n  0.08828913947130013,\n  0.18056946022911624,\n  0.37818764803666793),\n 'feature1_feature2_product_20_bias': (0.7625017822189728,\n  0.2318191029047587,\n  0.18154965701427578,\n  1.166728190999021),\n 'feature1_feature3_product_10_bias': (-0.015252206024110149,\n  -0.005581430304921642,\n  0.1827338773734668,\n  -0.13999281727404586),\n 'feature2_feature3_sum_5_bias': (-0.0891136093992343,\n  -0.03304309990555343,\n  0.18231803504708077,\n  -0.29093720701770337),\n 'feature1_feature3_product_10_20': (0.42884927419326213,\n  0.14029416348185664,\n  0.18186391790539327,\n  0.66145151202799),\n 'feature1_feature2_product_5_bias': (0.9587190629716076,\n  0.28625438246546,\n  0.1813359499570924,\n  1.4682934218419508),\n 'feature1_feature3_sum_5_mean': (0.2238699612225752,\n  0.07543631918118021,\n  0.18223432897704617,\n  0.3042034916931751),\n 'feature1_feature2_quotient_5_bias': (-0.0809971396387017,\n  -0.0319745103196305,\n  0.18003555493377504,\n  -0.2886902553151182),\n 'feature1_feature2_quotient_5_20': (0.21976617364136097,\n  0.08284301655044257,\n  0.17655231715313913,\n  0.35594557785346637),\n 'feature1_feature2_sum_10_bias': (-0.06854943041700745,\n  -0.025419632018095384,\n  0.18260018431320318,\n  -0.2487381499034514),\n 'feature1_feature3_product_5_10': (0.016147239196431462,\n  0.00585028533932852,\n  0.18272969959054128,\n  -0.07743522094316363),\n 'feature1_feature2_quotient_10_20': (0.30717658613567544,\n  0.1132884016262019,\n  0.17648434928514337,\n  0.5285930565745357),\n 'feature1_feature3_product_20_vol': (-0.0030062593684270222,\n  -0.0011070048981575153,\n  0.18209964289305,\n  -0.1159090954975348),\n 'feature2_feature3_product_20_mean': (0.4027676196413983,\n  0.13259224416972382,\n  0.1818852545490334,\n  0.6190289831294198),\n 'feature1_feature3_sum_20_mean': (-0.03133751479520741,\n  -0.01164471767929165,\n  0.1821019956645725,\n  -0.1737746890900661),\n 'feature1_feature3_sum_5_10': (0.32994618628291206,\n  0.10892196526777487,\n  0.18244449588568415,\n  0.4873918768341002),\n 'feature1_feature3_product_5_bias': (-0.1251746049232615,\n  -0.04793851305607666,\n  0.18330248705182908,\n  -0.3706360680030868),\n 'feature2_feature3_product_5_mean': (0.0971235048473007,\n  0.03393202418271324,\n  0.18229395101789386,\n  0.07642614636919949),\n 'feature1_feature2_quotient_5_vol': (0.4304878353789303,\n  0.14437412026151164,\n  0.1787979115544001,\n  0.6956128244466112),\n 'feature1_feature2_product_5_20': (0.09013425784950169,\n  0.03225800216660035,\n  0.18207683309415765,\n  0.06732323908699221),\n 'feature1_feature2_difference_5_20': (0.18080893510143525,\n  0.06304995732979823,\n  0.18203640714067149,\n  0.2364909196242854),\n 'feature2_feature3_sum_20_vol': (-0.2283895491819541,\n  -0.09097536741181278,\n  0.18203465007190117,\n  -0.6096386999287171),\n 'feature3_10_mean': (0.1297758761194019,\n  0.04523627416567577,\n  0.18256075311586262,\n  0.13823493678106985),\n 'feature1_feature3_product_5_20': (0.2189115096362546,\n  0.07554285845772535,\n  0.18201523373992562,\n  0.3051549989331571),\n 'feature2_feature3_sum': (0.5813822200997691,\n  0.17827359622477013,\n  0.18158592099604115,\n  0.8716182144331593),\n 'feature2_feature3_product_10_bias': (-0.10896316313385879,\n  -0.04143599888192495,\n  0.18192737174340326,\n  -0.3376951928298972),\n 'feature1_feature2_difference_5_mean': (0.26053277224259563,\n  0.0869247820671799,\n  0.1822123664283755,\n  0.3672900109855435),\n 'feature3_10_bias': (0.23909352051145683,\n  0.0808330192457749,\n  0.18250418362857782,\n  0.33332397118949786),\n 'feature2_5_20': (0.5706888358273061,\n  0.1806965022971212,\n  0.1817383861358358,\n  0.8842188252789515),\n 'feature2_5_10': (0.2551283272186917,\n  0.08692608948551261,\n  0.18182578921488315,\n  0.3680780915319929),\n 'feature3_10_20': (0.011870846045230143,\n  0.004350805184087703,\n  0.18209748226607195,\n  -0.08593855676184725),\n 'feature1_feature3_product_10_mean': (0.14066082108713762,\n  0.0488765604941781,\n  0.18255601239785407,\n  0.1581791808162739),\n 'feature2_feature3_quotient_5_10': (0.49773482812897685,\n  0.1622571290739514,\n  0.18016427630861376,\n  0.7895967612928492),\n 'feature2_feature3_product': (0.8465266776875984,\n  0.24549845428361294,\n  0.1813331483280003,\n  1.2435589210403233),\n 'feature1_feature2_product_5_mean': (0.5817259801268655,\n  0.17947095130332746,\n  0.1819618385031009,\n  0.8763977799697261),\n 'feature1_feature3_difference_20_bias': (-0.22820502185396407,\n  -0.09089539956724291,\n  0.18203477634445175,\n  -0.6091989772185248),\n 'feature1_feature2_quotient_5_mean': (0.26481943296296007,\n  0.09252723290461584,\n  0.1789291712188283,\n  0.40534046187424566),\n 'feature1_feature2_difference_5_bias': (-0.07690306549762871,\n  -0.028396588256582422,\n  0.18231954304932393,\n  -0.2654492625811892),\n 'feature1_feature2_sum_5_10': (0.29370942691997093,\n  0.09786989544178337,\n  0.1824695288703327,\n  0.42675561187599503),\n 'feature3_5_bias': (-0.12240252614004643,\n  -0.0459166185902391,\n  0.18231114072846533,\n  -0.3615611110042665),\n 'feature1_feature2_difference_5_10': (0.18109781581703044,\n  0.062209367493336165,\n  0.18253661820473685,\n  0.2312378081092379),\n 'feature1_feature2_difference_10_20': (0.00980836487706327,\n  0.0035972061896396568,\n  0.18209781199486055,\n  -0.09007683085628337),\n 'feature1_feature2_product_10_mean': (0.35108900746082305,\n  0.11528198679195789,\n  0.18242918335235092,\n  0.5222957480872262),\n 'feature1_feature2_product_10_20': (-0.06570784486428183,\n  -0.024693582868052344,\n  0.18210148321030806,\n  -0.245432283582424),\n 'feature1_feature2_quotient_20_mean': (0.19345231229059734,\n  0.07342293614775963,\n  0.17657014553447187,\n  0.3025592802568645),\n 'feature2_feature3_sum_10_20': (0.5175502602598863,\n  0.16584146959838852,\n  0.18178710312022966,\n  0.8022652162619724),\n 'feature1_feature2_difference_10_mean': (0.598129274760445,\n  0.18529820504262373,\n  0.1822220890405995,\n  0.9071249589603537),\n 'feature2_feature3_difference_5_20': (-0.18704317769778522,\n  -0.07335084146519377,\n  0.18206360077751096,\n  -0.5127375327442428),\n 'feature1_feature2_sum_10_20': (0.15031880168628242,\n  0.05286818224412748,\n  0.1820517267542212,\n  0.18054309525172008),\n 'feature3': (-0.06260037657241202,\n  -0.022874503824316217,\n  0.181941444769242,\n  -0.23565001299563354),\n 'feature2_feature3_product_10_mean': (0.09536575067275521,\n  0.03357917902121832,\n  0.1825742626067237,\n  0.07437619534834827),\n 'feature1_feature2_quotient_20_vol': (0.09138990807918557,\n  0.03565743189032,\n  0.17662531536632253,\n  0.0886477222013523),\n 'feature2_feature3_difference_20_bias': (-0.12667461530098922,\n  -0.0486076197631895,\n  0.18209023209283925,\n  -0.3767781444103476),\n 'feature1_feature3_product_5_vol': (0.00045839292700144973,\n  0.0001649972534909505,\n  0.18231808796558988,\n  -0.10879338944281076),\n 'feature2_5_bias': (0.279416420924711,\n  0.09459454851767046,\n  0.1810032800721692,\n  0.412117109081826),\n 'feature1_5_bias': (0.11055573588297785,\n  0.03927172469795415,\n  0.18328059885579645,\n  0.10514874361097526),\n 'feature1_feature2_sum_5_20': (0.12484423124625144,\n  0.0442296749128519,\n  0.18206321777181417,\n  0.13308385520912716),\n 'feature1_feature3_quotient_5_bias': (0.6809043491124682,\n  0.21188342637320723,\n  0.1811803239481202,\n  1.059074308908686),\n 'feature2_feature3_quotient_10_20': (0.728257686230505,\n  0.23038524589011988,\n  0.17844825102695333,\n  1.178970624141016),\n 'feature2_feature3_quotient_5_bias': (0.6289782530994765,\n  0.19757579356723065,\n  0.17901001503550296,\n  0.991988037831359),\n 'feature1_feature2_product_5_10': (0.4063860154779799,\n  0.13387408703826043,\n  0.1818473748612314,\n  0.6262069338375562),\n 'feature2_feature3_sum_5_vol': (0.166883649444596,\n  0.05713370448533639,\n  0.1822644572749012,\n  0.20373530330891285),\n 'feature1_feature2_sum_10_mean': (0.08780854108550407,\n  0.03098786948939014,\n  0.18257685860057576,\n  0.06018215875555375),\n 'feature3_20_mean': (0.08438933994804465,\n  0.030253405960326774,\n  0.18207880708095275,\n  0.05631301151796365),\n 'feature3_5_10': (-0.1396796902718732,\n  -0.053090578411687184,\n  0.18258550614166963,\n  -0.400308764678044),\n 'feature1_feature2_product_20_mean': (0.2869558413738511,\n  0.0972526181081863,\n  0.1819718739792981,\n  0.4245305410053363),\n 'feature1_feature2_quotient_10_vol': (0.06266355088679165,\n  0.023695638110501793,\n  0.17873553055328648,\n  0.020676572246501438),\n 'feature1_feature3_product': (0.533744001347612,\n  0.16544313149653567,\n  0.18162783226478726,\n  0.800775573230982),\n 'feature2_feature3_product_5_vol': (0.14257507033285988,\n  0.04915222946108866,\n  0.18227571529338266,\n  0.1599347966577257),\n 'feature2_feature3_sum_5_20': (0.44939139315704346,\n  0.1462979214503115,\n  0.18184676629168064,\n  0.6945293778154445),\n 'feature3_5_20': (0.16094715064170462,\n  0.05643653535096904,\n  0.18204657819284198,\n  0.20014952059341543)}"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import statistics\n",
    "\n",
    "performance_dict = {}\n",
    "data_to_backtest = pd.read_csv('Dataset.csv', usecols=['day', 'index_return'])\n",
    "data_to_backtest['index_return'] = data_to_backtest['index_return'].shift(-1)\n",
    "data_to_backtest['index_return'] = data_to_backtest['index_return'].str.rstrip('%').astype('float')\n",
    "\n",
    "for k, v in IC_dict.items():\n",
    "    if (v < 0.02) and (v > - 0.02):\n",
    "        continue\n",
    "    else:\n",
    "        # 进行交易\n",
    "        backtest_iter = data_to_backtest[['day', 'index_return']]\n",
    "        backtest_iter[k] = data[k]\n",
    "        rev_list = []\n",
    "        if v > 0:\n",
    "            for row in backtest_iter.iterrows():\n",
    "                if row[1][k] > 0:\n",
    "                    rev_list.append(row[1]['index_return'])\n",
    "                elif row[1][k] < 0:\n",
    "                    rev_list.append(-row[1]['index_return'])\n",
    "        elif v < 0:\n",
    "            for row in backtest_iter.iterrows():\n",
    "                if row[1][k] > 0:\n",
    "                    rev_list.append(-row[1]['index_return'])\n",
    "                elif row[1][k] < 0:\n",
    "                    rev_list.append(row[1]['index_return'])\n",
    "        rev_list = rev_list[:-1]\n",
    "        rev_list = [x / 100 for x in rev_list]\n",
    "        cumulated_return = math.prod([x + 1 for x in rev_list]) - 1\n",
    "        annualized_return = (1 + cumulated_return) ** (252 / len(rev_list)) - 1\n",
    "        annualized_vol = np.std(rev_list) * math.sqrt(252)\n",
    "        sharpe = (annualized_return - 0.02) / annualized_vol\n",
    "        performance_dict[k] = (cumulated_return, annualized_return, annualized_vol, sharpe)\n",
    "\n",
    "performance_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:22:06.453220Z",
     "start_time": "2023-12-14T13:22:05.595848Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          累计收益      年化收益     年化波动率      夏普比率  \\\nfeature1_feature2_sum_20_mean         0.358558  0.119328  0.181920  0.545997   \nfeature1_feature2_difference_20_bias  0.332180  0.111283  0.181940  0.501719   \nfeature2_feature3_difference_5_mean   0.501403  0.157548  0.182032  0.755625   \nfeature2_20_bias                      0.201339  0.069813  0.182025  0.273657   \nfeature3_10_vol                      -0.232567 -0.091517  0.182533 -0.610941   \n...                                        ...       ...       ...       ...   \nfeature1_feature2_quotient_10_vol     0.062664  0.023696  0.178736  0.020677   \nfeature1_feature3_product             0.533744  0.165443  0.181628  0.800776   \nfeature2_feature3_product_5_vol       0.142575  0.049152  0.182276  0.159935   \nfeature2_feature3_sum_5_20            0.449391  0.146298  0.181847  0.694529   \nfeature3_5_20                         0.160947  0.056437  0.182047  0.200150   \n\n                                          IC比率  \nfeature1_feature2_sum_20_mean        -0.025818  \nfeature1_feature2_difference_20_bias -0.041838  \nfeature2_feature3_difference_5_mean  -0.041340  \nfeature2_20_bias                     -0.022023  \nfeature3_10_vol                       0.024099  \n...                                        ...  \nfeature1_feature2_quotient_10_vol     0.038015  \nfeature1_feature3_product            -0.023646  \nfeature2_feature3_product_5_vol       0.035745  \nfeature2_feature3_sum_5_20           -0.064280  \nfeature3_5_20                        -0.054832  \n\n[88 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>累计收益</th>\n      <th>年化收益</th>\n      <th>年化波动率</th>\n      <th>夏普比率</th>\n      <th>IC比率</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>feature1_feature2_sum_20_mean</th>\n      <td>0.358558</td>\n      <td>0.119328</td>\n      <td>0.181920</td>\n      <td>0.545997</td>\n      <td>-0.025818</td>\n    </tr>\n    <tr>\n      <th>feature1_feature2_difference_20_bias</th>\n      <td>0.332180</td>\n      <td>0.111283</td>\n      <td>0.181940</td>\n      <td>0.501719</td>\n      <td>-0.041838</td>\n    </tr>\n    <tr>\n      <th>feature2_feature3_difference_5_mean</th>\n      <td>0.501403</td>\n      <td>0.157548</td>\n      <td>0.182032</td>\n      <td>0.755625</td>\n      <td>-0.041340</td>\n    </tr>\n    <tr>\n      <th>feature2_20_bias</th>\n      <td>0.201339</td>\n      <td>0.069813</td>\n      <td>0.182025</td>\n      <td>0.273657</td>\n      <td>-0.022023</td>\n    </tr>\n    <tr>\n      <th>feature3_10_vol</th>\n      <td>-0.232567</td>\n      <td>-0.091517</td>\n      <td>0.182533</td>\n      <td>-0.610941</td>\n      <td>0.024099</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>feature1_feature2_quotient_10_vol</th>\n      <td>0.062664</td>\n      <td>0.023696</td>\n      <td>0.178736</td>\n      <td>0.020677</td>\n      <td>0.038015</td>\n    </tr>\n    <tr>\n      <th>feature1_feature3_product</th>\n      <td>0.533744</td>\n      <td>0.165443</td>\n      <td>0.181628</td>\n      <td>0.800776</td>\n      <td>-0.023646</td>\n    </tr>\n    <tr>\n      <th>feature2_feature3_product_5_vol</th>\n      <td>0.142575</td>\n      <td>0.049152</td>\n      <td>0.182276</td>\n      <td>0.159935</td>\n      <td>0.035745</td>\n    </tr>\n    <tr>\n      <th>feature2_feature3_sum_5_20</th>\n      <td>0.449391</td>\n      <td>0.146298</td>\n      <td>0.181847</td>\n      <td>0.694529</td>\n      <td>-0.064280</td>\n    </tr>\n    <tr>\n      <th>feature3_5_20</th>\n      <td>0.160947</td>\n      <td>0.056437</td>\n      <td>0.182047</td>\n      <td>0.200150</td>\n      <td>-0.054832</td>\n    </tr>\n  </tbody>\n</table>\n<p>88 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_df = pd.DataFrame(performance_dict)\n",
    "performance_df = performance_df.T\n",
    "performance_df.rename(columns={0: '累计收益', 1: '年化收益', 2: '年化波动率', 3: '夏普比率'}, inplace=True)\n",
    "performance_df['IC比率'] = 0\n",
    "for row in performance_df.iterrows():\n",
    "    performance_df.loc[row[0], 'IC比率'] = IC_dict[row[0]]\n",
    "performance_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:33:59.093314Z",
     "start_time": "2023-12-14T13:33:59.071835Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 开始回测"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "     day  feature1  feature2  feature3  index_return  feature1_feature2_sum  \\\n0      1 -0.466642  1.317203  1.357251      0.117669               0.942208   \n1      2 -0.615909 -0.452524  0.865544      0.038288               0.008084   \n2      3 -0.056160  0.432339  1.172861     -0.269313               0.739137   \n3      4 -0.280059 -0.536797  0.988470     -1.122658               0.150233   \n4      5 -0.280059 -0.410388  0.681154      0.375657               0.211154   \n..   ...       ...       ...       ...           ...                    ...   \n700  701 -0.690542 -0.578933 -1.838845      0.931324              -0.093452   \n701  702 -0.130793 -0.283979 -0.240797     -0.080783               0.353303   \n702  703 -0.727858 -1.084569 -0.486650     -0.467765              -0.357443   \n703  704 -0.727858 -0.958160 -0.548114      0.534419              -0.296522   \n704  705 -0.690542 -0.958160  0.312373           NaN              -0.276215   \n\n     feature1_feature2_difference  feature1_feature2_product  \\\n0                       -1.920112                   0.181151   \n1                       -0.358175                   1.200372   \n2                       -0.748659                  -0.235803   \n3                        0.357713                  -0.822627   \n4                        0.162471                  -0.174032   \n..                            ...                        ...   \n700                     -0.293094                   0.536334   \n701                      0.227552                   1.617326   \n702                      0.422794                  -0.050490   \n703                      0.227552                   0.042166   \n704                      0.292632                   0.119380   \n\n     feature1_feature2_quotient  feature1_feature3_sum  ...  \\\n0                     -0.650135               0.527213  ...   \n1                     -0.525788               0.040429  ...   \n2                     -0.321847               0.851735  ...   \n3                      0.035639               0.486647  ...   \n4                     -0.100986               0.283821  ...   \n..                          ...                    ...  ...   \n700                   -0.595905              -1.622747  ...   \n701                   -0.031279              -0.162397  ...   \n702                    0.094193              -0.973702  ...   \n703                   -0.432791              -1.014268  ...   \n704                   -0.257130              -0.405789  ...   \n\n     feature2_feature3_quotient_5_vol  feature2_feature3_quotient_10_mean  \\\n0                                 NaN                                 NaN   \n1                                 NaN                                 NaN   \n2                                 NaN                                 NaN   \n3                                 NaN                                 NaN   \n4                           -0.053922                                 NaN   \n..                                ...                                 ...   \n700                         -0.339185                           -0.239254   \n701                         -0.330087                           -0.245885   \n702                          0.043699                           -0.292229   \n703                          0.164947                           -0.321500   \n704                          0.032856                           -0.371143   \n\n     feature2_feature3_quotient_10_bias  feature2_feature3_quotient_10_vol  \\\n0                                   NaN                                NaN   \n1                                   NaN                                NaN   \n2                                   NaN                                NaN   \n3                                   NaN                                NaN   \n4                                   NaN                                NaN   \n..                                  ...                                ...   \n700                            0.494242                          -0.476520   \n701                           -0.236152                          -0.473627   \n702                           -1.455313                          -0.335132   \n703                           -1.219603                          -0.276219   \n704                           -1.297501                          -0.220441   \n\n     feature2_feature3_quotient_20_mean  feature2_feature3_quotient_20_bias  \\\n0                                   NaN                                 NaN   \n1                                   NaN                                 NaN   \n2                                   NaN                                 NaN   \n3                                   NaN                                 NaN   \n4                                   NaN                                 NaN   \n..                                  ...                                 ...   \n700                           -0.237888                            0.289437   \n701                           -0.225353                           -0.264600   \n702                           -0.251949                           -1.142495   \n703                           -0.265190                           -0.969758   \n704                           -0.312347                           -1.024103   \n\n     feature2_feature3_quotient_20_vol  feature2_feature3_quotient_5_10  \\\n0                                  NaN                              NaN   \n1                                  NaN                              NaN   \n2                                  NaN                              NaN   \n3                                  NaN                              NaN   \n4                                  NaN                              NaN   \n..                                 ...                              ...   \n700                          -0.397931                         0.432678   \n701                          -0.409356                         0.450037   \n702                          -0.374912                         0.179975   \n703                          -0.359044                        -0.185826   \n704                          -0.355752                        -0.928604   \n\n     feature2_feature3_quotient_5_20  feature2_feature3_quotient_10_20  \n0                                NaN                               NaN  \n1                                NaN                               NaN  \n2                                NaN                               NaN  \n3                                NaN                               NaN  \n4                                NaN                               NaN  \n..                               ...                               ...  \n700                         0.141660                         -0.059595  \n701                         0.097192                         -0.132994  \n702                        -0.103598                         -0.218575  \n703                        -0.332869                         -0.289509  \n704                        -0.716179                         -0.324210  \n\n[705 rows x 197 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day</th>\n      <th>feature1</th>\n      <th>feature2</th>\n      <th>feature3</th>\n      <th>index_return</th>\n      <th>feature1_feature2_sum</th>\n      <th>feature1_feature2_difference</th>\n      <th>feature1_feature2_product</th>\n      <th>feature1_feature2_quotient</th>\n      <th>feature1_feature3_sum</th>\n      <th>...</th>\n      <th>feature2_feature3_quotient_5_vol</th>\n      <th>feature2_feature3_quotient_10_mean</th>\n      <th>feature2_feature3_quotient_10_bias</th>\n      <th>feature2_feature3_quotient_10_vol</th>\n      <th>feature2_feature3_quotient_20_mean</th>\n      <th>feature2_feature3_quotient_20_bias</th>\n      <th>feature2_feature3_quotient_20_vol</th>\n      <th>feature2_feature3_quotient_5_10</th>\n      <th>feature2_feature3_quotient_5_20</th>\n      <th>feature2_feature3_quotient_10_20</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>-0.466642</td>\n      <td>1.317203</td>\n      <td>1.357251</td>\n      <td>0.117669</td>\n      <td>0.942208</td>\n      <td>-1.920112</td>\n      <td>0.181151</td>\n      <td>-0.650135</td>\n      <td>0.527213</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>-0.615909</td>\n      <td>-0.452524</td>\n      <td>0.865544</td>\n      <td>0.038288</td>\n      <td>0.008084</td>\n      <td>-0.358175</td>\n      <td>1.200372</td>\n      <td>-0.525788</td>\n      <td>0.040429</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>-0.056160</td>\n      <td>0.432339</td>\n      <td>1.172861</td>\n      <td>-0.269313</td>\n      <td>0.739137</td>\n      <td>-0.748659</td>\n      <td>-0.235803</td>\n      <td>-0.321847</td>\n      <td>0.851735</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>-0.280059</td>\n      <td>-0.536797</td>\n      <td>0.988470</td>\n      <td>-1.122658</td>\n      <td>0.150233</td>\n      <td>0.357713</td>\n      <td>-0.822627</td>\n      <td>0.035639</td>\n      <td>0.486647</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>-0.280059</td>\n      <td>-0.410388</td>\n      <td>0.681154</td>\n      <td>0.375657</td>\n      <td>0.211154</td>\n      <td>0.162471</td>\n      <td>-0.174032</td>\n      <td>-0.100986</td>\n      <td>0.283821</td>\n      <td>...</td>\n      <td>-0.053922</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>700</th>\n      <td>701</td>\n      <td>-0.690542</td>\n      <td>-0.578933</td>\n      <td>-1.838845</td>\n      <td>0.931324</td>\n      <td>-0.093452</td>\n      <td>-0.293094</td>\n      <td>0.536334</td>\n      <td>-0.595905</td>\n      <td>-1.622747</td>\n      <td>...</td>\n      <td>-0.339185</td>\n      <td>-0.239254</td>\n      <td>0.494242</td>\n      <td>-0.476520</td>\n      <td>-0.237888</td>\n      <td>0.289437</td>\n      <td>-0.397931</td>\n      <td>0.432678</td>\n      <td>0.141660</td>\n      <td>-0.059595</td>\n    </tr>\n    <tr>\n      <th>701</th>\n      <td>702</td>\n      <td>-0.130793</td>\n      <td>-0.283979</td>\n      <td>-0.240797</td>\n      <td>-0.080783</td>\n      <td>0.353303</td>\n      <td>0.227552</td>\n      <td>1.617326</td>\n      <td>-0.031279</td>\n      <td>-0.162397</td>\n      <td>...</td>\n      <td>-0.330087</td>\n      <td>-0.245885</td>\n      <td>-0.236152</td>\n      <td>-0.473627</td>\n      <td>-0.225353</td>\n      <td>-0.264600</td>\n      <td>-0.409356</td>\n      <td>0.450037</td>\n      <td>0.097192</td>\n      <td>-0.132994</td>\n    </tr>\n    <tr>\n      <th>702</th>\n      <td>703</td>\n      <td>-0.727858</td>\n      <td>-1.084569</td>\n      <td>-0.486650</td>\n      <td>-0.467765</td>\n      <td>-0.357443</td>\n      <td>0.422794</td>\n      <td>-0.050490</td>\n      <td>0.094193</td>\n      <td>-0.973702</td>\n      <td>...</td>\n      <td>0.043699</td>\n      <td>-0.292229</td>\n      <td>-1.455313</td>\n      <td>-0.335132</td>\n      <td>-0.251949</td>\n      <td>-1.142495</td>\n      <td>-0.374912</td>\n      <td>0.179975</td>\n      <td>-0.103598</td>\n      <td>-0.218575</td>\n    </tr>\n    <tr>\n      <th>703</th>\n      <td>704</td>\n      <td>-0.727858</td>\n      <td>-0.958160</td>\n      <td>-0.548114</td>\n      <td>0.534419</td>\n      <td>-0.296522</td>\n      <td>0.227552</td>\n      <td>0.042166</td>\n      <td>-0.432791</td>\n      <td>-1.014268</td>\n      <td>...</td>\n      <td>0.164947</td>\n      <td>-0.321500</td>\n      <td>-1.219603</td>\n      <td>-0.276219</td>\n      <td>-0.265190</td>\n      <td>-0.969758</td>\n      <td>-0.359044</td>\n      <td>-0.185826</td>\n      <td>-0.332869</td>\n      <td>-0.289509</td>\n    </tr>\n    <tr>\n      <th>704</th>\n      <td>705</td>\n      <td>-0.690542</td>\n      <td>-0.958160</td>\n      <td>0.312373</td>\n      <td>NaN</td>\n      <td>-0.276215</td>\n      <td>0.292632</td>\n      <td>0.119380</td>\n      <td>-0.257130</td>\n      <td>-0.405789</td>\n      <td>...</td>\n      <td>0.032856</td>\n      <td>-0.371143</td>\n      <td>-1.297501</td>\n      <td>-0.220441</td>\n      <td>-0.312347</td>\n      <td>-1.024103</td>\n      <td>-0.355752</td>\n      <td>-0.928604</td>\n      <td>-0.716179</td>\n      <td>-0.324210</td>\n    </tr>\n  </tbody>\n</table>\n<p>705 rows × 197 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T12:41:46.312990Z",
     "start_time": "2023-12-14T12:41:46.303030Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "根据我们预测的收益率进行交易，收益率大于0做多，反之做空\n",
    "假设期初净值为1，下面开始计算"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "     day  index_return\n0      1          0.10\n1      2          0.02\n2      3         -0.29\n3      4         -1.15\n4      5          0.36\n..   ...           ...\n700  701          0.92\n701  702         -0.10\n702  703         -0.49\n703  704          0.52\n704  705           NaN\n\n[705 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day</th>\n      <th>index_return</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.10</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>-0.29</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>-1.15</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.36</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>700</th>\n      <td>701</td>\n      <td>0.92</td>\n    </tr>\n    <tr>\n      <th>701</th>\n      <td>702</td>\n      <td>-0.10</td>\n    </tr>\n    <tr>\n      <th>702</th>\n      <td>703</td>\n      <td>-0.49</td>\n    </tr>\n    <tr>\n      <th>703</th>\n      <td>704</td>\n      <td>0.52</td>\n    </tr>\n    <tr>\n      <th>704</th>\n      <td>705</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>705 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_backtest = pd.read_csv('Dataset.csv', usecols=['day', 'index_return'])\n",
    "data_to_backtest['index_return'] = data_to_backtest['index_return'].shift(-1)\n",
    "data_to_backtest['index_return'] = data_to_backtest['index_return'].str.rstrip('%').astype('float')\n",
    "data_to_backtest"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T12:41:46.335615Z",
     "start_time": "2023-12-14T12:41:46.314296Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "     day  index_return   predict\n0      1          0.10 -0.159120\n1      2          0.02 -0.062834\n2      3         -0.29 -0.025465\n3      4         -1.15 -0.148071\n4      5          0.36  0.099315\n..   ...           ...       ...\n700  701          0.92 -0.082124\n701  702         -0.10 -0.193279\n702  703         -0.49  0.234850\n703  704          0.52  0.106943\n704  705           NaN  0.052091\n\n[705 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day</th>\n      <th>index_return</th>\n      <th>predict</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.10</td>\n      <td>-0.159120</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.02</td>\n      <td>-0.062834</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>-0.29</td>\n      <td>-0.025465</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>-1.15</td>\n      <td>-0.148071</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.36</td>\n      <td>0.099315</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>700</th>\n      <td>701</td>\n      <td>0.92</td>\n      <td>-0.082124</td>\n    </tr>\n    <tr>\n      <th>701</th>\n      <td>702</td>\n      <td>-0.10</td>\n      <td>-0.193279</td>\n    </tr>\n    <tr>\n      <th>702</th>\n      <td>703</td>\n      <td>-0.49</td>\n      <td>0.234850</td>\n    </tr>\n    <tr>\n      <th>703</th>\n      <td>704</td>\n      <td>0.52</td>\n      <td>0.106943</td>\n    </tr>\n    <tr>\n      <th>704</th>\n      <td>705</td>\n      <td>NaN</td>\n      <td>0.052091</td>\n    </tr>\n  </tbody>\n</table>\n<p>705 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_backtest['predict'] = gbm.predict(X)\n",
    "data_to_backtest"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T12:41:46.335987Z",
     "start_time": "2023-12-14T12:41:46.321501Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "0.22162162162162163"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = data_to_backtest[data_to_backtest['predict'] < - 0.2]\n",
    "len(test[test['index_return'] > 0]) / len(test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T12:41:46.353466Z",
     "start_time": "2023-12-14T12:41:46.327887Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在7.0进行做空，收益为0.73\n",
      "在8.0进行做空，收益为0.26\n",
      "在10.0进行做多，收益为2.34\n",
      "在17.0进行做多，收益为-1.18\n",
      "在18.0进行做多，收益为0.53\n",
      "在19.0进行做多，收益为2.39\n",
      "在24.0进行做空，收益为0.98\n",
      "在25.0进行做空，收益为2.12\n",
      "在29.0进行做多，收益为0.98\n",
      "在30.0进行做多，收益为0.52\n",
      "在31.0进行做多，收益为0.8\n",
      "在32.0进行做多，收益为-0.66\n",
      "在34.0进行做多，收益为2.24\n",
      "在37.0进行做多，收益为0.35\n",
      "在38.0进行做空，收益为1.91\n",
      "在39.0进行做多，收益为0.15\n",
      "在40.0进行做多，收益为0.26\n",
      "在41.0进行做多，收益为0.22\n",
      "在42.0进行做多，收益为-0.11\n",
      "在43.0进行做多，收益为2.05\n",
      "在44.0进行做多，收益为3.03\n",
      "在49.0进行做空，收益为0.77\n",
      "在50.0进行做空，收益为-0.8\n",
      "在52.0进行做空，收益为0.31\n",
      "在53.0进行做空，收益为1.26\n",
      "在54.0进行做空，收益为0.57\n",
      "在55.0进行做空，收益为-0.17\n",
      "在56.0进行做空，收益为-0.81\n",
      "在57.0进行做多，收益为0.76\n",
      "在60.0进行做多，收益为1.21\n",
      "在62.0进行做多，收益为1.47\n",
      "在66.0进行做空，收益为0.99\n",
      "在67.0进行做多，收益为0.06\n",
      "在70.0进行做多，收益为-0.18\n",
      "在71.0进行做空，收益为0.06\n",
      "在73.0进行做多，收益为0.3\n",
      "在75.0进行做多，收益为-0.62\n",
      "在76.0进行做空，收益为1.27\n",
      "在77.0进行做空，收益为-0.18\n",
      "在79.0进行做多，收益为-0.42\n",
      "在80.0进行做多，收益为2.16\n",
      "在81.0进行做多，收益为0.0\n",
      "在82.0进行做空，收益为0.2\n",
      "在83.0进行做空，收益为-0.18\n",
      "在85.0进行做空，收益为0.24\n",
      "在86.0进行做空，收益为1.34\n",
      "在90.0进行做多，收益为0.2\n",
      "在91.0进行做多，收益为0.18\n",
      "在92.0进行做多，收益为1.27\n",
      "在94.0进行做多，收益为0.94\n",
      "在95.0进行做空，收益为1.62\n",
      "在97.0进行做多，收益为-0.14\n",
      "在98.0进行做多，收益为0.84\n",
      "在99.0进行做多，收益为0.44\n",
      "在101.0进行做多，收益为1.41\n",
      "在102.0进行做多，收益为1.9\n",
      "在103.0进行做多，收益为1.09\n",
      "在104.0进行做多，收益为1.92\n",
      "在106.0进行做多，收益为1.77\n",
      "在109.0进行做多，收益为2.85\n",
      "在110.0进行做多，收益为-0.32\n",
      "在113.0进行做多，收益为1.12\n",
      "在115.0进行做多，收益为0.7\n",
      "在116.0进行做多，收益为1.63\n",
      "在118.0进行做多，收益为1.01\n",
      "在121.0进行做空，收益为2.73\n",
      "在123.0进行做多，收益为1.23\n",
      "在124.0进行做多，收益为1.53\n",
      "在125.0进行做多，收益为-0.29\n",
      "在127.0进行做多，收益为0.16\n",
      "在128.0进行做多，收益为1.5\n",
      "在129.0进行做多，收益为2.17\n",
      "在130.0进行做多，收益为2.15\n",
      "在135.0进行做空，收益为2.54\n",
      "在137.0进行做多，收益为-2.43\n",
      "在139.0进行做空，收益为1.27\n",
      "在141.0进行做空，收益为3.14\n",
      "在142.0进行做空，收益为0.34\n",
      "在143.0进行做空，收益为3.48\n",
      "在144.0进行做空，收益为2.15\n",
      "在145.0进行做空，收益为-0.66\n",
      "在152.0进行做空，收益为2.63\n",
      "在153.0进行做空，收益为-1.0\n",
      "在154.0进行做空，收益为0.95\n",
      "在157.0进行做多，收益为2.27\n",
      "在161.0进行做多，收益为1.25\n",
      "在162.0进行做多，收益为1.0\n",
      "在163.0进行做多，收益为-0.43\n",
      "在167.0进行做空，收益为1.73\n",
      "在168.0进行做空，收益为0.16\n",
      "在171.0进行做多，收益为0.34\n",
      "在172.0进行做多，收益为2.44\n",
      "在177.0进行做空，收益为1.13\n",
      "在178.0进行做多，收益为0.28\n",
      "在179.0进行做多，收益为0.55\n",
      "在180.0进行做多，收益为0.88\n",
      "在186.0进行做多，收益为0.44\n",
      "在189.0进行做多，收益为1.45\n",
      "在190.0进行做多，收益为0.06\n",
      "在193.0进行做多，收益为-1.0\n",
      "在195.0进行做多，收益为3.14\n",
      "在197.0进行做空，收益为-0.32\n",
      "在198.0进行做空，收益为0.32\n",
      "在199.0进行做空，收益为-0.21\n",
      "在200.0进行做空，收益为-0.19\n",
      "在201.0进行做空，收益为0.97\n",
      "在202.0进行做空，收益为0.66\n",
      "在203.0进行做空，收益为-0.51\n",
      "在205.0进行做空，收益为0.87\n",
      "在207.0进行做多，收益为0.67\n",
      "在208.0进行做空，收益为0.87\n",
      "在209.0进行做多，收益为-1.11\n",
      "在211.0进行做多，收益为0.43\n",
      "在213.0进行做空，收益为0.24\n",
      "在214.0进行做多，收益为0.63\n",
      "在216.0进行做多，收益为0.17\n",
      "在217.0进行做多，收益为1.63\n",
      "在221.0进行做多，收益为0.11\n",
      "在222.0进行做空，收益为2.85\n",
      "在223.0进行做多，收益为0.1\n",
      "在224.0进行做空，收益为0.06\n",
      "在225.0进行做空，收益为-1.12\n",
      "在228.0进行做多，收益为1.26\n",
      "在229.0进行做多，收益为0.18\n",
      "在231.0进行做多，收益为1.34\n",
      "在233.0进行做多，收益为0.35\n",
      "在236.0进行做空，收益为-0.16\n",
      "在237.0进行做空，收益为1.22\n",
      "在238.0进行做空，收益为3.22\n",
      "在239.0进行做空，收益为3.53\n",
      "在241.0进行做多，收益为1.89\n",
      "在242.0进行做多，收益为-0.8\n",
      "在243.0进行做多，收益为2.56\n",
      "在245.0进行做多，收益为0.91\n",
      "在247.0进行做多，收益为-0.55\n",
      "在248.0进行做多，收益为1.3\n",
      "在249.0进行做多，收益为1.14\n",
      "在251.0进行做空，收益为0.84\n",
      "在252.0进行做空，收益为0.54\n",
      "在253.0进行做空，收益为0.1\n",
      "在254.0进行做空，收益为2.1\n",
      "在255.0进行做多，收益为1.18\n",
      "在257.0进行做空，收益为1.91\n",
      "在260.0进行做空，收益为-0.2\n",
      "在261.0进行做空，收益为1.96\n",
      "在266.0进行做多，收益为0.0\n",
      "在267.0进行做多，收益为-0.53\n",
      "在268.0进行做多，收益为1.88\n",
      "在269.0进行做多，收益为1.2\n",
      "在274.0进行做空，收益为1.5\n",
      "在275.0进行做空，收益为1.02\n",
      "在279.0进行做多，收益为0.64\n",
      "在281.0进行做多，收益为0.58\n",
      "在282.0进行做多，收益为0.14\n",
      "在283.0进行做多，收益为-1.02\n",
      "在284.0进行做多，收益为0.66\n",
      "在285.0进行做多，收益为1.32\n",
      "在286.0进行做多，收益为0.12\n",
      "在288.0进行做多，收益为1.15\n",
      "在290.0进行做空，收益为-0.37\n",
      "在291.0进行做空，收益为1.16\n",
      "在292.0进行做空，收益为-0.98\n",
      "在295.0进行做空，收益为-0.65\n",
      "在297.0进行做空，收益为0.34\n",
      "在298.0进行做空，收益为1.31\n",
      "在299.0进行做空，收益为0.69\n",
      "在301.0进行做空，收益为0.37\n",
      "在302.0进行做空，收益为1.04\n",
      "在306.0进行做多，收益为0.12\n",
      "在309.0进行做多，收益为1.62\n",
      "在311.0进行做空，收益为0.12\n",
      "在312.0进行做空，收益为-0.02\n",
      "在313.0进行做空，收益为-0.06\n",
      "在314.0进行做空，收益为0.98\n",
      "在315.0进行做多，收益为1.07\n",
      "在316.0进行做多，收益为0.45\n",
      "在320.0进行做空，收益为0.74\n",
      "在325.0进行做多，收益为0.93\n",
      "在326.0进行做多，收益为-0.16\n",
      "在329.0进行做多，收益为1.66\n",
      "在331.0进行做多，收益为0.57\n",
      "在334.0进行做空，收益为-0.58\n",
      "在336.0进行做空，收益为1.51\n",
      "在340.0进行做空，收益为0.57\n",
      "在344.0进行做多，收益为0.8\n",
      "在346.0进行做空，收益为0.45\n",
      "在347.0进行做空，收益为1.02\n",
      "在348.0进行做空，收益为1.03\n",
      "在350.0进行做空，收益为-0.46\n",
      "在353.0进行做空，收益为1.65\n",
      "在355.0进行做多，收益为0.85\n",
      "在356.0进行做多，收益为0.96\n",
      "在359.0进行做空，收益为0.93\n",
      "在360.0进行做空，收益为-0.17\n",
      "在361.0进行做空，收益为2.28\n",
      "在363.0进行做空，收益为1.95\n",
      "在364.0进行做多，收益为-1.21\n",
      "在365.0进行做多，收益为1.53\n",
      "在368.0进行做空，收益为0.26\n",
      "在369.0进行做空，收益为0.84\n",
      "在370.0进行做空，收益为1.06\n",
      "在372.0进行做空，收益为-0.39\n",
      "在373.0进行做空，收益为-0.24\n",
      "在375.0进行做空，收益为0.37\n",
      "在376.0进行做空，收益为1.29\n",
      "在377.0进行做多，收益为1.07\n",
      "在378.0进行做空，收益为2.03\n",
      "在379.0进行做多，收益为0.97\n",
      "在381.0进行做多，收益为0.83\n",
      "在382.0进行做空，收益为0.89\n",
      "在384.0进行做空，收益为1.23\n",
      "在385.0进行做空，收益为3.18\n",
      "在387.0进行做空，收益为0.91\n",
      "在388.0进行做多，收益为1.59\n",
      "在390.0进行做空，收益为3.06\n",
      "在392.0进行做多，收益为4.32\n",
      "在393.0进行做多，收益为1.97\n",
      "在394.0进行做多，收益为0.66\n",
      "在397.0进行做空，收益为-0.52\n",
      "在399.0进行做多，收益为-1.79\n",
      "在401.0进行做多，收益为-0.34\n",
      "在402.0进行做多，收益为2.9\n",
      "在403.0进行做多，收益为-0.73\n",
      "在404.0进行做多，收益为1.26\n",
      "在405.0进行做空，收益为0.28\n",
      "在406.0进行做空，收益为1.29\n",
      "在407.0进行做多，收益为0.52\n",
      "在408.0进行做空，收益为3.1\n",
      "在409.0进行做多，收益为1.95\n",
      "在411.0进行做多，收益为1.26\n",
      "在412.0进行做空，收益为0.07\n",
      "在413.0进行做空，收益为0.55\n",
      "在415.0进行做空，收益为1.55\n",
      "在416.0进行做空，收益为1.84\n",
      "在418.0进行做空，收益为4.93\n",
      "在420.0进行做多，收益为2.96\n",
      "在421.0进行做多，收益为0.64\n",
      "在422.0进行做多，收益为2.42\n",
      "在424.0进行做空，收益为2.52\n",
      "在425.0进行做空，收益为0.82\n",
      "在426.0进行做空，收益为-1.11\n",
      "在427.0进行做空，收益为-1.43\n",
      "在428.0进行做空，收益为0.43\n",
      "在429.0进行做空，收益为-0.76\n",
      "在431.0进行做多，收益为1.24\n",
      "在434.0进行做多，收益为1.95\n",
      "在435.0进行做空，收益为0.59\n",
      "在436.0进行做空，收益为2.34\n",
      "在437.0进行做空，收益为-0.61\n",
      "在438.0进行做空，收益为-0.25\n",
      "在442.0进行做多，收益为-0.22\n",
      "在443.0进行做多，收益为0.17\n",
      "在444.0进行做多，收益为1.86\n",
      "在445.0进行做多，收益为0.31\n",
      "在446.0进行做多，收益为0.98\n",
      "在448.0进行做多，收益为1.51\n",
      "在450.0进行做多，收益为0.79\n",
      "在451.0进行做多，收益为1.33\n",
      "在452.0进行做多，收益为-0.65\n",
      "在453.0进行做多，收益为1.39\n",
      "在455.0进行做空，收益为0.09\n",
      "在456.0进行做空，收益为1.27\n",
      "在457.0进行做多，收益为1.71\n",
      "在459.0进行做多，收益为1.11\n",
      "在460.0进行做多，收益为1.06\n",
      "在462.0进行做多，收益为1.45\n",
      "在463.0进行做多，收益为-0.4\n",
      "在464.0进行做多，收益为0.65\n",
      "在466.0进行做空，收益为1.47\n",
      "在468.0进行做空，收益为0.32\n",
      "在469.0进行做空，收益为1.67\n",
      "在470.0进行做空，收益为0.94\n",
      "在471.0进行做空，收益为-0.16\n",
      "在472.0进行做空，收益为-0.02\n",
      "在473.0进行做空，收益为1.69\n",
      "在475.0进行做空，收益为0.56\n",
      "在477.0进行做空，收益为1.12\n",
      "在481.0进行做多，收益为-0.49\n",
      "在482.0进行做空，收益为-0.02\n",
      "在485.0进行做空，收益为1.96\n",
      "在487.0进行做多，收益为0.86\n",
      "在488.0进行做多，收益为1.34\n",
      "在491.0进行做空，收益为1.11\n",
      "在492.0进行做多，收益为2.04\n",
      "在495.0进行做空，收益为0.19\n",
      "在497.0进行做空，收益为0.88\n",
      "在499.0进行做多，收益为0.72\n",
      "在500.0进行做多，收益为-0.48\n",
      "在507.0进行做空，收益为0.86\n",
      "在509.0进行做多，收益为-0.22\n",
      "在511.0进行做空，收益为-0.07\n",
      "在512.0进行做空，收益为0.42\n",
      "在514.0进行做空，收益为-0.42\n",
      "在515.0进行做空，收益为1.12\n",
      "在516.0进行做空，收益为0.93\n",
      "在519.0进行做多，收益为0.13\n",
      "在527.0进行做空，收益为0.57\n",
      "在528.0进行做空，收益为2.21\n",
      "在529.0进行做多，收益为0.19\n",
      "在531.0进行做空，收益为0.82\n",
      "在534.0进行做空，收益为0.21\n",
      "在535.0进行做空，收益为1.59\n",
      "在536.0进行做空，收益为0.58\n",
      "在538.0进行做空，收益为2.94\n",
      "在541.0进行做多，收益为-0.71\n",
      "在542.0进行做空，收益为2.48\n",
      "在543.0进行做多，收益为-0.9\n",
      "在544.0进行做多，收益为3.56\n",
      "在545.0进行做多，收益为1.21\n",
      "在547.0进行做多，收益为3.26\n",
      "在552.0进行做多，收益为0.16\n",
      "在553.0进行做多，收益为1.9\n",
      "在555.0进行做空，收益为0.39\n",
      "在563.0进行做多，收益为3.08\n",
      "在564.0进行做多，收益为0.13\n",
      "在565.0进行做多，收益为1.09\n",
      "在566.0进行做多，收益为-0.62\n",
      "在575.0进行做空，收益为0.08\n",
      "在576.0进行做空，收益为-0.05\n",
      "在577.0进行做空，收益为1.54\n",
      "在581.0进行做空，收益为0.21\n",
      "在583.0进行做多，收益为1.17\n",
      "在589.0进行做多，收益为1.95\n",
      "在591.0进行做多，收益为0.8\n",
      "在592.0进行做多，收益为0.1\n",
      "在594.0进行做多，收益为0.2\n",
      "在595.0进行做多，收益为1.39\n",
      "在596.0进行做多，收益为1.57\n",
      "在599.0进行做多，收益为0.63\n",
      "在603.0进行做多，收益为0.94\n",
      "在604.0进行做空，收益为0.36\n",
      "在608.0进行做多，收益为-0.44\n",
      "在609.0进行做多，收益为1.35\n",
      "在610.0进行做多，收益为-0.61\n",
      "在611.0进行做多，收益为0.93\n",
      "在612.0进行做空，收益为-0.02\n",
      "在613.0进行做空，收益为0.51\n",
      "在615.0进行做空，收益为1.42\n",
      "在616.0进行做空，收益为-2.43\n",
      "在617.0进行做空，收益为-0.27\n",
      "在618.0进行做空，收益为0.89\n",
      "在621.0进行做空，收益为0.42\n",
      "在622.0进行做多，收益为0.62\n",
      "在623.0进行做多，收益为1.43\n",
      "在626.0进行做空，收益为0.53\n",
      "在627.0进行做空，收益为1.46\n",
      "在628.0进行做空，收益为0.37\n",
      "在630.0进行做空，收益为1.32\n",
      "在632.0进行做空，收益为0.6\n",
      "在633.0进行做空，收益为-0.05\n",
      "在636.0进行做空，收益为0.51\n",
      "在638.0进行做多，收益为0.43\n",
      "在640.0进行做空，收益为0.3\n",
      "在641.0进行做空，收益为0.37\n",
      "在642.0进行做空，收益为0.3\n",
      "在643.0进行做多，收益为0.15\n",
      "在646.0进行做多，收益为0.99\n",
      "在649.0进行做多，收益为0.63\n",
      "在650.0进行做多，收益为-0.44\n",
      "在652.0进行做空，收益为0.07\n",
      "在653.0进行做空，收益为0.68\n",
      "在656.0进行做空，收益为-0.31\n",
      "在657.0进行做空，收益为0.89\n",
      "在659.0进行做空，收益为1.95\n",
      "在660.0进行做空，收益为1.24\n",
      "在663.0进行做多，收益为0.73\n",
      "在664.0进行做多，收益为1.03\n",
      "在668.0进行做空，收益为0.86\n",
      "在669.0进行做空，收益为0.77\n",
      "在671.0进行做空，收益为1.33\n",
      "在675.0进行做多，收益为-0.1\n",
      "在677.0进行做多，收益为0.61\n",
      "在678.0进行做空，收益为1.41\n",
      "在679.0进行做空，收益为1.38\n",
      "在683.0进行做多，收益为0.1\n",
      "在685.0进行做多，收益为0.21\n",
      "在686.0进行做多，收益为1.44\n",
      "在690.0进行做多，收益为0.82\n",
      "在691.0进行做多，收益为0.45\n",
      "在696.0进行做多，收益为0.97\n",
      "在697.0进行做空，收益为0.81\n",
      "在700.0进行做空，收益为1.4\n",
      "在703.0进行做多，收益为-0.49\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "capital_list = [1,]\n",
    "cap = 1\n",
    "\n",
    "for row in data_to_backtest.iterrows():\n",
    "    if row[1]['predict'] > 0.2:\n",
    "        print(f'在{row[1][\"day\"]}进行做多，收益为{row[1][\"index_return\"]}')\n",
    "        cap += cap * row[1]['index_return'] / 100\n",
    "        capital_list.append(cap)\n",
    "    elif row[1]['predict'] < -0.2:\n",
    "        print(f\"在{row[1]['day']}进行做空，收益为{-row[1]['index_return']}\")\n",
    "        cap -= cap * row[1][\"index_return\"] / 100\n",
    "        capital_list.append(cap)\n",
    "    else:\n",
    "        capital_list.append(cap)\n",
    "\n",
    "plt.plot(capital_list)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T12:47:52.441552Z",
     "start_time": "2023-12-14T12:47:52.350139Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "     day  index_return   predict        cap\n0      1          0.10 -0.159120   1.000000\n1      2          0.02 -0.062834   1.000000\n2      3         -0.29 -0.025465   1.000000\n3      4         -1.15 -0.148071   1.000000\n4      5          0.36  0.099315   1.000000\n..   ...           ...       ...        ...\n700  701          0.92 -0.082124  21.176880\n701  702         -0.10 -0.193279  21.176880\n702  703         -0.49  0.234850  21.176880\n703  704          0.52  0.106943  21.073113\n704  705           NaN  0.052091  21.073113\n\n[705 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day</th>\n      <th>index_return</th>\n      <th>predict</th>\n      <th>cap</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.10</td>\n      <td>-0.159120</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.02</td>\n      <td>-0.062834</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>-0.29</td>\n      <td>-0.025465</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>-1.15</td>\n      <td>-0.148071</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.36</td>\n      <td>0.099315</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>700</th>\n      <td>701</td>\n      <td>0.92</td>\n      <td>-0.082124</td>\n      <td>21.176880</td>\n    </tr>\n    <tr>\n      <th>701</th>\n      <td>702</td>\n      <td>-0.10</td>\n      <td>-0.193279</td>\n      <td>21.176880</td>\n    </tr>\n    <tr>\n      <th>702</th>\n      <td>703</td>\n      <td>-0.49</td>\n      <td>0.234850</td>\n      <td>21.176880</td>\n    </tr>\n    <tr>\n      <th>703</th>\n      <td>704</td>\n      <td>0.52</td>\n      <td>0.106943</td>\n      <td>21.073113</td>\n    </tr>\n    <tr>\n      <th>704</th>\n      <td>705</td>\n      <td>NaN</td>\n      <td>0.052091</td>\n      <td>21.073113</td>\n    </tr>\n  </tbody>\n</table>\n<p>705 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_backtest['cap'] = pd.Series(capital_list)\n",
    "data_to_backtest"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T12:48:00.654660Z",
     "start_time": "2023-12-14T12:48:00.630664Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
