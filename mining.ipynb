{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from itertools import combinations\n",
    "import shutup\n",
    "\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "     day  feature1  feature2  feature3  index_return\n0      1         9        59        72           NaN\n1      2         5        17        64        0.0010\n2      3        20        38        69        0.0002\n3      4        14        15        66       -0.0029\n4      5        14        18        61       -0.0115\n..   ...       ...       ...       ...           ...\n700  701         3        14        20       -0.0140\n701  702        18        21        46        0.0092\n702  703         2         2        42       -0.0010\n703  704         2         5        41       -0.0049\n704  705         3         5        55        0.0052\n\n[705 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day</th>\n      <th>feature1</th>\n      <th>feature2</th>\n      <th>feature3</th>\n      <th>index_return</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>9</td>\n      <td>59</td>\n      <td>72</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>5</td>\n      <td>17</td>\n      <td>64</td>\n      <td>0.0010</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>20</td>\n      <td>38</td>\n      <td>69</td>\n      <td>0.0002</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>14</td>\n      <td>15</td>\n      <td>66</td>\n      <td>-0.0029</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>14</td>\n      <td>18</td>\n      <td>61</td>\n      <td>-0.0115</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>700</th>\n      <td>701</td>\n      <td>3</td>\n      <td>14</td>\n      <td>20</td>\n      <td>-0.0140</td>\n    </tr>\n    <tr>\n      <th>701</th>\n      <td>702</td>\n      <td>18</td>\n      <td>21</td>\n      <td>46</td>\n      <td>0.0092</td>\n    </tr>\n    <tr>\n      <th>702</th>\n      <td>703</td>\n      <td>2</td>\n      <td>2</td>\n      <td>42</td>\n      <td>-0.0010</td>\n    </tr>\n    <tr>\n      <th>703</th>\n      <td>704</td>\n      <td>2</td>\n      <td>5</td>\n      <td>41</td>\n      <td>-0.0049</td>\n    </tr>\n    <tr>\n      <th>704</th>\n      <td>705</td>\n      <td>3</td>\n      <td>5</td>\n      <td>55</td>\n      <td>0.0052</td>\n    </tr>\n  </tbody>\n</table>\n<p>705 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('D:\\PycharmProjects\\QuantInterview\\Dataset.csv', dtype={'feature1': np.int8, 'feature2': np.int8, 'feature3': np.int8, 'index_return': str})\n",
    "data['index_return'] = data['index_return'].str.rstrip('%').astype('float') / 100.0\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "        print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        print(f\"Decreased by {decrease:.2f}%\")\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 根据已有特征构建出新的特征\n",
    "\n",
    "由于根据现有feature无法得知具体是什么feature，因此尽可能多地根据给到地3个feature构建新的feature，并用机器学习模型预测"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "     day  feature1  feature2  feature3  index_return  feature1_feature2_sum  \\\n0      1         9        59        72           NaN                     68   \n1      2         5        17        64        0.0010                     22   \n2      3        20        38        69        0.0002                     58   \n3      4        14        15        66       -0.0029                     29   \n4      5        14        18        61       -0.0115                     32   \n..   ...       ...       ...       ...           ...                    ...   \n700  701         3        14        20       -0.0140                     17   \n701  702        18        21        46        0.0092                     39   \n702  703         2         2        42       -0.0010                      4   \n703  704         2         5        41       -0.0049                      7   \n704  705         3         5        55        0.0052                      8   \n\n     feature1_feature2_difference  feature1_feature2_product  \\\n0                             -50                         19   \n1                             -12                         85   \n2                             -18                         -8   \n3                              -1                        -46   \n4                              -4                         -4   \n..                            ...                        ...   \n700                           -11                         42   \n701                            -3                        122   \n702                             0                          4   \n703                            -3                         10   \n704                            -2                         15   \n\n     feature1_feature2_quotient  feature1_feature3_sum  \\\n0                      0.152542                     81   \n1                      0.294118                     69   \n2                      0.526316                     89   \n3                      0.933333                     80   \n4                      0.777778                     75   \n..                          ...                    ...   \n700                    0.214286                     23   \n701                    0.857143                     64   \n702                    1.000000                     44   \n703                    0.400000                     43   \n704                    0.600000                     58   \n\n     feature1_feature3_difference  feature1_feature3_product  \\\n0                             -63                       -120   \n1                             -59                         64   \n2                             -49                        100   \n3                             -52                       -100   \n4                             -47                         86   \n..                            ...                        ...   \n700                           -17                         60   \n701                           -28                         60   \n702                           -40                         84   \n703                           -39                         82   \n704                           -52                        -91   \n\n     feature1_feature3_quotient  feature2_feature3_sum  \\\n0                      0.125000                   -125   \n1                      0.078125                     81   \n2                      0.289855                    107   \n3                      0.212121                     81   \n4                      0.229508                     79   \n..                          ...                    ...   \n700                    0.150000                     34   \n701                    0.391304                     67   \n702                    0.047619                     44   \n703                    0.048780                     46   \n704                    0.054545                     60   \n\n     feature2_feature3_difference  feature2_feature3_product  \\\n0                             -13                       -104   \n1                             -47                         64   \n2                             -31                         62   \n3                             -51                        -34   \n4                             -43                         74   \n..                            ...                        ...   \n700                            -6                         24   \n701                           -25                        -58   \n702                           -40                         84   \n703                           -36                        -51   \n704                           -50                         19   \n\n     feature2_feature3_quotient  \n0                      0.819444  \n1                      0.265625  \n2                      0.550725  \n3                      0.227273  \n4                      0.295082  \n..                          ...  \n700                    0.700000  \n701                    0.456522  \n702                    0.047619  \n703                    0.121951  \n704                    0.090909  \n\n[705 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day</th>\n      <th>feature1</th>\n      <th>feature2</th>\n      <th>feature3</th>\n      <th>index_return</th>\n      <th>feature1_feature2_sum</th>\n      <th>feature1_feature2_difference</th>\n      <th>feature1_feature2_product</th>\n      <th>feature1_feature2_quotient</th>\n      <th>feature1_feature3_sum</th>\n      <th>feature1_feature3_difference</th>\n      <th>feature1_feature3_product</th>\n      <th>feature1_feature3_quotient</th>\n      <th>feature2_feature3_sum</th>\n      <th>feature2_feature3_difference</th>\n      <th>feature2_feature3_product</th>\n      <th>feature2_feature3_quotient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>9</td>\n      <td>59</td>\n      <td>72</td>\n      <td>NaN</td>\n      <td>68</td>\n      <td>-50</td>\n      <td>19</td>\n      <td>0.152542</td>\n      <td>81</td>\n      <td>-63</td>\n      <td>-120</td>\n      <td>0.125000</td>\n      <td>-125</td>\n      <td>-13</td>\n      <td>-104</td>\n      <td>0.819444</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>5</td>\n      <td>17</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>22</td>\n      <td>-12</td>\n      <td>85</td>\n      <td>0.294118</td>\n      <td>69</td>\n      <td>-59</td>\n      <td>64</td>\n      <td>0.078125</td>\n      <td>81</td>\n      <td>-47</td>\n      <td>64</td>\n      <td>0.265625</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>20</td>\n      <td>38</td>\n      <td>69</td>\n      <td>0.0002</td>\n      <td>58</td>\n      <td>-18</td>\n      <td>-8</td>\n      <td>0.526316</td>\n      <td>89</td>\n      <td>-49</td>\n      <td>100</td>\n      <td>0.289855</td>\n      <td>107</td>\n      <td>-31</td>\n      <td>62</td>\n      <td>0.550725</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>14</td>\n      <td>15</td>\n      <td>66</td>\n      <td>-0.0029</td>\n      <td>29</td>\n      <td>-1</td>\n      <td>-46</td>\n      <td>0.933333</td>\n      <td>80</td>\n      <td>-52</td>\n      <td>-100</td>\n      <td>0.212121</td>\n      <td>81</td>\n      <td>-51</td>\n      <td>-34</td>\n      <td>0.227273</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>14</td>\n      <td>18</td>\n      <td>61</td>\n      <td>-0.0115</td>\n      <td>32</td>\n      <td>-4</td>\n      <td>-4</td>\n      <td>0.777778</td>\n      <td>75</td>\n      <td>-47</td>\n      <td>86</td>\n      <td>0.229508</td>\n      <td>79</td>\n      <td>-43</td>\n      <td>74</td>\n      <td>0.295082</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>700</th>\n      <td>701</td>\n      <td>3</td>\n      <td>14</td>\n      <td>20</td>\n      <td>-0.0140</td>\n      <td>17</td>\n      <td>-11</td>\n      <td>42</td>\n      <td>0.214286</td>\n      <td>23</td>\n      <td>-17</td>\n      <td>60</td>\n      <td>0.150000</td>\n      <td>34</td>\n      <td>-6</td>\n      <td>24</td>\n      <td>0.700000</td>\n    </tr>\n    <tr>\n      <th>701</th>\n      <td>702</td>\n      <td>18</td>\n      <td>21</td>\n      <td>46</td>\n      <td>0.0092</td>\n      <td>39</td>\n      <td>-3</td>\n      <td>122</td>\n      <td>0.857143</td>\n      <td>64</td>\n      <td>-28</td>\n      <td>60</td>\n      <td>0.391304</td>\n      <td>67</td>\n      <td>-25</td>\n      <td>-58</td>\n      <td>0.456522</td>\n    </tr>\n    <tr>\n      <th>702</th>\n      <td>703</td>\n      <td>2</td>\n      <td>2</td>\n      <td>42</td>\n      <td>-0.0010</td>\n      <td>4</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1.000000</td>\n      <td>44</td>\n      <td>-40</td>\n      <td>84</td>\n      <td>0.047619</td>\n      <td>44</td>\n      <td>-40</td>\n      <td>84</td>\n      <td>0.047619</td>\n    </tr>\n    <tr>\n      <th>703</th>\n      <td>704</td>\n      <td>2</td>\n      <td>5</td>\n      <td>41</td>\n      <td>-0.0049</td>\n      <td>7</td>\n      <td>-3</td>\n      <td>10</td>\n      <td>0.400000</td>\n      <td>43</td>\n      <td>-39</td>\n      <td>82</td>\n      <td>0.048780</td>\n      <td>46</td>\n      <td>-36</td>\n      <td>-51</td>\n      <td>0.121951</td>\n    </tr>\n    <tr>\n      <th>704</th>\n      <td>705</td>\n      <td>3</td>\n      <td>5</td>\n      <td>55</td>\n      <td>0.0052</td>\n      <td>8</td>\n      <td>-2</td>\n      <td>15</td>\n      <td>0.600000</td>\n      <td>58</td>\n      <td>-52</td>\n      <td>-91</td>\n      <td>0.054545</td>\n      <td>60</td>\n      <td>-50</td>\n      <td>19</td>\n      <td>0.090909</td>\n    </tr>\n  </tbody>\n</table>\n<p>705 rows × 17 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先构建各个feature的和与差，积与熵\n",
    "ori_features = ['feature1', 'feature2', 'feature3']\n",
    "for c in combinations(ori_features, 2):\n",
    "    data[f\"{c[0]}_{c[1]}_sum\"] = data[c[0]] + data[c[1]]  # 和\n",
    "    data[f\"{c[0]}_{c[1]}_difference\"] = data[c[0]] - data[c[1]]  # 差\n",
    "    data[f\"{c[0]}_{c[1]}_product\"] = data[c[0]] * data[c[1]]  # 积\n",
    "    data[f\"{c[0]}_{c[1]}_quotient\"] = data[c[0]] / data[c[1]]  # 商\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "     day  feature1  feature2  feature3  index_return  feature1_feature2_sum  \\\n0      1         9        59        72           NaN                     68   \n1      2         5        17        64        0.0010                     22   \n2      3        20        38        69        0.0002                     58   \n3      4        14        15        66       -0.0029                     29   \n4      5        14        18        61       -0.0115                     32   \n..   ...       ...       ...       ...           ...                    ...   \n700  701         3        14        20       -0.0140                     17   \n701  702        18        21        46        0.0092                     39   \n702  703         2         2        42       -0.0010                      4   \n703  704         2         5        41       -0.0049                      7   \n704  705         3         5        55        0.0052                      8   \n\n     feature1_feature2_difference  feature1_feature2_product  \\\n0                             -50                         19   \n1                             -12                         85   \n2                             -18                         -8   \n3                              -1                        -46   \n4                              -4                         -4   \n..                            ...                        ...   \n700                           -11                         42   \n701                            -3                        122   \n702                             0                          4   \n703                            -3                         10   \n704                            -2                         15   \n\n     feature1_feature2_quotient  feature1_feature3_sum  ...  \\\n0                      0.152542                     81  ...   \n1                      0.294118                     69  ...   \n2                      0.526316                     89  ...   \n3                      0.933333                     80  ...   \n4                      0.777778                     75  ...   \n..                          ...                    ...  ...   \n700                    0.214286                     23  ...   \n701                    0.857143                     64  ...   \n702                    1.000000                     44  ...   \n703                    0.400000                     43  ...   \n704                    0.600000                     58  ...   \n\n     feature2_feature3_quotient_5_vol  feature2_feature3_quotient_10_mean  \\\n0                                 NaN                                 NaN   \n1                                 NaN                                 NaN   \n2                                 NaN                                 NaN   \n3                                 NaN                                 NaN   \n4                            0.251271                                 NaN   \n..                                ...                                 ...   \n700                          0.144241                            0.516044   \n701                          0.147655                            0.510716   \n702                          0.287898                            0.473478   \n703                          0.333390                            0.449958   \n704                          0.283830                            0.410070   \n\n     feature2_feature3_quotient_10_bias  feature2_feature3_quotient_10_vol  \\\n0                                   NaN                                NaN   \n1                                   NaN                                NaN   \n2                                   NaN                                NaN   \n3                                   NaN                                NaN   \n4                                   NaN                                NaN   \n..                                  ...                                ...   \n700                            0.356474                           0.132196   \n701                           -0.106114                           0.133543   \n702                           -0.899427                           0.198008   \n703                           -0.728972                           0.225431   \n704                           -0.778308                           0.251394   \n\n     feature2_feature3_quotient_20_mean  feature2_feature3_quotient_20_bias  \\\n0                                   NaN                                 NaN   \n1                                   NaN                                 NaN   \n2                                   NaN                                 NaN   \n3                                   NaN                                 NaN   \n4                                   NaN                                 NaN   \n..                                  ...                                 ...   \n700                            0.525114                            0.333044   \n701                            0.535120                           -0.146879   \n702                            0.513890                           -0.907336   \n703                            0.503320                           -0.757707   \n704                            0.465678                           -0.804781   \n\n     feature2_feature3_quotient_20_vol  feature2_feature3_quotient_5_10  \\\n0                                  NaN                              NaN   \n1                                  NaN                              NaN   \n2                                  NaN                              NaN   \n3                                  NaN                              NaN   \n4                                  NaN                              NaN   \n..                                 ...                              ...   \n700                           0.165271                         1.148872   \n701                           0.153807                         1.154709   \n702                           0.188367                         1.063893   \n703                           0.204288                         0.940882   \n704                           0.207592                         0.691103   \n\n     feature2_feature3_quotient_5_20  feature2_feature3_quotient_10_20  \n0                                NaN                               NaN  \n1                                NaN                               NaN  \n2                                NaN                               NaN  \n3                                NaN                               NaN  \n4                                NaN                               NaN  \n..                               ...                               ...  \n700                         1.129028                          0.982727  \n701                         1.102049                          0.954395  \n702                         0.980229                          0.921361  \n703                         0.841130                          0.893980  \n704                         0.608575                          0.880586  \n\n[705 rows x 197 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day</th>\n      <th>feature1</th>\n      <th>feature2</th>\n      <th>feature3</th>\n      <th>index_return</th>\n      <th>feature1_feature2_sum</th>\n      <th>feature1_feature2_difference</th>\n      <th>feature1_feature2_product</th>\n      <th>feature1_feature2_quotient</th>\n      <th>feature1_feature3_sum</th>\n      <th>...</th>\n      <th>feature2_feature3_quotient_5_vol</th>\n      <th>feature2_feature3_quotient_10_mean</th>\n      <th>feature2_feature3_quotient_10_bias</th>\n      <th>feature2_feature3_quotient_10_vol</th>\n      <th>feature2_feature3_quotient_20_mean</th>\n      <th>feature2_feature3_quotient_20_bias</th>\n      <th>feature2_feature3_quotient_20_vol</th>\n      <th>feature2_feature3_quotient_5_10</th>\n      <th>feature2_feature3_quotient_5_20</th>\n      <th>feature2_feature3_quotient_10_20</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>9</td>\n      <td>59</td>\n      <td>72</td>\n      <td>NaN</td>\n      <td>68</td>\n      <td>-50</td>\n      <td>19</td>\n      <td>0.152542</td>\n      <td>81</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>5</td>\n      <td>17</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>22</td>\n      <td>-12</td>\n      <td>85</td>\n      <td>0.294118</td>\n      <td>69</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>20</td>\n      <td>38</td>\n      <td>69</td>\n      <td>0.0002</td>\n      <td>58</td>\n      <td>-18</td>\n      <td>-8</td>\n      <td>0.526316</td>\n      <td>89</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>14</td>\n      <td>15</td>\n      <td>66</td>\n      <td>-0.0029</td>\n      <td>29</td>\n      <td>-1</td>\n      <td>-46</td>\n      <td>0.933333</td>\n      <td>80</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>14</td>\n      <td>18</td>\n      <td>61</td>\n      <td>-0.0115</td>\n      <td>32</td>\n      <td>-4</td>\n      <td>-4</td>\n      <td>0.777778</td>\n      <td>75</td>\n      <td>...</td>\n      <td>0.251271</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>700</th>\n      <td>701</td>\n      <td>3</td>\n      <td>14</td>\n      <td>20</td>\n      <td>-0.0140</td>\n      <td>17</td>\n      <td>-11</td>\n      <td>42</td>\n      <td>0.214286</td>\n      <td>23</td>\n      <td>...</td>\n      <td>0.144241</td>\n      <td>0.516044</td>\n      <td>0.356474</td>\n      <td>0.132196</td>\n      <td>0.525114</td>\n      <td>0.333044</td>\n      <td>0.165271</td>\n      <td>1.148872</td>\n      <td>1.129028</td>\n      <td>0.982727</td>\n    </tr>\n    <tr>\n      <th>701</th>\n      <td>702</td>\n      <td>18</td>\n      <td>21</td>\n      <td>46</td>\n      <td>0.0092</td>\n      <td>39</td>\n      <td>-3</td>\n      <td>122</td>\n      <td>0.857143</td>\n      <td>64</td>\n      <td>...</td>\n      <td>0.147655</td>\n      <td>0.510716</td>\n      <td>-0.106114</td>\n      <td>0.133543</td>\n      <td>0.535120</td>\n      <td>-0.146879</td>\n      <td>0.153807</td>\n      <td>1.154709</td>\n      <td>1.102049</td>\n      <td>0.954395</td>\n    </tr>\n    <tr>\n      <th>702</th>\n      <td>703</td>\n      <td>2</td>\n      <td>2</td>\n      <td>42</td>\n      <td>-0.0010</td>\n      <td>4</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1.000000</td>\n      <td>44</td>\n      <td>...</td>\n      <td>0.287898</td>\n      <td>0.473478</td>\n      <td>-0.899427</td>\n      <td>0.198008</td>\n      <td>0.513890</td>\n      <td>-0.907336</td>\n      <td>0.188367</td>\n      <td>1.063893</td>\n      <td>0.980229</td>\n      <td>0.921361</td>\n    </tr>\n    <tr>\n      <th>703</th>\n      <td>704</td>\n      <td>2</td>\n      <td>5</td>\n      <td>41</td>\n      <td>-0.0049</td>\n      <td>7</td>\n      <td>-3</td>\n      <td>10</td>\n      <td>0.400000</td>\n      <td>43</td>\n      <td>...</td>\n      <td>0.333390</td>\n      <td>0.449958</td>\n      <td>-0.728972</td>\n      <td>0.225431</td>\n      <td>0.503320</td>\n      <td>-0.757707</td>\n      <td>0.204288</td>\n      <td>0.940882</td>\n      <td>0.841130</td>\n      <td>0.893980</td>\n    </tr>\n    <tr>\n      <th>704</th>\n      <td>705</td>\n      <td>3</td>\n      <td>5</td>\n      <td>55</td>\n      <td>0.0052</td>\n      <td>8</td>\n      <td>-2</td>\n      <td>15</td>\n      <td>0.600000</td>\n      <td>58</td>\n      <td>...</td>\n      <td>0.283830</td>\n      <td>0.410070</td>\n      <td>-0.778308</td>\n      <td>0.251394</td>\n      <td>0.465678</td>\n      <td>-0.804781</td>\n      <td>0.207592</td>\n      <td>0.691103</td>\n      <td>0.608575</td>\n      <td>0.880586</td>\n    </tr>\n  </tbody>\n</table>\n<p>705 rows × 197 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下面计算移动平均指标\n",
    "feature_cols = list(data.columns)\n",
    "feature_cols.remove('day')\n",
    "feature_cols.remove('index_return')\n",
    "\n",
    "periods = [5, 10, 20]  # 短线操作中常用的均线参数\n",
    "for ft in feature_cols:\n",
    "    for period in periods:\n",
    "        data[f'{ft}_{period}_mean'] = data[ft].rolling(period).mean()  # 每个feature的移动平均\n",
    "        data[f'{ft}_{period}_bias'] = data[ft] / data[f'{ft}_{period}_mean'] - 1  # 每个feature的乖离度\n",
    "        try:\n",
    "            data[f'{ft}_{period}_vol'] = np.sqrt(data[ft].rolling(period).var())  # 每个feature的波动率\n",
    "        except TypeError:\n",
    "            print(data[ft].rolling(period))\n",
    "            break\n",
    "    for c in combinations(periods, 2):\n",
    "        data[f'{ft}_{c[0]}_{c[1]}'] = data[ft].rolling(c[0]).mean() / data[ft].rolling(c[1]).mean()  # 每个feature的移动平均之比\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1.00 MB\n",
      "Memory usage after optimization is: 0.51 MB\n",
      "Decreased by 49.39%\n"
     ]
    },
    {
     "data": {
      "text/plain": "     day  feature1  feature2  feature3  index_return  feature1_feature2_sum  \\\n0      1         9        59        72           NaN                     68   \n1      2         5        17        64        0.0010                     22   \n2      3        20        38        69        0.0002                     58   \n3      4        14        15        66       -0.0029                     29   \n4      5        14        18        61       -0.0115                     32   \n..   ...       ...       ...       ...           ...                    ...   \n700  701         3        14        20       -0.0140                     17   \n701  702        18        21        46        0.0092                     39   \n702  703         2         2        42       -0.0010                      4   \n703  704         2         5        41       -0.0049                      7   \n704  705         3         5        55        0.0052                      8   \n\n     feature1_feature2_difference  feature1_feature2_product  \\\n0                             -50                         19   \n1                             -12                         85   \n2                             -18                         -8   \n3                              -1                        -46   \n4                              -4                         -4   \n..                            ...                        ...   \n700                           -11                         42   \n701                            -3                        122   \n702                             0                          4   \n703                            -3                         10   \n704                            -2                         15   \n\n     feature1_feature2_quotient  feature1_feature3_sum  ...  \\\n0                      0.152542                     81  ...   \n1                      0.294118                     69  ...   \n2                      0.526316                     89  ...   \n3                      0.933333                     80  ...   \n4                      0.777778                     75  ...   \n..                          ...                    ...  ...   \n700                    0.214286                     23  ...   \n701                    0.857143                     64  ...   \n702                    1.000000                     44  ...   \n703                    0.400000                     43  ...   \n704                    0.600000                     58  ...   \n\n     feature2_feature3_quotient_5_vol  feature2_feature3_quotient_10_mean  \\\n0                                 NaN                                 NaN   \n1                                 NaN                                 NaN   \n2                                 NaN                                 NaN   \n3                                 NaN                                 NaN   \n4                            0.251271                                 NaN   \n..                                ...                                 ...   \n700                          0.144241                            0.516044   \n701                          0.147655                            0.510716   \n702                          0.287898                            0.473478   \n703                          0.333390                            0.449958   \n704                          0.283830                            0.410070   \n\n     feature2_feature3_quotient_10_bias  feature2_feature3_quotient_10_vol  \\\n0                                   NaN                                NaN   \n1                                   NaN                                NaN   \n2                                   NaN                                NaN   \n3                                   NaN                                NaN   \n4                                   NaN                                NaN   \n..                                  ...                                ...   \n700                            0.356474                           0.132196   \n701                           -0.106114                           0.133543   \n702                           -0.899427                           0.198008   \n703                           -0.728972                           0.225431   \n704                           -0.778308                           0.251394   \n\n     feature2_feature3_quotient_20_mean  feature2_feature3_quotient_20_bias  \\\n0                                   NaN                                 NaN   \n1                                   NaN                                 NaN   \n2                                   NaN                                 NaN   \n3                                   NaN                                 NaN   \n4                                   NaN                                 NaN   \n..                                  ...                                 ...   \n700                            0.525114                            0.333044   \n701                            0.535120                           -0.146879   \n702                            0.513889                           -0.907336   \n703                            0.503320                           -0.757707   \n704                            0.465678                           -0.804781   \n\n     feature2_feature3_quotient_20_vol  feature2_feature3_quotient_5_10  \\\n0                                  NaN                              NaN   \n1                                  NaN                              NaN   \n2                                  NaN                              NaN   \n3                                  NaN                              NaN   \n4                                  NaN                              NaN   \n..                                 ...                              ...   \n700                           0.165271                         1.148872   \n701                           0.153807                         1.154709   \n702                           0.188367                         1.063893   \n703                           0.204288                         0.940882   \n704                           0.207592                         0.691103   \n\n     feature2_feature3_quotient_5_20  feature2_feature3_quotient_10_20  \n0                                NaN                               NaN  \n1                                NaN                               NaN  \n2                                NaN                               NaN  \n3                                NaN                               NaN  \n4                                NaN                               NaN  \n..                               ...                               ...  \n700                         1.129027                          0.982727  \n701                         1.102049                          0.954395  \n702                         0.980229                          0.921361  \n703                         0.841130                          0.893980  \n704                         0.608575                          0.880586  \n\n[705 rows x 197 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day</th>\n      <th>feature1</th>\n      <th>feature2</th>\n      <th>feature3</th>\n      <th>index_return</th>\n      <th>feature1_feature2_sum</th>\n      <th>feature1_feature2_difference</th>\n      <th>feature1_feature2_product</th>\n      <th>feature1_feature2_quotient</th>\n      <th>feature1_feature3_sum</th>\n      <th>...</th>\n      <th>feature2_feature3_quotient_5_vol</th>\n      <th>feature2_feature3_quotient_10_mean</th>\n      <th>feature2_feature3_quotient_10_bias</th>\n      <th>feature2_feature3_quotient_10_vol</th>\n      <th>feature2_feature3_quotient_20_mean</th>\n      <th>feature2_feature3_quotient_20_bias</th>\n      <th>feature2_feature3_quotient_20_vol</th>\n      <th>feature2_feature3_quotient_5_10</th>\n      <th>feature2_feature3_quotient_5_20</th>\n      <th>feature2_feature3_quotient_10_20</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>9</td>\n      <td>59</td>\n      <td>72</td>\n      <td>NaN</td>\n      <td>68</td>\n      <td>-50</td>\n      <td>19</td>\n      <td>0.152542</td>\n      <td>81</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>5</td>\n      <td>17</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>22</td>\n      <td>-12</td>\n      <td>85</td>\n      <td>0.294118</td>\n      <td>69</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>20</td>\n      <td>38</td>\n      <td>69</td>\n      <td>0.0002</td>\n      <td>58</td>\n      <td>-18</td>\n      <td>-8</td>\n      <td>0.526316</td>\n      <td>89</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>14</td>\n      <td>15</td>\n      <td>66</td>\n      <td>-0.0029</td>\n      <td>29</td>\n      <td>-1</td>\n      <td>-46</td>\n      <td>0.933333</td>\n      <td>80</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>14</td>\n      <td>18</td>\n      <td>61</td>\n      <td>-0.0115</td>\n      <td>32</td>\n      <td>-4</td>\n      <td>-4</td>\n      <td>0.777778</td>\n      <td>75</td>\n      <td>...</td>\n      <td>0.251271</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>700</th>\n      <td>701</td>\n      <td>3</td>\n      <td>14</td>\n      <td>20</td>\n      <td>-0.0140</td>\n      <td>17</td>\n      <td>-11</td>\n      <td>42</td>\n      <td>0.214286</td>\n      <td>23</td>\n      <td>...</td>\n      <td>0.144241</td>\n      <td>0.516044</td>\n      <td>0.356474</td>\n      <td>0.132196</td>\n      <td>0.525114</td>\n      <td>0.333044</td>\n      <td>0.165271</td>\n      <td>1.148872</td>\n      <td>1.129027</td>\n      <td>0.982727</td>\n    </tr>\n    <tr>\n      <th>701</th>\n      <td>702</td>\n      <td>18</td>\n      <td>21</td>\n      <td>46</td>\n      <td>0.0092</td>\n      <td>39</td>\n      <td>-3</td>\n      <td>122</td>\n      <td>0.857143</td>\n      <td>64</td>\n      <td>...</td>\n      <td>0.147655</td>\n      <td>0.510716</td>\n      <td>-0.106114</td>\n      <td>0.133543</td>\n      <td>0.535120</td>\n      <td>-0.146879</td>\n      <td>0.153807</td>\n      <td>1.154709</td>\n      <td>1.102049</td>\n      <td>0.954395</td>\n    </tr>\n    <tr>\n      <th>702</th>\n      <td>703</td>\n      <td>2</td>\n      <td>2</td>\n      <td>42</td>\n      <td>-0.0010</td>\n      <td>4</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1.000000</td>\n      <td>44</td>\n      <td>...</td>\n      <td>0.287898</td>\n      <td>0.473478</td>\n      <td>-0.899427</td>\n      <td>0.198008</td>\n      <td>0.513889</td>\n      <td>-0.907336</td>\n      <td>0.188367</td>\n      <td>1.063893</td>\n      <td>0.980229</td>\n      <td>0.921361</td>\n    </tr>\n    <tr>\n      <th>703</th>\n      <td>704</td>\n      <td>2</td>\n      <td>5</td>\n      <td>41</td>\n      <td>-0.0049</td>\n      <td>7</td>\n      <td>-3</td>\n      <td>10</td>\n      <td>0.400000</td>\n      <td>43</td>\n      <td>...</td>\n      <td>0.333390</td>\n      <td>0.449958</td>\n      <td>-0.728972</td>\n      <td>0.225431</td>\n      <td>0.503320</td>\n      <td>-0.757707</td>\n      <td>0.204288</td>\n      <td>0.940882</td>\n      <td>0.841130</td>\n      <td>0.893980</td>\n    </tr>\n    <tr>\n      <th>704</th>\n      <td>705</td>\n      <td>3</td>\n      <td>5</td>\n      <td>55</td>\n      <td>0.0052</td>\n      <td>8</td>\n      <td>-2</td>\n      <td>15</td>\n      <td>0.600000</td>\n      <td>58</td>\n      <td>...</td>\n      <td>0.283830</td>\n      <td>0.410070</td>\n      <td>-0.778308</td>\n      <td>0.251394</td>\n      <td>0.465678</td>\n      <td>-0.804781</td>\n      <td>0.207592</td>\n      <td>0.691103</td>\n      <td>0.608575</td>\n      <td>0.880586</td>\n    </tr>\n  </tbody>\n</table>\n<p>705 rows × 197 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = reduce_mem_usage(data, verbose=1)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 缩尾（4%）并标准化"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "     day  feature1  feature2  feature3  index_return  feature1_feature2_sum  \\\n0      1 -0.466642  1.317203  1.357251           NaN               0.942208   \n1      2 -0.615909 -0.452524  0.865544      0.117669               0.008084   \n2      3 -0.056160  0.432339  1.172861      0.038288               0.739137   \n3      4 -0.280059 -0.536797  0.988470     -0.269313               0.150233   \n4      5 -0.280059 -0.410388  0.681154     -1.122658               0.211154   \n..   ...       ...       ...       ...           ...                    ...   \n700  701 -0.690542 -0.578933 -1.838845     -1.370723              -0.093452   \n701  702 -0.130793 -0.283979 -0.240797      0.931324               0.353303   \n702  703 -0.727858 -1.084569 -0.486650     -0.080783              -0.357443   \n703  704 -0.727858 -0.958160 -0.548114     -0.467765              -0.296522   \n704  705 -0.690542 -0.958160  0.312373      0.534419              -0.276215   \n\n     feature1_feature2_difference  feature1_feature2_product  \\\n0                       -1.920112                   0.181151   \n1                       -0.358175                   1.200372   \n2                       -0.748659                  -0.235803   \n3                        0.357713                  -0.822627   \n4                        0.162471                  -0.174032   \n..                            ...                        ...   \n700                     -0.293094                   0.536334   \n701                      0.227552                   1.617326   \n702                      0.422794                  -0.050490   \n703                      0.227552                   0.042166   \n704                      0.292632                   0.119380   \n\n     feature1_feature2_quotient  feature1_feature3_sum  ...  \\\n0                     -0.650135               0.527213  ...   \n1                     -0.525788               0.040429  ...   \n2                     -0.321847               0.851735  ...   \n3                      0.035639               0.486647  ...   \n4                     -0.100986               0.283821  ...   \n..                          ...                    ...  ...   \n700                   -0.595905              -1.622747  ...   \n701                   -0.031279              -0.162397  ...   \n702                    0.094193              -0.973702  ...   \n703                   -0.432791              -1.014268  ...   \n704                   -0.257130              -0.405789  ...   \n\n     feature2_feature3_quotient_5_vol  feature2_feature3_quotient_10_mean  \\\n0                                 NaN                                 NaN   \n1                                 NaN                                 NaN   \n2                                 NaN                                 NaN   \n3                                 NaN                                 NaN   \n4                           -0.053922                                 NaN   \n..                                ...                                 ...   \n700                         -0.339185                           -0.239254   \n701                         -0.330087                           -0.245885   \n702                          0.043699                           -0.292229   \n703                          0.164947                           -0.321500   \n704                          0.032856                           -0.371143   \n\n     feature2_feature3_quotient_10_bias  feature2_feature3_quotient_10_vol  \\\n0                                   NaN                                NaN   \n1                                   NaN                                NaN   \n2                                   NaN                                NaN   \n3                                   NaN                                NaN   \n4                                   NaN                                NaN   \n..                                  ...                                ...   \n700                            0.494242                          -0.476520   \n701                           -0.236152                          -0.473627   \n702                           -1.455313                          -0.335132   \n703                           -1.219603                          -0.276219   \n704                           -1.297501                          -0.220441   \n\n     feature2_feature3_quotient_20_mean  feature2_feature3_quotient_20_bias  \\\n0                                   NaN                                 NaN   \n1                                   NaN                                 NaN   \n2                                   NaN                                 NaN   \n3                                   NaN                                 NaN   \n4                                   NaN                                 NaN   \n..                                  ...                                 ...   \n700                           -0.237888                            0.289437   \n701                           -0.225353                           -0.264600   \n702                           -0.251949                           -1.142495   \n703                           -0.265190                           -0.969758   \n704                           -0.312347                           -1.024103   \n\n     feature2_feature3_quotient_20_vol  feature2_feature3_quotient_5_10  \\\n0                                  NaN                              NaN   \n1                                  NaN                              NaN   \n2                                  NaN                              NaN   \n3                                  NaN                              NaN   \n4                                  NaN                              NaN   \n..                                 ...                              ...   \n700                          -0.397931                         0.432678   \n701                          -0.409356                         0.450037   \n702                          -0.374912                         0.179975   \n703                          -0.359044                        -0.185826   \n704                          -0.355752                        -0.928604   \n\n     feature2_feature3_quotient_5_20  feature2_feature3_quotient_10_20  \n0                                NaN                               NaN  \n1                                NaN                               NaN  \n2                                NaN                               NaN  \n3                                NaN                               NaN  \n4                                NaN                               NaN  \n..                               ...                               ...  \n700                         0.141660                         -0.059595  \n701                         0.097192                         -0.132994  \n702                        -0.103598                         -0.218575  \n703                        -0.332869                         -0.289509  \n704                        -0.716179                         -0.324210  \n\n[705 rows x 197 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day</th>\n      <th>feature1</th>\n      <th>feature2</th>\n      <th>feature3</th>\n      <th>index_return</th>\n      <th>feature1_feature2_sum</th>\n      <th>feature1_feature2_difference</th>\n      <th>feature1_feature2_product</th>\n      <th>feature1_feature2_quotient</th>\n      <th>feature1_feature3_sum</th>\n      <th>...</th>\n      <th>feature2_feature3_quotient_5_vol</th>\n      <th>feature2_feature3_quotient_10_mean</th>\n      <th>feature2_feature3_quotient_10_bias</th>\n      <th>feature2_feature3_quotient_10_vol</th>\n      <th>feature2_feature3_quotient_20_mean</th>\n      <th>feature2_feature3_quotient_20_bias</th>\n      <th>feature2_feature3_quotient_20_vol</th>\n      <th>feature2_feature3_quotient_5_10</th>\n      <th>feature2_feature3_quotient_5_20</th>\n      <th>feature2_feature3_quotient_10_20</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>-0.466642</td>\n      <td>1.317203</td>\n      <td>1.357251</td>\n      <td>NaN</td>\n      <td>0.942208</td>\n      <td>-1.920112</td>\n      <td>0.181151</td>\n      <td>-0.650135</td>\n      <td>0.527213</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>-0.615909</td>\n      <td>-0.452524</td>\n      <td>0.865544</td>\n      <td>0.117669</td>\n      <td>0.008084</td>\n      <td>-0.358175</td>\n      <td>1.200372</td>\n      <td>-0.525788</td>\n      <td>0.040429</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>-0.056160</td>\n      <td>0.432339</td>\n      <td>1.172861</td>\n      <td>0.038288</td>\n      <td>0.739137</td>\n      <td>-0.748659</td>\n      <td>-0.235803</td>\n      <td>-0.321847</td>\n      <td>0.851735</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>-0.280059</td>\n      <td>-0.536797</td>\n      <td>0.988470</td>\n      <td>-0.269313</td>\n      <td>0.150233</td>\n      <td>0.357713</td>\n      <td>-0.822627</td>\n      <td>0.035639</td>\n      <td>0.486647</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>-0.280059</td>\n      <td>-0.410388</td>\n      <td>0.681154</td>\n      <td>-1.122658</td>\n      <td>0.211154</td>\n      <td>0.162471</td>\n      <td>-0.174032</td>\n      <td>-0.100986</td>\n      <td>0.283821</td>\n      <td>...</td>\n      <td>-0.053922</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>700</th>\n      <td>701</td>\n      <td>-0.690542</td>\n      <td>-0.578933</td>\n      <td>-1.838845</td>\n      <td>-1.370723</td>\n      <td>-0.093452</td>\n      <td>-0.293094</td>\n      <td>0.536334</td>\n      <td>-0.595905</td>\n      <td>-1.622747</td>\n      <td>...</td>\n      <td>-0.339185</td>\n      <td>-0.239254</td>\n      <td>0.494242</td>\n      <td>-0.476520</td>\n      <td>-0.237888</td>\n      <td>0.289437</td>\n      <td>-0.397931</td>\n      <td>0.432678</td>\n      <td>0.141660</td>\n      <td>-0.059595</td>\n    </tr>\n    <tr>\n      <th>701</th>\n      <td>702</td>\n      <td>-0.130793</td>\n      <td>-0.283979</td>\n      <td>-0.240797</td>\n      <td>0.931324</td>\n      <td>0.353303</td>\n      <td>0.227552</td>\n      <td>1.617326</td>\n      <td>-0.031279</td>\n      <td>-0.162397</td>\n      <td>...</td>\n      <td>-0.330087</td>\n      <td>-0.245885</td>\n      <td>-0.236152</td>\n      <td>-0.473627</td>\n      <td>-0.225353</td>\n      <td>-0.264600</td>\n      <td>-0.409356</td>\n      <td>0.450037</td>\n      <td>0.097192</td>\n      <td>-0.132994</td>\n    </tr>\n    <tr>\n      <th>702</th>\n      <td>703</td>\n      <td>-0.727858</td>\n      <td>-1.084569</td>\n      <td>-0.486650</td>\n      <td>-0.080783</td>\n      <td>-0.357443</td>\n      <td>0.422794</td>\n      <td>-0.050490</td>\n      <td>0.094193</td>\n      <td>-0.973702</td>\n      <td>...</td>\n      <td>0.043699</td>\n      <td>-0.292229</td>\n      <td>-1.455313</td>\n      <td>-0.335132</td>\n      <td>-0.251949</td>\n      <td>-1.142495</td>\n      <td>-0.374912</td>\n      <td>0.179975</td>\n      <td>-0.103598</td>\n      <td>-0.218575</td>\n    </tr>\n    <tr>\n      <th>703</th>\n      <td>704</td>\n      <td>-0.727858</td>\n      <td>-0.958160</td>\n      <td>-0.548114</td>\n      <td>-0.467765</td>\n      <td>-0.296522</td>\n      <td>0.227552</td>\n      <td>0.042166</td>\n      <td>-0.432791</td>\n      <td>-1.014268</td>\n      <td>...</td>\n      <td>0.164947</td>\n      <td>-0.321500</td>\n      <td>-1.219603</td>\n      <td>-0.276219</td>\n      <td>-0.265190</td>\n      <td>-0.969758</td>\n      <td>-0.359044</td>\n      <td>-0.185826</td>\n      <td>-0.332869</td>\n      <td>-0.289509</td>\n    </tr>\n    <tr>\n      <th>704</th>\n      <td>705</td>\n      <td>-0.690542</td>\n      <td>-0.958160</td>\n      <td>0.312373</td>\n      <td>0.534419</td>\n      <td>-0.276215</td>\n      <td>0.292632</td>\n      <td>0.119380</td>\n      <td>-0.257130</td>\n      <td>-0.405789</td>\n      <td>...</td>\n      <td>0.032856</td>\n      <td>-0.371143</td>\n      <td>-1.297501</td>\n      <td>-0.220441</td>\n      <td>-0.312347</td>\n      <td>-1.024103</td>\n      <td>-0.355752</td>\n      <td>-0.928604</td>\n      <td>-0.716179</td>\n      <td>-0.324210</td>\n    </tr>\n  </tbody>\n</table>\n<p>705 rows × 197 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "feature_cols = list(data.columns)\n",
    "feature_cols.remove('day')\n",
    "feature_cols.remove('index_return')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data[feature_cols + ['index_return']] = data[feature_cols + ['index_return']].clip(lower=data[feature_cols + ['index_return']].quantile(0.04), upper=data[feature_cols + ['index_return']].quantile(0.96), axis=1)\n",
    "data[feature_cols + ['index_return']] = pd.DataFrame(scaler.fit_transform(data[feature_cols + ['index_return']]), columns=[feature_cols + ['index_return']])\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 使用LightGBM进行表格数据预测并使用Shapley解释特征重要性"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-14 01:10:20,250] A new study created in memory with name: no-name-878913a4-7c81-4c45-ac1f-378a2d7c84ef\n",
      "[I 2023-12-14 01:10:20,362] Trial 0 finished with value: 0.7198320394813236 and parameters: {'lambda_l1': 7.867547422534093e-08, 'lambda_l2': 0.0002959490414679992, 'num_leaves': 84, 'feature_fraction': 0.574679673890646, 'bagging_fraction': 0.9341937446121666, 'bagging_freq': 4, 'min_child_samples': 31}. Best is trial 0 with value: 0.7198320394813236.\n",
      "[I 2023-12-14 01:10:20,431] Trial 1 finished with value: 0.8236554583449395 and parameters: {'lambda_l1': 0.008771999550090979, 'lambda_l2': 1.0249355784384713, 'num_leaves': 44, 'feature_fraction': 0.8811455071979257, 'bagging_fraction': 0.522369982880337, 'bagging_freq': 5, 'min_child_samples': 34}. Best is trial 0 with value: 0.7198320394813236.\n",
      "[I 2023-12-14 01:10:20,466] Trial 2 finished with value: 0.6772608322737856 and parameters: {'lambda_l1': 6.9837113297283056, 'lambda_l2': 6.701226626295953e-06, 'num_leaves': 85, 'feature_fraction': 0.49726194651417055, 'bagging_fraction': 0.4208256297049041, 'bagging_freq': 7, 'min_child_samples': 62}. Best is trial 2 with value: 0.6772608322737856.\n",
      "[I 2023-12-14 01:10:20,616] Trial 3 finished with value: 0.73187747403625 and parameters: {'lambda_l1': 0.035260666460209235, 'lambda_l2': 8.495379883264918e-06, 'num_leaves': 138, 'feature_fraction': 0.9153571830309697, 'bagging_fraction': 0.935050847800798, 'bagging_freq': 6, 'min_child_samples': 27}. Best is trial 2 with value: 0.6772608322737856.\n",
      "[I 2023-12-14 01:10:20,667] Trial 4 finished with value: 0.7336783655668068 and parameters: {'lambda_l1': 0.007263490937727727, 'lambda_l2': 1.5813975286238308e-06, 'num_leaves': 232, 'feature_fraction': 0.4552699206820952, 'bagging_fraction': 0.8577961728793607, 'bagging_freq': 6, 'min_child_samples': 98}. Best is trial 2 with value: 0.6772608322737856.\n",
      "[I 2023-12-14 01:10:20,715] Trial 5 finished with value: 0.7097199402141307 and parameters: {'lambda_l1': 0.03318807492464358, 'lambda_l2': 0.683490195822173, 'num_leaves': 207, 'feature_fraction': 0.5521401438382487, 'bagging_fraction': 0.5762818852122706, 'bagging_freq': 1, 'min_child_samples': 74}. Best is trial 2 with value: 0.6772608322737856.\n",
      "[I 2023-12-14 01:10:20,741] Trial 6 finished with value: 0.7239795681580415 and parameters: {'lambda_l1': 2.1189143418343965, 'lambda_l2': 2.6020112050265616e-05, 'num_leaves': 18, 'feature_fraction': 0.41113525231708364, 'bagging_fraction': 0.54824689088086, 'bagging_freq': 4, 'min_child_samples': 91}. Best is trial 2 with value: 0.6772608322737856.\n",
      "[I 2023-12-14 01:10:20,795] Trial 7 finished with value: 0.7113800586115714 and parameters: {'lambda_l1': 1.2701484925082908e-06, 'lambda_l2': 1.4869259437355555, 'num_leaves': 205, 'feature_fraction': 0.41617649941785423, 'bagging_fraction': 0.4477931705191346, 'bagging_freq': 4, 'min_child_samples': 43}. Best is trial 2 with value: 0.6772608322737856.\n",
      "[I 2023-12-14 01:10:20,849] Trial 8 finished with value: 0.7251464184773144 and parameters: {'lambda_l1': 1.3300850083079544e-07, 'lambda_l2': 0.0008836904284935244, 'num_leaves': 225, 'feature_fraction': 0.885543400600973, 'bagging_fraction': 0.4930804948343015, 'bagging_freq': 3, 'min_child_samples': 76}. Best is trial 2 with value: 0.6772608322737856.\n",
      "[I 2023-12-14 01:10:20,935] Trial 9 finished with value: 0.7929625209168001 and parameters: {'lambda_l1': 0.02259518181228616, 'lambda_l2': 4.090347959765094e-06, 'num_leaves': 72, 'feature_fraction': 0.5215230469980694, 'bagging_fraction': 0.5588558727492645, 'bagging_freq': 1, 'min_child_samples': 26}. Best is trial 2 with value: 0.6772608322737856.\n",
      "[I 2023-12-14 01:10:21,073] Trial 10 finished with value: 0.7764689042663937 and parameters: {'lambda_l1': 6.851616911130022, 'lambda_l2': 1.323418609556722e-08, 'num_leaves': 130, 'feature_fraction': 0.6631734873602168, 'bagging_fraction': 0.40175366637123944, 'bagging_freq': 7, 'min_child_samples': 5}. Best is trial 2 with value: 0.6772608322737856.\n",
      "[I 2023-12-14 01:10:21,157] Trial 11 finished with value: 0.7245610707040664 and parameters: {'lambda_l1': 0.6317377717519707, 'lambda_l2': 0.032401726713513004, 'num_leaves': 158, 'feature_fraction': 0.5904968300592212, 'bagging_fraction': 0.6730657196341118, 'bagging_freq': 1, 'min_child_samples': 65}. Best is trial 2 with value: 0.6772608322737856.\n",
      "[I 2023-12-14 01:10:21,254] Trial 12 finished with value: 0.7228397619390347 and parameters: {'lambda_l1': 0.00017035839347338399, 'lambda_l2': 5.626487792261605, 'num_leaves': 177, 'feature_fraction': 0.7343990558211535, 'bagging_fraction': 0.635613699241725, 'bagging_freq': 2, 'min_child_samples': 68}. Best is trial 2 with value: 0.6772608322737856.\n",
      "[I 2023-12-14 01:10:21,316] Trial 13 finished with value: 0.6909635807145679 and parameters: {'lambda_l1': 8.7261655318077, 'lambda_l2': 0.015786119443728045, 'num_leaves': 93, 'feature_fraction': 0.5240547423616162, 'bagging_fraction': 0.42615307814524406, 'bagging_freq': 7, 'min_child_samples': 59}. Best is trial 2 with value: 0.6772608322737856.\n",
      "[I 2023-12-14 01:10:21,382] Trial 14 finished with value: 0.6810393083107715 and parameters: {'lambda_l1': 9.571440963050131, 'lambda_l2': 0.018276944500698132, 'num_leaves': 90, 'feature_fraction': 0.49145512768136124, 'bagging_fraction': 0.4087378032817776, 'bagging_freq': 7, 'min_child_samples': 54}. Best is trial 2 with value: 0.6772608322737856.\n",
      "[I 2023-12-14 01:10:21,449] Trial 15 finished with value: 0.6790732544041445 and parameters: {'lambda_l1': 0.5647189774639609, 'lambda_l2': 0.01510005577015325, 'num_leaves': 106, 'feature_fraction': 0.47977221154836835, 'bagging_fraction': 0.4026490712285742, 'bagging_freq': 6, 'min_child_samples': 51}. Best is trial 2 with value: 0.6772608322737856.\n",
      "[I 2023-12-14 01:10:21,515] Trial 16 finished with value: 0.7456192173613059 and parameters: {'lambda_l1': 0.488543240274863, 'lambda_l2': 2.594969739320839e-07, 'num_leaves': 42, 'feature_fraction': 0.40076661745199604, 'bagging_fraction': 0.486389039735575, 'bagging_freq': 6, 'min_child_samples': 47}. Best is trial 2 with value: 0.6772608322737856.\n",
      "[I 2023-12-14 01:10:21,588] Trial 17 finished with value: 0.7581034275720973 and parameters: {'lambda_l1': 0.37642235577266825, 'lambda_l2': 0.00010515309878227385, 'num_leaves': 111, 'feature_fraction': 0.6390847691302832, 'bagging_fraction': 0.7379400887276328, 'bagging_freq': 5, 'min_child_samples': 87}. Best is trial 2 with value: 0.6772608322737856.\n",
      "[I 2023-12-14 01:10:21,660] Trial 18 finished with value: 0.8207420376199477 and parameters: {'lambda_l1': 0.00019481852568301391, 'lambda_l2': 0.0015783441412867347, 'num_leaves': 60, 'feature_fraction': 0.47685516499233604, 'bagging_fraction': 0.4697012028750617, 'bagging_freq': 5, 'min_child_samples': 42}. Best is trial 2 with value: 0.6772608322737856.\n",
      "[I 2023-12-14 01:10:21,916] Trial 19 finished with value: 0.7333720918092786 and parameters: {'lambda_l1': 0.24316496421671385, 'lambda_l2': 5.820602359261539e-05, 'num_leaves': 118, 'feature_fraction': 0.7306818727043656, 'bagging_fraction': 0.4743396212467296, 'bagging_freq': 6, 'min_child_samples': 6}. Best is trial 2 with value: 0.6772608322737856.\n",
      "[I 2023-12-14 01:10:21,975] Trial 20 finished with value: 0.7128987446241342 and parameters: {'lambda_l1': 0.0020324740529527098, 'lambda_l2': 0.0019666899855368, 'num_leaves': 17, 'feature_fraction': 0.47350918922126495, 'bagging_fraction': 0.6032067439341875, 'bagging_freq': 7, 'min_child_samples': 81}. Best is trial 2 with value: 0.6772608322737856.\n",
      "[I 2023-12-14 01:10:22,038] Trial 21 finished with value: 0.6756719261089045 and parameters: {'lambda_l1': 6.10135906372529, 'lambda_l2': 0.02707658588395438, 'num_leaves': 96, 'feature_fraction': 0.4847615024639124, 'bagging_fraction': 0.40036273875251266, 'bagging_freq': 7, 'min_child_samples': 56}. Best is trial 21 with value: 0.6756719261089045.\n",
      "[I 2023-12-14 01:10:22,110] Trial 22 finished with value: 0.7009346518940965 and parameters: {'lambda_l1': 1.7975429984036764, 'lambda_l2': 0.08560320263288214, 'num_leaves': 163, 'feature_fraction': 0.5829703890393703, 'bagging_fraction': 0.41522929878407033, 'bagging_freq': 7, 'min_child_samples': 59}. Best is trial 21 with value: 0.6756719261089045.\n",
      "[I 2023-12-14 01:10:22,186] Trial 23 finished with value: 0.7142832562060755 and parameters: {'lambda_l1': 0.11519828478654166, 'lambda_l2': 0.004911498669092951, 'num_leaves': 100, 'feature_fraction': 0.5183692445771324, 'bagging_fraction': 0.5119359352205308, 'bagging_freq': 6, 'min_child_samples': 51}. Best is trial 21 with value: 0.6756719261089045.\n",
      "[I 2023-12-14 01:10:22,252] Trial 24 finished with value: 0.6994365006121614 and parameters: {'lambda_l1': 2.3644227766347634, 'lambda_l2': 0.0003577606060940956, 'num_leaves': 57, 'feature_fraction': 0.44878468704524255, 'bagging_fraction': 0.4584740955651561, 'bagging_freq': 5, 'min_child_samples': 64}. Best is trial 21 with value: 0.6756719261089045.\n",
      "[I 2023-12-14 01:10:22,330] Trial 25 finished with value: 0.680627413446893 and parameters: {'lambda_l1': 1.2820719706274626, 'lambda_l2': 0.1463212589921109, 'num_leaves': 141, 'feature_fraction': 0.44140220425936016, 'bagging_fraction': 0.40082024955545564, 'bagging_freq': 7, 'min_child_samples': 40}. Best is trial 21 with value: 0.6756719261089045.\n",
      "[I 2023-12-14 01:10:22,405] Trial 26 finished with value: 0.7233120722604561 and parameters: {'lambda_l1': 0.14216346368168026, 'lambda_l2': 0.0058612813539319665, 'num_leaves': 112, 'feature_fraction': 0.5120165352561769, 'bagging_fraction': 0.5248429416879414, 'bagging_freq': 6, 'min_child_samples': 55}. Best is trial 21 with value: 0.6756719261089045.\n",
      "[I 2023-12-14 01:10:22,473] Trial 27 finished with value: 0.691987750665454 and parameters: {'lambda_l1': 3.037953053726954, 'lambda_l2': 0.11608570952486867, 'num_leaves': 77, 'feature_fraction': 0.6196271105344184, 'bagging_fraction': 0.456002005482411, 'bagging_freq': 7, 'min_child_samples': 71}. Best is trial 21 with value: 0.6756719261089045.\n",
      "[I 2023-12-14 01:10:22,551] Trial 28 finished with value: 0.6793106257820049 and parameters: {'lambda_l1': 8.713398798799659, 'lambda_l2': 0.004554056571964177, 'num_leaves': 33, 'feature_fraction': 0.5485280653597058, 'bagging_fraction': 0.44406281964678335, 'bagging_freq': 6, 'min_child_samples': 16}. Best is trial 21 with value: 0.6756719261089045.\n",
      "[I 2023-12-14 01:10:22,636] Trial 29 finished with value: 0.7710872333986268 and parameters: {'lambda_l1': 1.0664024955174345, 'lambda_l2': 0.0005032294083255105, 'num_leaves': 78, 'feature_fraction': 0.5732575935986318, 'bagging_fraction': 0.502252889998446, 'bagging_freq': 3, 'min_child_samples': 35}. Best is trial 21 with value: 0.6756719261089045.\n",
      "[I 2023-12-14 01:10:22,715] Trial 30 finished with value: 0.723255913388802 and parameters: {'lambda_l1': 0.12643381139035892, 'lambda_l2': 7.575020876442235e-05, 'num_leaves': 155, 'feature_fraction': 0.489062699634228, 'bagging_fraction': 0.5431659505698452, 'bagging_freq': 5, 'min_child_samples': 61}. Best is trial 21 with value: 0.6756719261089045.\n",
      "[I 2023-12-14 01:10:22,791] Trial 31 finished with value: 0.647437194658588 and parameters: {'lambda_l1': 8.159068587025143, 'lambda_l2': 0.002023672178083324, 'num_leaves': 6, 'feature_fraction': 0.5535793492575833, 'bagging_fraction': 0.4367578543726366, 'bagging_freq': 6, 'min_child_samples': 17}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:22,872] Trial 32 finished with value: 0.7380179607442261 and parameters: {'lambda_l1': 0.5862988096828085, 'lambda_l2': 0.00032391003004009943, 'num_leaves': 6, 'feature_fraction': 0.4516059200615832, 'bagging_fraction': 0.45087388184029914, 'bagging_freq': 7, 'min_child_samples': 16}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:22,945] Trial 33 finished with value: 0.6983161226758314 and parameters: {'lambda_l1': 3.0603588502606716, 'lambda_l2': 0.002944164782218056, 'num_leaves': 59, 'feature_fraction': 0.5453006220688363, 'bagging_fraction': 0.42855694943189365, 'bagging_freq': 6, 'min_child_samples': 47}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:23,045] Trial 34 finished with value: 0.7196732473961368 and parameters: {'lambda_l1': 2.36853700608214, 'lambda_l2': 0.0009981404595555948, 'num_leaves': 102, 'feature_fraction': 0.4855304502346573, 'bagging_fraction': 0.4935048245168079, 'bagging_freq': 5, 'min_child_samples': 23}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:23,112] Trial 35 finished with value: 0.6890019528601762 and parameters: {'lambda_l1': 0.1008323231365019, 'lambda_l2': 0.025949941846101953, 'num_leaves': 126, 'feature_fraction': 0.434103280795061, 'bagging_fraction': 0.4037917550898722, 'bagging_freq': 6, 'min_child_samples': 81}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:23,194] Trial 36 finished with value: 0.7262040108396478 and parameters: {'lambda_l1': 9.957543920040806, 'lambda_l2': 0.39415255360731316, 'num_leaves': 34, 'feature_fraction': 0.9915120183410588, 'bagging_fraction': 0.5162122712450693, 'bagging_freq': 7, 'min_child_samples': 30}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:23,284] Trial 37 finished with value: 0.679257568513764 and parameters: {'lambda_l1': 0.6849441533582876, 'lambda_l2': 0.0001587500876412585, 'num_leaves': 183, 'feature_fraction': 0.4660349590281336, 'bagging_fraction': 0.42991600306262795, 'bagging_freq': 4, 'min_child_samples': 38}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:23,354] Trial 38 finished with value: 0.7398798523305157 and parameters: {'lambda_l1': 3.206546547759398, 'lambda_l2': 0.009400730286597206, 'num_leaves': 50, 'feature_fraction': 0.42757480323271685, 'bagging_fraction': 0.465453152240145, 'bagging_freq': 6, 'min_child_samples': 48}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:23,437] Trial 39 finished with value: 0.724174564494485 and parameters: {'lambda_l1': 0.06848055132603298, 'lambda_l2': 3.7189368962232267e-05, 'num_leaves': 253, 'feature_fraction': 0.40363567770859865, 'bagging_fraction': 0.5734761238660789, 'bagging_freq': 5, 'min_child_samples': 97}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:23,508] Trial 40 finished with value: 0.7111593700193611 and parameters: {'lambda_l1': 0.023671852266220754, 'lambda_l2': 1.0930210067905337e-05, 'num_leaves': 133, 'feature_fraction': 0.5532936557175014, 'bagging_fraction': 0.4377379152808493, 'bagging_freq': 6, 'min_child_samples': 77}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:23,597] Trial 41 finished with value: 0.7237501486650133 and parameters: {'lambda_l1': 0.8368140946050403, 'lambda_l2': 0.00016049207897512427, 'num_leaves': 188, 'feature_fraction': 0.4620190061588984, 'bagging_fraction': 0.4306976254545925, 'bagging_freq': 3, 'min_child_samples': 36}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:23,715] Trial 42 finished with value: 0.7941002084342373 and parameters: {'lambda_l1': 0.312405518796124, 'lambda_l2': 0.0008583987733721772, 'num_leaves': 148, 'feature_fraction': 0.5096513800316854, 'bagging_fraction': 0.4892982380601658, 'bagging_freq': 4, 'min_child_samples': 19}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:23,809] Trial 43 finished with value: 0.7147602440149114 and parameters: {'lambda_l1': 1.0112192971857599, 'lambda_l2': 0.0002664906913500277, 'num_leaves': 219, 'feature_fraction': 0.4762894149396539, 'bagging_fraction': 0.43684226500466056, 'bagging_freq': 4, 'min_child_samples': 38}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:23,880] Trial 44 finished with value: 0.67769659293346 and parameters: {'lambda_l1': 4.0479395828089, 'lambda_l2': 1.745476053902487e-05, 'num_leaves': 84, 'feature_fraction': 0.4314221237061272, 'bagging_fraction': 0.47333150312197053, 'bagging_freq': 3, 'min_child_samples': 57}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:23,950] Trial 45 finished with value: 0.6839989033073577 and parameters: {'lambda_l1': 4.3149129199548675, 'lambda_l2': 1.601626693431919e-05, 'num_leaves': 71, 'feature_fraction': 0.42472494740423195, 'bagging_fraction': 0.5270681507232997, 'bagging_freq': 2, 'min_child_samples': 67}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:24,027] Trial 46 finished with value: 0.67347685903164 and parameters: {'lambda_l1': 4.54652042606339, 'lambda_l2': 3.1833156024951498e-06, 'num_leaves': 97, 'feature_fraction': 0.5328296088912211, 'bagging_fraction': 0.47598790694097576, 'bagging_freq': 2, 'min_child_samples': 57}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:24,104] Trial 47 finished with value: 0.7035912005025221 and parameters: {'lambda_l1': 5.216819949355423, 'lambda_l2': 1.1489335254576564e-06, 'num_leaves': 87, 'feature_fraction': 0.6025175449957704, 'bagging_fraction': 0.5467885439456106, 'bagging_freq': 2, 'min_child_samples': 57}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:24,167] Trial 48 finished with value: 0.6706163102705978 and parameters: {'lambda_l1': 9.914789382177103, 'lambda_l2': 5.7689056362998315e-06, 'num_leaves': 69, 'feature_fraction': 0.5447717278081722, 'bagging_fraction': 0.4820605757617893, 'bagging_freq': 3, 'min_child_samples': 63}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:24,244] Trial 49 finished with value: 0.7266651193699452 and parameters: {'lambda_l1': 1.6093772081203934, 'lambda_l2': 6.0767309599525325e-06, 'num_leaves': 121, 'feature_fraction': 0.5647421473648633, 'bagging_fraction': 0.4967213348383791, 'bagging_freq': 2, 'min_child_samples': 63}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:24,299] Trial 50 finished with value: 0.67354210084046 and parameters: {'lambda_l1': 9.377880085395862, 'lambda_l2': 1.7806725299216931e-06, 'num_leaves': 2, 'feature_fraction': 0.532583945882532, 'bagging_fraction': 0.4756410115125752, 'bagging_freq': 3, 'min_child_samples': 71}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:24,361] Trial 51 finished with value: 0.6883422197990774 and parameters: {'lambda_l1': 5.40851616296323, 'lambda_l2': 2.985263366386234e-06, 'num_leaves': 4, 'feature_fraction': 0.5350883410193219, 'bagging_fraction': 0.4726387873361445, 'bagging_freq': 3, 'min_child_samples': 71}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:24,420] Trial 52 finished with value: 0.6529262639105768 and parameters: {'lambda_l1': 8.661370012778729, 'lambda_l2': 8.894554275814455e-07, 'num_leaves': 21, 'feature_fraction': 0.5011920420277882, 'bagging_fraction': 0.4483951856315659, 'bagging_freq': 1, 'min_child_samples': 71}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:24,486] Trial 53 finished with value: 0.6871939752838083 and parameters: {'lambda_l1': 8.316740568308814, 'lambda_l2': 8.857793096231717e-07, 'num_leaves': 20, 'feature_fraction': 0.5335553307803952, 'bagging_fraction': 0.5328155975533408, 'bagging_freq': 1, 'min_child_samples': 70}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:24,544] Trial 54 finished with value: 0.6602285424461748 and parameters: {'lambda_l1': 9.907799876580365, 'lambda_l2': 3.2386804655107647e-07, 'num_leaves': 23, 'feature_fraction': 0.5058391012128919, 'bagging_fraction': 0.4551914293117715, 'bagging_freq': 1, 'min_child_samples': 75}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:24,606] Trial 55 finished with value: 0.7153641381673917 and parameters: {'lambda_l1': 1.6724908523189297, 'lambda_l2': 3.2514880135490534e-07, 'num_leaves': 22, 'feature_fraction': 0.5681057775044273, 'bagging_fraction': 0.5082831672488164, 'bagging_freq': 1, 'min_child_samples': 75}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:24,662] Trial 56 finished with value: 0.6645094099851674 and parameters: {'lambda_l1': 9.686289778843925, 'lambda_l2': 2.5442766789510056e-06, 'num_leaves': 31, 'feature_fraction': 0.503930751326926, 'bagging_fraction': 0.45505122681325405, 'bagging_freq': 2, 'min_child_samples': 85}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:24,722] Trial 57 finished with value: 0.6852513036652096 and parameters: {'lambda_l1': 0.298783508738236, 'lambda_l2': 7.988150702489209e-08, 'num_leaves': 31, 'feature_fraction': 0.5989692815879198, 'bagging_fraction': 0.45159454452031556, 'bagging_freq': 1, 'min_child_samples': 86}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:24,787] Trial 58 finished with value: 0.7013842246147338 and parameters: {'lambda_l1': 1.491681272923776, 'lambda_l2': 3.7779789416275356e-06, 'num_leaves': 45, 'feature_fraction': 0.5117879725251425, 'bagging_fraction': 0.5668620212476312, 'bagging_freq': 2, 'min_child_samples': 81}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:24,845] Trial 59 finished with value: 0.6827038473804176 and parameters: {'lambda_l1': 3.4309711926493214, 'lambda_l2': 4.761110862841148e-07, 'num_leaves': 13, 'feature_fraction': 0.5067901239137583, 'bagging_fraction': 0.4833648093734773, 'bagging_freq': 2, 'min_child_samples': 93}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:24,903] Trial 60 finished with value: 0.6694463492532906 and parameters: {'lambda_l1': 0.5139067918746006, 'lambda_l2': 7.673590754194753e-06, 'num_leaves': 30, 'feature_fraction': 0.5862178245351362, 'bagging_fraction': 0.4223000131528571, 'bagging_freq': 1, 'min_child_samples': 78}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:24,960] Trial 61 finished with value: 0.6549132187220044 and parameters: {'lambda_l1': 9.864436634792796, 'lambda_l2': 7.403707761694529e-06, 'num_leaves': 26, 'feature_fraction': 0.5577114419160774, 'bagging_fraction': 0.42453246145881474, 'bagging_freq': 1, 'min_child_samples': 88}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:25,020] Trial 62 finished with value: 0.675055750380783 and parameters: {'lambda_l1': 2.2396735907457774, 'lambda_l2': 8.328694566708019e-06, 'num_leaves': 27, 'feature_fraction': 0.5640036104967088, 'bagging_fraction': 0.41627752203092805, 'bagging_freq': 1, 'min_child_samples': 86}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:25,077] Trial 63 finished with value: 0.6525312115839582 and parameters: {'lambda_l1': 9.565710999703603, 'lambda_l2': 1.8085582143541919e-06, 'num_leaves': 40, 'feature_fraction': 0.4991847289827949, 'bagging_fraction': 0.42461332427887083, 'bagging_freq': 1, 'min_child_samples': 90}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:25,135] Trial 64 finished with value: 0.6983100141922755 and parameters: {'lambda_l1': 1.0261980138764004, 'lambda_l2': 2.020031367241138e-06, 'num_leaves': 43, 'feature_fraction': 0.4998292162701102, 'bagging_fraction': 0.416521464505422, 'bagging_freq': 1, 'min_child_samples': 91}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:25,200] Trial 65 finished with value: 0.6724030782268184 and parameters: {'lambda_l1': 5.022715388432832, 'lambda_l2': 7.03336237666738e-07, 'num_leaves': 36, 'feature_fraction': 0.577916341416545, 'bagging_fraction': 0.4498836346946002, 'bagging_freq': 1, 'min_child_samples': 78}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:25,260] Trial 66 finished with value: 0.6647317229987877 and parameters: {'lambda_l1': 0.5002644946223076, 'lambda_l2': 1.169646150729643e-07, 'num_leaves': 12, 'feature_fraction': 0.6188869214674955, 'bagging_fraction': 0.42080097455395, 'bagging_freq': 1, 'min_child_samples': 99}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:25,317] Trial 67 finished with value: 0.6553318861555849 and parameters: {'lambda_l1': 2.6933312497820734, 'lambda_l2': 1.4882195300335377e-07, 'num_leaves': 11, 'feature_fraction': 0.6402943738100538, 'bagging_fraction': 0.440639857804635, 'bagging_freq': 1, 'min_child_samples': 98}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:25,377] Trial 68 finished with value: 0.6777835979025687 and parameters: {'lambda_l1': 1.7101156572792329, 'lambda_l2': 2.2522717936633013e-07, 'num_leaves': 11, 'feature_fraction': 0.6589374765369225, 'bagging_fraction': 0.45164952468287534, 'bagging_freq': 1, 'min_child_samples': 95}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:25,435] Trial 69 finished with value: 0.6849352130702192 and parameters: {'lambda_l1': 3.13470592297733, 'lambda_l2': 1.2261982420029123e-06, 'num_leaves': 22, 'feature_fraction': 0.49448492830329455, 'bagging_fraction': 0.43960450328264544, 'bagging_freq': 1, 'min_child_samples': 91}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:25,496] Trial 70 finished with value: 0.6708368085834231 and parameters: {'lambda_l1': 9.681086566355534, 'lambda_l2': 5.109812643604726e-07, 'num_leaves': 49, 'feature_fraction': 0.45811654608113117, 'bagging_fraction': 0.5070557679901514, 'bagging_freq': 2, 'min_child_samples': 84}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:25,553] Trial 71 finished with value: 0.653547178239364 and parameters: {'lambda_l1': 2.6650721179785917, 'lambda_l2': 1.3761074017481228e-07, 'num_leaves': 15, 'feature_fraction': 0.6126169275352322, 'bagging_fraction': 0.4191027080994478, 'bagging_freq': 1, 'min_child_samples': 100}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:25,612] Trial 72 finished with value: 0.6648537494382042 and parameters: {'lambda_l1': 5.220961718424994, 'lambda_l2': 5.550513127350651e-08, 'num_leaves': 39, 'feature_fraction': 0.5161149325977936, 'bagging_fraction': 0.4578129366151783, 'bagging_freq': 1, 'min_child_samples': 100}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:25,671] Trial 73 finished with value: 0.6843324303866799 and parameters: {'lambda_l1': 2.5529786493064734, 'lambda_l2': 1.7614497434119756e-07, 'num_leaves': 24, 'feature_fraction': 0.5566239367088077, 'bagging_fraction': 0.4070860862456577, 'bagging_freq': 1, 'min_child_samples': 88}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:25,732] Trial 74 finished with value: 0.6684903141698754 and parameters: {'lambda_l1': 0.8753767144154134, 'lambda_l2': 3.53853884401433e-08, 'num_leaves': 13, 'feature_fraction': 0.610525606209676, 'bagging_fraction': 0.4372623531634418, 'bagging_freq': 1, 'min_child_samples': 96}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:25,791] Trial 75 finished with value: 0.6731605673409743 and parameters: {'lambda_l1': 0.2282574363178246, 'lambda_l2': 3.827102966242786e-07, 'num_leaves': 53, 'feature_fraction': 0.6355519998768885, 'bagging_fraction': 0.4006459885991638, 'bagging_freq': 2, 'min_child_samples': 90}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:25,850] Trial 76 finished with value: 0.6680251714992541 and parameters: {'lambda_l1': 5.729615297203651, 'lambda_l2': 7.63775082188752e-07, 'num_leaves': 16, 'feature_fraction': 0.5878774379672289, 'bagging_fraction': 0.4661245716519272, 'bagging_freq': 1, 'min_child_samples': 94}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:25,908] Trial 77 finished with value: 0.6698641894505132 and parameters: {'lambda_l1': 1.8011481753231522, 'lambda_l2': 1.918955621110772e-07, 'num_leaves': 8, 'feature_fraction': 0.5236676245779442, 'bagging_fraction': 0.42057902649728385, 'bagging_freq': 1, 'min_child_samples': 83}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:26,040] Trial 78 finished with value: 0.71392510048825 and parameters: {'lambda_l1': 2.7474802307951665, 'lambda_l2': 2.130530145264489e-06, 'num_leaves': 28, 'feature_fraction': 0.49578134117750633, 'bagging_fraction': 0.45966713258845393, 'bagging_freq': 2, 'min_child_samples': 9}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:26,101] Trial 79 finished with value: 0.689129405671421 and parameters: {'lambda_l1': 1.1550254308611188, 'lambda_l2': 5.086892078243232e-07, 'num_leaves': 37, 'feature_fraction': 0.474141887284729, 'bagging_fraction': 0.4389092738090348, 'bagging_freq': 1, 'min_child_samples': 89}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:26,167] Trial 80 finished with value: 0.6807812807424637 and parameters: {'lambda_l1': 6.521335389413, 'lambda_l2': 2.3717635475278542e-08, 'num_leaves': 63, 'feature_fraction': 0.552245855299759, 'bagging_fraction': 0.496487215058723, 'bagging_freq': 1, 'min_child_samples': 93}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:26,231] Trial 81 finished with value: 0.6620703487938013 and parameters: {'lambda_l1': 0.4825826191432901, 'lambda_l2': 9.30554231930979e-08, 'num_leaves': 17, 'feature_fraction': 0.6164715336498526, 'bagging_fraction': 0.4203166487931224, 'bagging_freq': 1, 'min_child_samples': 100}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:26,288] Trial 82 finished with value: 0.6579544312794336 and parameters: {'lambda_l1': 3.495591083236761, 'lambda_l2': 1.1047868068196703e-07, 'num_leaves': 2, 'feature_fraction': 0.631843355139772, 'bagging_fraction': 0.4314759622672586, 'bagging_freq': 1, 'min_child_samples': 98}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:26,345] Trial 83 finished with value: 0.6508638445294357 and parameters: {'lambda_l1': 3.5227328869645906, 'lambda_l2': 9.228150288524303e-08, 'num_leaves': 2, 'feature_fraction': 0.6377705383078828, 'bagging_fraction': 0.4260398770303659, 'bagging_freq': 1, 'min_child_samples': 98}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:26,404] Trial 84 finished with value: 0.6568871412940623 and parameters: {'lambda_l1': 4.186132720153971, 'lambda_l2': 1.677050743922278e-07, 'num_leaves': 3, 'feature_fraction': 0.6780270700940919, 'bagging_fraction': 0.44152901402552797, 'bagging_freq': 1, 'min_child_samples': 97}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:26,463] Trial 85 finished with value: 0.654570244855078 and parameters: {'lambda_l1': 3.4740526363768325, 'lambda_l2': 1.1513464178900526e-07, 'num_leaves': 4, 'feature_fraction': 0.6876092088575492, 'bagging_fraction': 0.43259891063350864, 'bagging_freq': 1, 'min_child_samples': 97}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:26,522] Trial 86 finished with value: 0.6534670974246617 and parameters: {'lambda_l1': 0.7429778505870422, 'lambda_l2': 1.551515541509952e-08, 'num_leaves': 8, 'feature_fraction': 0.6897760209656839, 'bagging_fraction': 0.41083200028578176, 'bagging_freq': 1, 'min_child_samples': 97}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:26,582] Trial 87 finished with value: 0.6630869493926157 and parameters: {'lambda_l1': 0.805450934361645, 'lambda_l2': 4.252843879099725e-08, 'num_leaves': 9, 'feature_fraction': 0.7009304548984137, 'bagging_fraction': 0.4129062815695014, 'bagging_freq': 1, 'min_child_samples': 93}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:26,639] Trial 88 finished with value: 0.6642906886658765 and parameters: {'lambda_l1': 1.3052508657349788, 'lambda_l2': 1.8226874165578468e-08, 'num_leaves': 18, 'feature_fraction': 0.6530428072243046, 'bagging_fraction': 0.40960881809132565, 'bagging_freq': 1, 'min_child_samples': 96}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:26,697] Trial 89 finished with value: 0.6600389647865681 and parameters: {'lambda_l1': 2.162214553980536, 'lambda_l2': 6.34472119037029e-08, 'num_leaves': 8, 'feature_fraction': 0.6776075513507783, 'bagging_fraction': 0.4300943334931336, 'bagging_freq': 2, 'min_child_samples': 91}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:26,758] Trial 90 finished with value: 0.6772004307187455 and parameters: {'lambda_l1': 6.018610198496412, 'lambda_l2': 3.149480239009326e-08, 'num_leaves': 16, 'feature_fraction': 0.647969254549306, 'bagging_fraction': 0.4878399578016334, 'bagging_freq': 1, 'min_child_samples': 100}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:26,817] Trial 91 finished with value: 0.6568475685177021 and parameters: {'lambda_l1': 4.037261006141373, 'lambda_l2': 1.1327989552158126e-08, 'num_leaves': 4, 'feature_fraction': 0.6771029042623937, 'bagging_fraction': 0.44122828463701463, 'bagging_freq': 1, 'min_child_samples': 97}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:26,878] Trial 92 finished with value: 0.6857917325738803 and parameters: {'lambda_l1': 2.531283076149664, 'lambda_l2': 1.5313624101912542e-08, 'num_leaves': 10, 'feature_fraction': 0.714390322896782, 'bagging_fraction': 0.46285372892515564, 'bagging_freq': 1, 'min_child_samples': 95}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:26,940] Trial 93 finished with value: 0.6572317202521828 and parameters: {'lambda_l1': 5.892523918366472, 'lambda_l2': 1.1595439348564207e-08, 'num_leaves': 25, 'feature_fraction': 0.6756373863321399, 'bagging_fraction': 0.40174539956095345, 'bagging_freq': 1, 'min_child_samples': 88}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:27,007] Trial 94 finished with value: 0.6573710027163884 and parameters: {'lambda_l1': 1.2812914262162993, 'lambda_l2': 2.427008837160592e-08, 'num_leaves': 7, 'feature_fraction': 0.6276771924351638, 'bagging_fraction': 0.4271680903960996, 'bagging_freq': 1, 'min_child_samples': 98}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:27,068] Trial 95 finished with value: 0.6650106568260404 and parameters: {'lambda_l1': 3.3604920494669623, 'lambda_l2': 6.023532594991068e-08, 'num_leaves': 17, 'feature_fraction': 0.6671658319833957, 'bagging_fraction': 0.4421690552565115, 'bagging_freq': 1, 'min_child_samples': 94}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:27,128] Trial 96 finished with value: 0.6924015283817428 and parameters: {'lambda_l1': 0.8173763020931393, 'lambda_l2': 1.1828848904336927e-08, 'num_leaves': 3, 'feature_fraction': 0.6441195378445335, 'bagging_fraction': 0.47143740198400597, 'bagging_freq': 2, 'min_child_samples': 97}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:27,187] Trial 97 finished with value: 0.6808756544348011 and parameters: {'lambda_l1': 1.6160296789711435, 'lambda_l2': 4.140238411513125e-08, 'num_leaves': 27, 'feature_fraction': 0.6052988404455271, 'bagging_fraction': 0.44519428561237007, 'bagging_freq': 1, 'min_child_samples': 92}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:27,251] Trial 98 finished with value: 0.6596627675605056 and parameters: {'lambda_l1': 6.009327488724517, 'lambda_l2': 1.269740096627927e-07, 'num_leaves': 41, 'feature_fraction': 0.6944594660378278, 'bagging_fraction': 0.41215203532713307, 'bagging_freq': 1, 'min_child_samples': 89}. Best is trial 31 with value: 0.647437194658588.\n",
      "[I 2023-12-14 01:10:27,310] Trial 99 finished with value: 0.683825072505849 and parameters: {'lambda_l1': 4.056410164467917, 'lambda_l2': 1.0259615508185242e-08, 'num_leaves': 21, 'feature_fraction': 0.6420880425096919, 'bagging_fraction': 0.4814011758427259, 'bagging_freq': 1, 'min_child_samples': 100}. Best is trial 31 with value: 0.647437194658588.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 100\n",
      "Best trial:\n",
      "  Value: 0.647437194658588\n",
      "  Params: \n",
      "    lambda_l1: 8.159068587025143\n",
      "    lambda_l2: 0.002023672178083324\n",
      "    num_leaves: 6\n",
      "    feature_fraction: 0.5535793492575833\n",
      "    bagging_fraction: 0.4367578543726366\n",
      "    bagging_freq: 6\n",
      "    min_child_samples: 17\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = data[feature_cols]\n",
    "y = data['index_return']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "# 使用LightGBM，用optuna调参\n",
    "def objective(trial):\n",
    "    ...\n",
    "\n",
    "    # 2. Suggest values of the hyperparameters using a trial object.\n",
    "    param = {\n",
    "        'objective': 'regression',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'feature_pre_filter': False\n",
    "    }\n",
    "\n",
    "    gbm = lgb.train(param, train_data)\n",
    "    preds = gbm.predict(X_test)\n",
    "    loss = mean_squared_error(y_test, preds)\n",
    "    return loss\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17369407202672293\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import shap\n",
    "\n",
    "bst_params = {\n",
    "        'objective': 'regression',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'lambda_l1': 0.6311636859918583,\n",
    "        'lambda_l2': 2.9691410198069135e-08,\n",
    "        'num_leaves': 2,\n",
    "        'feature_fraction': 0.8717773875737789,\n",
    "        'bagging_fraction': 0.9678736282682716,\n",
    "        'bagging_freq': 6,\n",
    "        'min_child_samples': 38,\n",
    "        'feature_pre_filter': False\n",
    "    }\n",
    "gbm = lgb.train(bst_params, train_data)\n",
    "gbm_explainer = shap.TreeExplainer(gbm)\n",
    "preds = gbm.predict(X_test)\n",
    "r2 = r2_score(y_test, preds)\n",
    "gbm_shap = gbm_explainer.shap_values(X)\n",
    "print(r2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "     feature1  feature2  feature3  feature1_feature2_sum  \\\n0         0.0       0.0  0.065891                    0.0   \n1         0.0       0.0 -0.010586                    0.0   \n2         0.0       0.0  0.065891                    0.0   \n3         0.0       0.0 -0.010586                    0.0   \n4         0.0       0.0 -0.010586                    0.0   \n..        ...       ...       ...                    ...   \n700       0.0       0.0 -0.010586                    0.0   \n701       0.0       0.0 -0.010586                    0.0   \n702       0.0       0.0 -0.010586                    0.0   \n703       0.0       0.0 -0.010586                    0.0   \n704       0.0       0.0 -0.010586                    0.0   \n\n     feature1_feature2_difference  feature1_feature2_product  \\\n0                             0.0                        0.0   \n1                             0.0                        0.0   \n2                             0.0                        0.0   \n3                             0.0                        0.0   \n4                             0.0                        0.0   \n..                            ...                        ...   \n700                           0.0                        0.0   \n701                           0.0                        0.0   \n702                           0.0                        0.0   \n703                           0.0                        0.0   \n704                           0.0                        0.0   \n\n     feature1_feature2_quotient  feature1_feature3_sum  \\\n0                           0.0                    0.0   \n1                           0.0                    0.0   \n2                           0.0                    0.0   \n3                           0.0                    0.0   \n4                           0.0                    0.0   \n..                          ...                    ...   \n700                         0.0                    0.0   \n701                         0.0                    0.0   \n702                         0.0                    0.0   \n703                         0.0                    0.0   \n704                         0.0                    0.0   \n\n     feature1_feature3_difference  feature1_feature3_product  ...  \\\n0                             0.0                  -0.001996  ...   \n1                             0.0                  -0.001996  ...   \n2                             0.0                  -0.001996  ...   \n3                             0.0                  -0.001996  ...   \n4                             0.0                  -0.001996  ...   \n..                            ...                        ...  ...   \n700                           0.0                  -0.001996  ...   \n701                           0.0                  -0.001996  ...   \n702                           0.0                  -0.001996  ...   \n703                           0.0                  -0.001996  ...   \n704                           0.0                  -0.001996  ...   \n\n     feature2_feature3_quotient_5_vol  feature2_feature3_quotient_10_mean  \\\n0                                 0.0                                 0.0   \n1                                 0.0                                 0.0   \n2                                 0.0                                 0.0   \n3                                 0.0                                 0.0   \n4                                 0.0                                 0.0   \n..                                ...                                 ...   \n700                               0.0                                 0.0   \n701                               0.0                                 0.0   \n702                               0.0                                 0.0   \n703                               0.0                                 0.0   \n704                               0.0                                 0.0   \n\n     feature2_feature3_quotient_10_bias  feature2_feature3_quotient_10_vol  \\\n0                                   0.0                                0.0   \n1                                   0.0                                0.0   \n2                                   0.0                                0.0   \n3                                   0.0                                0.0   \n4                                   0.0                                0.0   \n..                                  ...                                ...   \n700                                 0.0                                0.0   \n701                                 0.0                                0.0   \n702                                 0.0                                0.0   \n703                                 0.0                                0.0   \n704                                 0.0                                0.0   \n\n     feature2_feature3_quotient_20_mean  feature2_feature3_quotient_20_bias  \\\n0                                   0.0                                 0.0   \n1                                   0.0                                 0.0   \n2                                   0.0                                 0.0   \n3                                   0.0                                 0.0   \n4                                   0.0                                 0.0   \n..                                  ...                                 ...   \n700                                 0.0                                 0.0   \n701                                 0.0                                 0.0   \n702                                 0.0                                 0.0   \n703                                 0.0                                 0.0   \n704                                 0.0                                 0.0   \n\n     feature2_feature3_quotient_20_vol  feature2_feature3_quotient_5_10  \\\n0                                  0.0                              0.0   \n1                                  0.0                              0.0   \n2                                  0.0                              0.0   \n3                                  0.0                              0.0   \n4                                  0.0                              0.0   \n..                                 ...                              ...   \n700                                0.0                              0.0   \n701                                0.0                              0.0   \n702                                0.0                              0.0   \n703                                0.0                              0.0   \n704                                0.0                              0.0   \n\n     feature2_feature3_quotient_5_20  feature2_feature3_quotient_10_20  \n0                                0.0                               0.0  \n1                                0.0                               0.0  \n2                                0.0                               0.0  \n3                                0.0                               0.0  \n4                                0.0                               0.0  \n..                               ...                               ...  \n700                              0.0                               0.0  \n701                              0.0                               0.0  \n702                              0.0                               0.0  \n703                              0.0                               0.0  \n704                              0.0                               0.0  \n\n[705 rows x 195 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature1</th>\n      <th>feature2</th>\n      <th>feature3</th>\n      <th>feature1_feature2_sum</th>\n      <th>feature1_feature2_difference</th>\n      <th>feature1_feature2_product</th>\n      <th>feature1_feature2_quotient</th>\n      <th>feature1_feature3_sum</th>\n      <th>feature1_feature3_difference</th>\n      <th>feature1_feature3_product</th>\n      <th>...</th>\n      <th>feature2_feature3_quotient_5_vol</th>\n      <th>feature2_feature3_quotient_10_mean</th>\n      <th>feature2_feature3_quotient_10_bias</th>\n      <th>feature2_feature3_quotient_10_vol</th>\n      <th>feature2_feature3_quotient_20_mean</th>\n      <th>feature2_feature3_quotient_20_bias</th>\n      <th>feature2_feature3_quotient_20_vol</th>\n      <th>feature2_feature3_quotient_5_10</th>\n      <th>feature2_feature3_quotient_5_20</th>\n      <th>feature2_feature3_quotient_10_20</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.065891</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.001996</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.010586</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.001996</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.065891</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.001996</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.010586</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.001996</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.010586</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.001996</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>700</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.010586</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.001996</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>701</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.010586</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.001996</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>702</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.010586</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.001996</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>703</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.010586</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.001996</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>704</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.010586</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.001996</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>705 rows × 195 columns</p>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_shap = pd.DataFrame(gbm_shap)\n",
    "gbm_shap.columns = X.columns\n",
    "gbm_shap"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "feature1                              0.000000\nfeature2                              0.000000\nfeature3                             -0.000064\nfeature1_feature2_sum                 0.000000\nfeature1_feature2_difference          0.000000\n                                        ...   \nfeature2_feature3_quotient_20_bias    0.000000\nfeature2_feature3_quotient_20_vol     0.000000\nfeature2_feature3_quotient_5_10       0.000000\nfeature2_feature3_quotient_5_20       0.000000\nfeature2_feature3_quotient_10_20      0.000000\nLength: 195, dtype: float64"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance = gbm_shap.mean(axis=0)\n",
    "feature_importance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "feature2_5_bias                       -0.004964\nfeature1_feature2_quotient_10_vol     -0.000704\nfeature2_feature3_product_5_mean      -0.000383\nfeature2_feature3_sum_5_bias          -0.000303\nfeature1_feature3_product_5_20        -0.000243\nfeature2_feature3_difference_5_bias   -0.000235\nfeature3_5_20                         -0.000154\nfeature1_feature2_sum_5_20            -0.000121\nfeature1_feature3_product             -0.000092\nfeature3_5_10                         -0.000066\nfeature3                              -0.000064\nfeature2_feature3_product_10_bias     -0.000040\nfeature1_feature2_product_20_bias     -0.000033\nfeature1_feature2_product_5_bias      -0.000021\nfeature1_feature2_sum_5_bias          -0.000004\nfeature1_feature3_product_10_bias      0.000014\nfeature1_feature3_sum_5_bias           0.000026\nfeature1_feature2_product_5_20         0.000032\nfeature2_5_vol                         0.000038\nfeature2_feature3_product_5_10         0.000066\nfeature2_feature3_difference_5_20      0.000080\nfeature2_feature3_product_5_bias       0.000101\nfeature1_feature2_product_5_vol        0.000106\nfeature1_feature3_product_10_vol       0.000138\nfeature1_feature2_quotient_20_mean     0.000228\nfeature2_feature3_difference           0.000257\nfeature1_feature3_quotient_5_20        0.000348\nfeature3_10_bias                       0.000500\nfeature1_feature3_product_5_10         0.000584\nfeature3_5_bias                        0.004337\ndtype: float64"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance[feature_importance != 0].sort_values()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
